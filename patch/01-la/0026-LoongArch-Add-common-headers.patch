From f9ef0e04aea509f4735495d3a03ecd2694ddcbb7 Mon Sep 17 00:00:00 2001
From: Huacai Chen <chenhuacai@loongson.cn>
Date: Thu, 31 Dec 2020 15:13:33 +0800
Subject: [PATCH 26/48] LoongArch: Add common headers

This patch adds common headers for basic LoongArch support.

Signed-off-by: Huacai Chen <chenhuacai@loongson.cn>
---
 arch/loongarch/include/asm/abi.h              |   28 +
 arch/loongarch/include/asm/acenv.h            |   17 +
 arch/loongarch/include/asm/acpi.h             |   39 +
 arch/loongarch/include/asm/addrspace.h        |  110 ++
 arch/loongarch/include/asm/asm-prototypes.h   |    7 +
 arch/loongarch/include/asm/asm.h              |  212 +++
 arch/loongarch/include/asm/asmmacro-64.h      |   44 +
 arch/loongarch/include/asm/asmmacro.h         |  293 ++++
 arch/loongarch/include/asm/atomic.h           |  298 ++++
 arch/loongarch/include/asm/barrier.h          |   53 +
 arch/loongarch/include/asm/bitops.h           |  253 +++
 arch/loongarch/include/asm/bitrev.h           |   34 +
 arch/loongarch/include/asm/bootinfo.h         |   33 +
 arch/loongarch/include/asm/bugs.h             |   24 +
 arch/loongarch/include/asm/clocksource.h      |   12 +
 arch/loongarch/include/asm/compiler.h         |   15 +
 arch/loongarch/include/asm/cpu-features.h     |   67 +
 arch/loongarch/include/asm/cpu-info.h         |  138 ++
 arch/loongarch/include/asm/cpu.h              |  123 ++
 arch/loongarch/include/asm/device.h           |   17 +
 arch/loongarch/include/asm/dma-direct.h       |   11 +
 arch/loongarch/include/asm/dma.h              |   13 +
 arch/loongarch/include/asm/dmi.h              |   24 +
 arch/loongarch/include/asm/efi.h              |   35 +
 arch/loongarch/include/asm/fpregdef.h         |   55 +
 arch/loongarch/include/asm/fpu.h              |  135 ++
 arch/loongarch/include/asm/fw.h               |   18 +
 arch/loongarch/include/asm/inst.h             |  249 +++
 arch/loongarch/include/asm/linkage.h          |   36 +
 arch/loongarch/include/asm/local.h            |  138 ++
 arch/loongarch/include/asm/loongarchregs.h    | 1554 +++++++++++++++++
 arch/loongarch/include/asm/page.h             |  133 ++
 arch/loongarch/include/asm/percpu.h           |   20 +
 arch/loongarch/include/asm/perf_event.h       |   10 +
 arch/loongarch/include/asm/pgalloc.h          |   97 +
 arch/loongarch/include/asm/pgtable-64.h       |  291 +++
 arch/loongarch/include/asm/pgtable-bits.h     |  135 ++
 arch/loongarch/include/asm/pgtable.h          |  341 ++++
 arch/loongarch/include/asm/prefetch.h         |   29 +
 arch/loongarch/include/asm/processor.h        |  219 +++
 arch/loongarch/include/asm/reboot.h           |   10 +
 arch/loongarch/include/asm/regdef.h           |   43 +
 arch/loongarch/include/asm/serial.h           |   11 +
 arch/loongarch/include/asm/setup.h            |   24 +
 arch/loongarch/include/asm/spinlock.h         |   12 +
 arch/loongarch/include/asm/spinlock_types.h   |   11 +
 arch/loongarch/include/asm/time.h             |   50 +
 arch/loongarch/include/asm/timex.h            |   36 +
 arch/loongarch/include/asm/topology.h         |   15 +
 arch/loongarch/include/asm/types.h            |   33 +
 arch/loongarch/include/uapi/asm/abidefs.h     |   21 +
 arch/loongarch/include/uapi/asm/bitfield.h    |   15 +
 arch/loongarch/include/uapi/asm/bitsperlong.h |    9 +
 arch/loongarch/include/uapi/asm/byteorder.h   |   13 +
 arch/loongarch/include/uapi/asm/inst.h        |  467 +++++
 arch/loongarch/include/uapi/asm/reg.h         |  103 ++
 56 files changed, 6233 insertions(+)
 create mode 100644 arch/loongarch/include/asm/abi.h
 create mode 100644 arch/loongarch/include/asm/acenv.h
 create mode 100644 arch/loongarch/include/asm/acpi.h
 create mode 100644 arch/loongarch/include/asm/addrspace.h
 create mode 100644 arch/loongarch/include/asm/asm-prototypes.h
 create mode 100644 arch/loongarch/include/asm/asm.h
 create mode 100644 arch/loongarch/include/asm/asmmacro-64.h
 create mode 100644 arch/loongarch/include/asm/asmmacro.h
 create mode 100644 arch/loongarch/include/asm/atomic.h
 create mode 100644 arch/loongarch/include/asm/barrier.h
 create mode 100644 arch/loongarch/include/asm/bitops.h
 create mode 100644 arch/loongarch/include/asm/bitrev.h
 create mode 100644 arch/loongarch/include/asm/bootinfo.h
 create mode 100644 arch/loongarch/include/asm/bugs.h
 create mode 100644 arch/loongarch/include/asm/clocksource.h
 create mode 100644 arch/loongarch/include/asm/compiler.h
 create mode 100644 arch/loongarch/include/asm/cpu-features.h
 create mode 100644 arch/loongarch/include/asm/cpu-info.h
 create mode 100644 arch/loongarch/include/asm/cpu.h
 create mode 100644 arch/loongarch/include/asm/device.h
 create mode 100644 arch/loongarch/include/asm/dma-direct.h
 create mode 100644 arch/loongarch/include/asm/dma.h
 create mode 100644 arch/loongarch/include/asm/dmi.h
 create mode 100644 arch/loongarch/include/asm/efi.h
 create mode 100644 arch/loongarch/include/asm/fpregdef.h
 create mode 100644 arch/loongarch/include/asm/fpu.h
 create mode 100644 arch/loongarch/include/asm/fw.h
 create mode 100644 arch/loongarch/include/asm/inst.h
 create mode 100644 arch/loongarch/include/asm/linkage.h
 create mode 100644 arch/loongarch/include/asm/local.h
 create mode 100644 arch/loongarch/include/asm/loongarchregs.h
 create mode 100644 arch/loongarch/include/asm/page.h
 create mode 100644 arch/loongarch/include/asm/percpu.h
 create mode 100644 arch/loongarch/include/asm/perf_event.h
 create mode 100644 arch/loongarch/include/asm/pgalloc.h
 create mode 100644 arch/loongarch/include/asm/pgtable-64.h
 create mode 100644 arch/loongarch/include/asm/pgtable-bits.h
 create mode 100644 arch/loongarch/include/asm/pgtable.h
 create mode 100644 arch/loongarch/include/asm/prefetch.h
 create mode 100644 arch/loongarch/include/asm/processor.h
 create mode 100644 arch/loongarch/include/asm/reboot.h
 create mode 100644 arch/loongarch/include/asm/regdef.h
 create mode 100644 arch/loongarch/include/asm/serial.h
 create mode 100644 arch/loongarch/include/asm/setup.h
 create mode 100644 arch/loongarch/include/asm/spinlock.h
 create mode 100644 arch/loongarch/include/asm/spinlock_types.h
 create mode 100644 arch/loongarch/include/asm/time.h
 create mode 100644 arch/loongarch/include/asm/timex.h
 create mode 100644 arch/loongarch/include/asm/topology.h
 create mode 100644 arch/loongarch/include/asm/types.h
 create mode 100644 arch/loongarch/include/uapi/asm/abidefs.h
 create mode 100644 arch/loongarch/include/uapi/asm/bitfield.h
 create mode 100644 arch/loongarch/include/uapi/asm/bitsperlong.h
 create mode 100644 arch/loongarch/include/uapi/asm/byteorder.h
 create mode 100644 arch/loongarch/include/uapi/asm/inst.h
 create mode 100644 arch/loongarch/include/uapi/asm/reg.h

diff --git a/arch/loongarch/include/asm/abi.h b/arch/loongarch/include/asm/abi.h
new file mode 100644
index 000000000000..e168a161a9c6
--- /dev/null
+++ b/arch/loongarch/include/asm/abi.h
@@ -0,0 +1,28 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_ABI_H
+#define _ASM_ABI_H
+
+#include <linux/signal_types.h>
+
+#include <asm/signal.h>
+#include <asm/siginfo.h>
+#include <asm/vdso.h>
+
+struct loongarch_abi {
+	const unsigned long	restart;
+	const int		audit_arch;
+
+	unsigned int off_sc_fpregs;
+	unsigned int off_sc_fcc;
+	unsigned int off_sc_fcsr;
+	unsigned int off_sc_vcsr;
+	unsigned int off_sc_flags;
+	unsigned int off_sc_scr;
+
+	struct loongarch_vdso_info *vdso;
+};
+
+#endif /* _ASM_ABI_H */
diff --git a/arch/loongarch/include/asm/acenv.h b/arch/loongarch/include/asm/acenv.h
new file mode 100644
index 000000000000..d1f41fbc9070
--- /dev/null
+++ b/arch/loongarch/include/asm/acenv.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * LoongArch specific ACPICA environments and implementation
+ *
+ * Author: Jianmin Lv <lvjianmin@loongson.cn>
+ *         Huacai Chen <chenhuacai@loongson.cn>
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+
+#ifndef _ASM_LOONGARCH_ACENV_H
+#define _ASM_LOONGARCH_ACENV_H
+
+/* The head file is required by ACPI core, but we have nothing to fill
+ * it now, update it later when needed.
+ */
+
+#endif /* _ASM_LOONGARCH_ACENV_H */
diff --git a/arch/loongarch/include/asm/acpi.h b/arch/loongarch/include/asm/acpi.h
new file mode 100644
index 000000000000..aab9223e1f4a
--- /dev/null
+++ b/arch/loongarch/include/asm/acpi.h
@@ -0,0 +1,39 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Author: Jianmin Lv <lvjianmin@loongson.cn>
+ *         Huacai Chen <chenhuacai@loongson.cn>
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef _ASM_LOONGARCH_ACPI_H
+#define _ASM_LOONGARCH_ACPI_H
+
+#ifdef CONFIG_ACPI
+extern int acpi_strict;
+extern int acpi_disabled;
+extern int acpi_pci_disabled;
+extern int acpi_noirq;
+
+static inline void disable_acpi(void)
+{
+	acpi_disabled = 1;
+	acpi_pci_disabled = 1;
+	acpi_noirq = 1;
+}
+
+static inline bool acpi_has_cpu_in_madt(void)
+{
+	return true;
+}
+
+extern struct list_head acpi_wakeup_device_list;
+
+#endif /* !CONFIG_ACPI */
+
+#define ACPI_TABLE_UPGRADE_MAX_PHYS (max_low_pfn << PAGE_SHIFT)
+
+#endif /* _ASM_LOONGARCH_ACPI_H */
diff --git a/arch/loongarch/include/asm/addrspace.h b/arch/loongarch/include/asm/addrspace.h
new file mode 100644
index 000000000000..bebff6ee1aec
--- /dev/null
+++ b/arch/loongarch/include/asm/addrspace.h
@@ -0,0 +1,110 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_ADDRSPACE_H
+#define _ASM_ADDRSPACE_H
+
+#include <linux/const.h>
+
+#include <asm/loongarchregs.h>
+
+/*
+ * This gives the physical RAM offset.
+ */
+#ifndef __ASSEMBLY__
+#ifndef PHYS_OFFSET
+#define PHYS_OFFSET	_AC(0, UL)
+#endif
+extern unsigned long vm_map_base;
+#endif /* __ASSEMBLY__ */
+
+#ifndef IO_BASE
+#define IO_BASE			CSR_DMW0_BASE
+#endif
+
+#ifndef CAC_BASE
+#define CAC_BASE		CSR_DMW1_BASE
+#endif
+
+#ifndef UNCAC_BASE
+#define UNCAC_BASE		CSR_DMW0_BASE
+#endif
+
+#define DMW_PABITS	48
+#define TO_PHYS_MASK	((1ULL << DMW_PABITS) - 1)
+
+/*
+ * Memory above this physical address will be considered highmem.
+ */
+#ifndef HIGHMEM_START
+#define HIGHMEM_START		(_AC(1, UL) << _AC(DMW_PABITS, UL))
+#endif
+
+#define TO_PHYS(x)		(	      ((x) & TO_PHYS_MASK))
+#define TO_CAC(x)		(CAC_BASE   | ((x) & TO_PHYS_MASK))
+#define TO_UNCAC(x)		(UNCAC_BASE | ((x) & TO_PHYS_MASK))
+
+/*
+ * This handles the memory map.
+ */
+#ifndef PAGE_OFFSET
+#define PAGE_OFFSET		(CAC_BASE + PHYS_OFFSET)
+#endif
+
+#ifndef FIXADDR_TOP
+#define FIXADDR_TOP		((unsigned long)(long)0xfffe0000)
+#endif
+
+/*
+ *  Configure language
+ */
+#ifdef __ASSEMBLY__
+#define _ATYPE_
+#define _ATYPE32_
+#define _ATYPE64_
+#define _CONST64_(x)	x
+#else
+#define _ATYPE_		__PTRDIFF_TYPE__
+#define _ATYPE32_	int
+#define _ATYPE64_	__s64
+#ifdef CONFIG_64BIT
+#define _CONST64_(x)	x ## L
+#else
+#define _CONST64_(x)	x ## LL
+#endif
+#endif
+
+/*
+ *  32/64-bit LoongArch address spaces
+ */
+#ifdef __ASSEMBLY__
+#define _ACAST32_
+#define _ACAST64_
+#else
+#define _ACAST32_		(_ATYPE_)(_ATYPE32_)	/* widen if necessary */
+#define _ACAST64_		(_ATYPE64_)		/* do _not_ narrow */
+#endif
+
+#ifdef CONFIG_32BIT
+
+#define UVRANGE			0x00000000
+#define KPRANGE0		0x80000000
+#define KPRANGE1		0xa0000000
+#define KVRANGE			0xc0000000
+
+#else
+
+#define XUVRANGE		_CONST64_(0x0000000000000000)
+#define XSPRANGE		_CONST64_(0x4000000000000000)
+#define XKPRANGE		_CONST64_(0x8000000000000000)
+#define XKVRANGE		_CONST64_(0xc000000000000000)
+
+#endif
+
+/*
+ * Returns the physical address of a KPRANGEx / XKPRANGE address
+ */
+#define PHYSADDR(a)		((_ACAST64_(a)) & TO_PHYS_MASK)
+
+#endif /* _ASM_ADDRSPACE_H */
diff --git a/arch/loongarch/include/asm/asm-prototypes.h b/arch/loongarch/include/asm/asm-prototypes.h
new file mode 100644
index 000000000000..ed06d3997420
--- /dev/null
+++ b/arch/loongarch/include/asm/asm-prototypes.h
@@ -0,0 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#include <linux/uaccess.h>
+#include <asm/fpu.h>
+#include <asm/mmu_context.h>
+#include <asm/page.h>
+#include <asm/ftrace.h>
+#include <asm-generic/asm-prototypes.h>
diff --git a/arch/loongarch/include/asm/asm.h b/arch/loongarch/include/asm/asm.h
new file mode 100644
index 000000000000..767c1acf1369
--- /dev/null
+++ b/arch/loongarch/include/asm/asm.h
@@ -0,0 +1,212 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Some useful macros for LoongArch assembler code
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_ASM_H
+#define __ASM_ASM_H
+
+#include <asm/abidefs.h>
+
+#define TEXT(msg)				\
+	.pushsection .data;			\
+8:	.asciz msg;				\
+	.popsection;
+
+/*
+ * Print formatted string
+ */
+#ifdef CONFIG_PRINTK
+#define ASM_PRINT(string)			\
+	la	a0, 8f;				\
+	bl	printk;				\
+	TEXT(string)
+#else
+#define ASM_PRINT(string)
+#endif
+
+#define ASM_PANIC(msg)				\
+	la	a0, 8f;				\
+	bl	panic;				\
+9:	b	9b;				\
+	TEXT(msg)
+
+/*
+ * LoongArch pref instruction.
+ * Use with .set noreorder only!
+ *
+ * LoongArch implementations are free to treat this as a nop.  Loongson-3
+ * is one of them.  So we should have an option not to use this instruction.
+ */
+#ifdef CONFIG_CPU_HAS_PREFETCH
+
+#define PREF(hint, addr, offs)				\
+		preld	hint, addr, offs;		\
+
+#define PREFX(hint, addr, index)			\
+		preldx	hint, addr, index;		\
+
+#else /* !CONFIG_CPU_HAS_PREFETCH */
+
+#define PREF(hint, addr, offs)
+#define PREFX(hint, addr, index)
+
+#endif /* !CONFIG_CPU_HAS_PREFETCH */
+
+/*
+ * Stack alignment
+ */
+#if (_LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64)
+#define ALSZ	15
+#define ALMASK	~15
+#endif
+
+/*
+ * Macros to handle different pointer/register sizes for 32/64-bit code
+ */
+
+/*
+ * Size of a register
+ */
+#ifdef __loongarch64
+#define SZREG	8
+#else
+#define SZREG	4
+#endif
+
+/*
+ * Use the following macros in assemblercode to load/store registers,
+ * pointers etc.
+ */
+#if (_LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64)
+#define REG_S		st.d
+#define REG_L		ld.d
+#define REG_SUBU	sub.d
+#define REG_ADDU	add.d
+#endif
+
+/*
+ * How to add/sub/load/store/shift C int variables.
+ */
+#if (_LOONGARCH_SZINT == 32)
+#define INT_ADDU	add.w
+#define INT_ADDIU	addi.w
+#define INT_SUBU	sub.w
+#define INT_L		ld.w
+#define INT_S		st.w
+#define INT_SLL		slli.w
+#define INT_SLLV	sll.w
+#define INT_SRL		srli.w
+#define INT_SRLV	srl.w
+#define INT_SRA		srai.w
+#define INT_SRAV	sra.w
+#endif
+
+#if (_LOONGARCH_SZINT == 64)
+#define INT_ADDU	add.d
+#define INT_ADDIU	addi.d
+#define INT_SUBU	sub.d
+#define INT_L		ld.d
+#define INT_S		st.d
+#define INT_SLL		slli.d
+#define INT_SLLV	sll.d
+#define INT_SRL		srli.d
+#define INT_SRLV	srl.d
+#define INT_SRA		sra.w
+#define INT_SRAV	sra.d
+#endif
+
+/*
+ * How to add/sub/load/store/shift C long variables.
+ */
+#if (_LOONGARCH_SZLONG == 32)
+#define LONG_ADDU	add.w
+#define LONG_ADDIU	addi.w
+#define LONG_SUBU	sub.w
+#define LONG_L		ld.w
+#define LONG_S		st.w
+#define LONG_SP		swp
+#define LONG_SLL	slli.w
+#define LONG_SLLV	sll.w
+#define LONG_SRL	srli.w
+#define LONG_SRLV	srl.w
+#define LONG_SRA	srai.w
+#define LONG_SRAV	sra.w
+
+#ifdef __ASSEMBLY__
+#define LONG		.word
+#endif
+#define LONGSIZE	4
+#define LONGMASK	3
+#define LONGLOG		2
+#endif
+
+#if (_LOONGARCH_SZLONG == 64)
+#define LONG_ADDU	add.d
+#define LONG_ADDIU	addi.d
+#define LONG_SUBU	sub.d
+#define LONG_L		ld.d
+#define LONG_S		st.d
+#define LONG_SP		sdp
+#define LONG_SLL	slli.d
+#define LONG_SLLV	sll.d
+#define LONG_SRL	srli.d
+#define LONG_SRLV	srl.d
+#define LONG_SRA	sra.w
+#define LONG_SRAV	sra.d
+
+#ifdef __ASSEMBLY__
+#define LONG		.dword
+#endif
+#define LONGSIZE	8
+#define LONGMASK	7
+#define LONGLOG		3
+#endif
+
+/*
+ * How to add/sub/load/store/shift pointers.
+ */
+#if (_LOONGARCH_SZPTR == 32)
+#define PTR_ADDU	add.w
+#define PTR_ADDIU	addi.w
+#define PTR_SUBU	sub.w
+#define PTR_L		ld.w
+#define PTR_S		st.w
+#define PTR_LI		li.w
+#define PTR_SLL		slli.w
+#define PTR_SLLV	sll.w
+#define PTR_SRL		srli.w
+#define PTR_SRLV	srl.w
+#define PTR_SRA		srai.w
+#define PTR_SRAV	sra.w
+
+#define PTR_SCALESHIFT	2
+
+#define PTR		.word
+#define PTRSIZE		4
+#define PTRLOG		2
+#endif
+
+#if (_LOONGARCH_SZPTR == 64)
+#define PTR_ADDU	add.d
+#define PTR_ADDIU	addi.d
+#define PTR_SUBU	sub.d
+#define PTR_L		ld.d
+#define PTR_S		st.d
+#define PTR_LI		li.d
+#define PTR_SLL		slli.d
+#define PTR_SLLV	sll.d
+#define PTR_SRL		srli.d
+#define PTR_SRLV	srl.d
+#define PTR_SRA		srai.d
+#define PTR_SRAV	sra.d
+
+#define PTR_SCALESHIFT	3
+
+#define PTR		.dword
+#define PTRSIZE		8
+#define PTRLOG		3
+#endif
+
+#endif /* __ASM_ASM_H */
diff --git a/arch/loongarch/include/asm/asmmacro-64.h b/arch/loongarch/include/asm/asmmacro-64.h
new file mode 100644
index 000000000000..dd2d89470604
--- /dev/null
+++ b/arch/loongarch/include/asm/asmmacro-64.h
@@ -0,0 +1,44 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * asmmacro.h: Assembler macros to make things easier to read.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_ASMMACRO_64_H
+#define _ASM_ASMMACRO_64_H
+
+#include <asm/asm-offsets.h>
+#include <asm/regdef.h>
+#include <asm/fpregdef.h>
+#include <asm/loongarchregs.h>
+
+	.macro	cpu_save_nonscratch thread
+	stptr.d	s0, \thread, THREAD_REG23
+	stptr.d	s1, \thread, THREAD_REG24
+	stptr.d	s2, \thread, THREAD_REG25
+	stptr.d	s3, \thread, THREAD_REG26
+	stptr.d	s4, \thread, THREAD_REG27
+	stptr.d	s5, \thread, THREAD_REG28
+	stptr.d	s6, \thread, THREAD_REG29
+	stptr.d	s7, \thread, THREAD_REG30
+	stptr.d	s8, \thread, THREAD_REG31
+	stptr.d	sp, \thread, THREAD_REG03
+	stptr.d	fp, \thread, THREAD_REG22
+	.endm
+
+	.macro	cpu_restore_nonscratch thread
+	ldptr.d	s0, \thread, THREAD_REG23
+	ldptr.d	s1, \thread, THREAD_REG24
+	ldptr.d	s2, \thread, THREAD_REG25
+	ldptr.d	s3, \thread, THREAD_REG26
+	ldptr.d	s4, \thread, THREAD_REG27
+	ldptr.d	s5, \thread, THREAD_REG28
+	ldptr.d	s6, \thread, THREAD_REG29
+	ldptr.d	s7, \thread, THREAD_REG30
+	ldptr.d	s8, \thread, THREAD_REG31
+	ldptr.d	sp, \thread, THREAD_REG03
+	ldptr.d	fp, \thread, THREAD_REG22
+	ldptr.d	ra, \thread, THREAD_REG01
+	.endm
+
+#endif /* _ASM_ASMMACRO_64_H */
diff --git a/arch/loongarch/include/asm/asmmacro.h b/arch/loongarch/include/asm/asmmacro.h
new file mode 100644
index 000000000000..3b436dd04056
--- /dev/null
+++ b/arch/loongarch/include/asm/asmmacro.h
@@ -0,0 +1,293 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_ASMMACRO_H
+#define _ASM_ASMMACRO_H
+
+#include <asm/asm-offsets.h>
+
+#ifdef CONFIG_32BIT
+#include <asm/asmmacro-32.h>
+#endif
+#ifdef CONFIG_64BIT
+#include <asm/asmmacro-64.h>
+#endif
+
+#undef v0
+#undef v1
+
+	.macro	parse_v var val
+	\var	= \val
+	.endm
+
+	.macro	parse_r var r
+	\var	= -1
+	.ifc	\r, $r0
+	\var	= 0
+	.endif
+	.ifc	\r, $r1
+	\var	= 1
+	.endif
+	.ifc	\r, $r2
+	\var	= 2
+	.endif
+	.ifc	\r, $r3
+	\var	= 3
+	.endif
+	.ifc	\r, $r4
+	\var	= 4
+	.endif
+	.ifc	\r, $r5
+	\var	= 5
+	.endif
+	.ifc	\r, $r6
+	\var	= 6
+	.endif
+	.ifc	\r, $r7
+	\var	= 7
+	.endif
+	.ifc	\r, $r8
+	\var	= 8
+	.endif
+	.ifc	\r, $r9
+	\var	= 9
+	.endif
+	.ifc	\r, $r10
+	\var	= 10
+	.endif
+	.ifc	\r, $r11
+	\var	= 11
+	.endif
+	.ifc	\r, $r12
+	\var	= 12
+	.endif
+	.ifc	\r, $r13
+	\var	= 13
+	.endif
+	.ifc	\r, $r14
+	\var	= 14
+	.endif
+	.ifc	\r, $r15
+	\var	= 15
+	.endif
+	.ifc	\r, $r16
+	\var	= 16
+	.endif
+	.ifc	\r, $r17
+	\var	= 17
+	.endif
+	.ifc	\r, $r18
+	\var	= 18
+	.endif
+	.ifc	\r, $r19
+	\var	= 19
+	.endif
+	.ifc	\r, $r20
+	\var	= 20
+	.endif
+	.ifc	\r, $r21
+	\var	= 21
+	.endif
+	.ifc	\r, $r22
+	\var	= 22
+	.endif
+	.ifc	\r, $r23
+	\var	= 23
+	.endif
+	.ifc	\r, $r24
+	\var	= 24
+	.endif
+	.ifc	\r, $r25
+	\var	= 25
+	.endif
+	.ifc	\r, $r26
+	\var	= 26
+	.endif
+	.ifc	\r, $r27
+	\var	= 27
+	.endif
+	.ifc	\r, $r28
+	\var	= 28
+	.endif
+	.ifc	\r, $r29
+	\var	= 29
+	.endif
+	.ifc	\r, $r30
+	\var	= 30
+	.endif
+	.ifc	\r, $r31
+	\var	= 31
+	.endif
+	.iflt	\var
+	.error	"Unable to parse register name \r"
+	.endif
+	.endm
+
+	.macro	local_irq_enable reg=t0
+	li.w	t1, (1 << 2)
+	li.w	t0, (1 << 2)
+	csrxchg	t0, t1, LOONGARCH_CSR_CRMD
+	.endm
+
+	.macro	local_irq_disable reg=t0
+	li.w	t1, (1 << 2)
+	li.w	t0, (0 << 2)
+	csrxchg	t0, t1, LOONGARCH_CSR_CRMD
+	.endm
+
+	.macro fpu_save_csr thread tmp
+	movfcsr2gr	\tmp, fcsr0
+	stptr.w	\tmp, \thread, THREAD_FCSR
+	.endm
+
+	.macro fpu_restore_csr thread tmp
+	ldptr.w	\tmp, \thread, THREAD_FCSR
+	movgr2fcsr	fcsr0, \tmp
+	.endm
+
+	.macro fpu_save_cc thread tmp0 tmp1
+	movcf2gr	\tmp0, $fcc0
+	move	\tmp1, \tmp0
+	movcf2gr	\tmp0, $fcc1
+	bstrins.d	\tmp1, \tmp0, 15, 8
+	movcf2gr	\tmp0, $fcc2
+	bstrins.d	\tmp1, \tmp0, 23, 16
+	movcf2gr	\tmp0, $fcc3
+	bstrins.d	\tmp1, \tmp0, 31, 24
+	movcf2gr	\tmp0, $fcc4
+	bstrins.d	\tmp1, \tmp0, 39, 32
+	movcf2gr	\tmp0, $fcc5
+	bstrins.d	\tmp1, \tmp0, 47, 40
+	movcf2gr	\tmp0, $fcc6
+	bstrins.d	\tmp1, \tmp0, 55, 48
+	movcf2gr	\tmp0, $fcc7
+	bstrins.d	\tmp1, \tmp0, 63, 56
+	stptr.d		\tmp1, \thread, THREAD_FCC
+	.endm
+
+	.macro fpu_restore_cc thread tmp0 tmp1
+	ldptr.d	\tmp0, \thread, THREAD_FCC
+	bstrpick.d	\tmp1, \tmp0, 7, 0
+	movgr2cf	$fcc0, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 15, 8
+	movgr2cf	$fcc1, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 23, 16
+	movgr2cf	$fcc2, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 31, 24
+	movgr2cf	$fcc3, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 39, 32
+	movgr2cf	$fcc4, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 47, 40
+	movgr2cf	$fcc5, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 55, 48
+	movgr2cf	$fcc6, \tmp1
+	bstrpick.d	\tmp1, \tmp0, 63, 56
+	movgr2cf	$fcc7, \tmp1
+	.endm
+
+	.macro	fpu_save_double thread tmp
+	li.w	\tmp, THREAD_FPR0
+	PTR_ADDU \tmp, \tmp, \thread
+	fst.d	$f0, \tmp, THREAD_FPR0  - THREAD_FPR0
+	fst.d	$f1, \tmp, THREAD_FPR1  - THREAD_FPR0
+	fst.d	$f2, \tmp, THREAD_FPR2  - THREAD_FPR0
+	fst.d	$f3, \tmp, THREAD_FPR3  - THREAD_FPR0
+	fst.d	$f4, \tmp, THREAD_FPR4  - THREAD_FPR0
+	fst.d	$f5, \tmp, THREAD_FPR5  - THREAD_FPR0
+	fst.d	$f6, \tmp, THREAD_FPR6  - THREAD_FPR0
+	fst.d	$f7, \tmp, THREAD_FPR7  - THREAD_FPR0
+	fst.d	$f8, \tmp, THREAD_FPR8  - THREAD_FPR0
+	fst.d	$f9, \tmp, THREAD_FPR9  - THREAD_FPR0
+	fst.d	$f10, \tmp, THREAD_FPR10 - THREAD_FPR0
+	fst.d	$f11, \tmp, THREAD_FPR11 - THREAD_FPR0
+	fst.d	$f12, \tmp, THREAD_FPR12 - THREAD_FPR0
+	fst.d	$f13, \tmp, THREAD_FPR13 - THREAD_FPR0
+	fst.d	$f14, \tmp, THREAD_FPR14 - THREAD_FPR0
+	fst.d	$f15, \tmp, THREAD_FPR15 - THREAD_FPR0
+	fst.d	$f16, \tmp, THREAD_FPR16 - THREAD_FPR0
+	fst.d	$f17, \tmp, THREAD_FPR17 - THREAD_FPR0
+	fst.d	$f18, \tmp, THREAD_FPR18 - THREAD_FPR0
+	fst.d	$f19, \tmp, THREAD_FPR19 - THREAD_FPR0
+	fst.d	$f20, \tmp, THREAD_FPR20 - THREAD_FPR0
+	fst.d	$f21, \tmp, THREAD_FPR21 - THREAD_FPR0
+	fst.d	$f22, \tmp, THREAD_FPR22 - THREAD_FPR0
+	fst.d	$f23, \tmp, THREAD_FPR23 - THREAD_FPR0
+	fst.d	$f24, \tmp, THREAD_FPR24 - THREAD_FPR0
+	fst.d	$f25, \tmp, THREAD_FPR25 - THREAD_FPR0
+	fst.d	$f26, \tmp, THREAD_FPR26 - THREAD_FPR0
+	fst.d	$f27, \tmp, THREAD_FPR27 - THREAD_FPR0
+	fst.d	$f28, \tmp, THREAD_FPR28 - THREAD_FPR0
+	fst.d	$f29, \tmp, THREAD_FPR29 - THREAD_FPR0
+	fst.d	$f30, \tmp, THREAD_FPR30 - THREAD_FPR0
+	fst.d	$f31, \tmp, THREAD_FPR31 - THREAD_FPR0
+	.endm
+
+	.macro	fpu_restore_double thread tmp
+	li.w	\tmp, THREAD_FPR0
+	PTR_ADDU \tmp, \tmp, \thread
+	fld.d	$f0, \tmp, THREAD_FPR0  - THREAD_FPR0
+	fld.d	$f1, \tmp, THREAD_FPR1  - THREAD_FPR0
+	fld.d	$f2, \tmp, THREAD_FPR2  - THREAD_FPR0
+	fld.d	$f3, \tmp, THREAD_FPR3  - THREAD_FPR0
+	fld.d	$f4, \tmp, THREAD_FPR4  - THREAD_FPR0
+	fld.d	$f5, \tmp, THREAD_FPR5  - THREAD_FPR0
+	fld.d	$f6, \tmp, THREAD_FPR6  - THREAD_FPR0
+	fld.d	$f7, \tmp, THREAD_FPR7  - THREAD_FPR0
+	fld.d	$f8, \tmp, THREAD_FPR8  - THREAD_FPR0
+	fld.d	$f9, \tmp, THREAD_FPR9  - THREAD_FPR0
+	fld.d	$f10, \tmp, THREAD_FPR10 - THREAD_FPR0
+	fld.d	$f11, \tmp, THREAD_FPR11 - THREAD_FPR0
+	fld.d	$f12, \tmp, THREAD_FPR12 - THREAD_FPR0
+	fld.d	$f13, \tmp, THREAD_FPR13 - THREAD_FPR0
+	fld.d	$f14, \tmp, THREAD_FPR14 - THREAD_FPR0
+	fld.d	$f15, \tmp, THREAD_FPR15 - THREAD_FPR0
+	fld.d	$f16, \tmp, THREAD_FPR16 - THREAD_FPR0
+	fld.d	$f17, \tmp, THREAD_FPR17 - THREAD_FPR0
+	fld.d	$f18, \tmp, THREAD_FPR18 - THREAD_FPR0
+	fld.d	$f19, \tmp, THREAD_FPR19 - THREAD_FPR0
+	fld.d	$f20, \tmp, THREAD_FPR20 - THREAD_FPR0
+	fld.d	$f21, \tmp, THREAD_FPR21 - THREAD_FPR0
+	fld.d	$f22, \tmp, THREAD_FPR22 - THREAD_FPR0
+	fld.d	$f23, \tmp, THREAD_FPR23 - THREAD_FPR0
+	fld.d	$f24, \tmp, THREAD_FPR24 - THREAD_FPR0
+	fld.d	$f25, \tmp, THREAD_FPR25 - THREAD_FPR0
+	fld.d	$f26, \tmp, THREAD_FPR26 - THREAD_FPR0
+	fld.d	$f27, \tmp, THREAD_FPR27 - THREAD_FPR0
+	fld.d	$f28, \tmp, THREAD_FPR28 - THREAD_FPR0
+	fld.d	$f29, \tmp, THREAD_FPR29 - THREAD_FPR0
+	fld.d	$f30, \tmp, THREAD_FPR30 - THREAD_FPR0
+	fld.d	$f31, \tmp, THREAD_FPR31 - THREAD_FPR0
+	.endm
+
+.macro move dst src
+	slli.d	\dst, \src, 0
+.endm
+
+.macro jr dst
+	jirl	zero, \dst, 0
+.endm
+
+.macro jalr	dst
+	jirl	ra, \dst, 0
+.endm
+
+.macro not dst src
+	nor	\dst, \src, zero
+.endm
+
+.macro bgt r0 r1 label
+	blt	\r1, \r0, \label
+.endm
+
+.macro bltz r0 label
+	blt	\r0, zero, \label
+.endm
+
+.macro bgez r0 label
+	bge \r0, zero, \label
+.endm
+
+#define v0 $r4
+#define v1 $r5
+#endif /* _ASM_ASMMACRO_H */
diff --git a/arch/loongarch/include/asm/atomic.h b/arch/loongarch/include/asm/atomic.h
new file mode 100644
index 000000000000..091552d84aa0
--- /dev/null
+++ b/arch/loongarch/include/asm/atomic.h
@@ -0,0 +1,298 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Atomic operations.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_ATOMIC_H
+#define _ASM_ATOMIC_H
+
+#include <linux/irqflags.h>
+#include <linux/types.h>
+#include <asm/barrier.h>
+#include <asm/compiler.h>
+#include <asm/cpu-features.h>
+#include <asm/cmpxchg.h>
+
+#define ATOMIC_INIT(i)	  { (i) }
+
+/*
+ * arch_atomic_read - read atomic variable
+ * @v: pointer of type atomic_t
+ *
+ * Atomically reads the value of @v.
+ */
+#define arch_atomic_read(v)	READ_ONCE((v)->counter)
+
+/*
+ * arch_atomic_set - set atomic variable
+ * @v: pointer of type atomic_t
+ * @i: required value
+ *
+ * Atomically sets the value of @v to @i.
+ */
+#define arch_atomic_set(v, i)	WRITE_ONCE((v)->counter, (i))
+
+#define ATOMIC_OP(op, I, asm_op)					\
+static inline void arch_atomic_##op(int i, atomic_t *v)			\
+{									\
+	__asm__ __volatile__(						\
+	"am"#asm_op"_db.w" " $zero, %1, %0	\n"			\
+	: "+ZB" (v->counter)						\
+	: "r" (I)							\
+	: "memory");							\
+}
+
+#define ATOMIC_OP_RETURN(op, I, asm_op, c_op)				\
+static inline int arch_atomic_##op##_return_relaxed(int i, atomic_t *v)	\
+{									\
+	int result;							\
+									\
+	__asm__ __volatile__(						\
+	"am"#asm_op"_db.w" " %1, %2, %0		\n"			\
+	: "+ZB" (v->counter), "=&r" (result)				\
+	: "r" (I)							\
+	: "memory");							\
+									\
+	return result c_op I;						\
+}
+
+#define ATOMIC_FETCH_OP(op, I, asm_op)					\
+static inline int arch_atomic_fetch_##op##_relaxed(int i, atomic_t *v)	\
+{									\
+	int result;							\
+									\
+	__asm__ __volatile__(						\
+	"am"#asm_op"_db.w" " %1, %2, %0		\n"			\
+	: "+ZB" (v->counter), "=&r" (result)				\
+	: "r" (I)							\
+	: "memory");							\
+									\
+	return result;							\
+}
+
+#define ATOMIC_OPS(op, I, asm_op, c_op)					\
+	ATOMIC_OP(op, I, asm_op)					\
+	ATOMIC_OP_RETURN(op, I, asm_op, c_op)				\
+	ATOMIC_FETCH_OP(op, I, asm_op)
+
+ATOMIC_OPS(add, i, add, +)
+ATOMIC_OPS(sub, -i, add, +)
+
+#define arch_atomic_add_return_relaxed	arch_atomic_add_return_relaxed
+#define arch_atomic_sub_return_relaxed	arch_atomic_sub_return_relaxed
+#define arch_atomic_fetch_add_relaxed	arch_atomic_fetch_add_relaxed
+#define arch_atomic_fetch_sub_relaxed	arch_atomic_fetch_sub_relaxed
+
+#undef ATOMIC_OPS
+
+#define ATOMIC_OPS(op, I, asm_op)					\
+	ATOMIC_OP(op, I, asm_op)					\
+	ATOMIC_FETCH_OP(op, I, asm_op)
+
+ATOMIC_OPS(and, i, and)
+ATOMIC_OPS(or, i, or)
+ATOMIC_OPS(xor, i, xor)
+
+#define arch_atomic_fetch_and_relaxed	arch_atomic_fetch_and_relaxed
+#define arch_atomic_fetch_or_relaxed	arch_atomic_fetch_or_relaxed
+#define arch_atomic_fetch_xor_relaxed	arch_atomic_fetch_xor_relaxed
+
+#undef ATOMIC_OPS
+#undef ATOMIC_FETCH_OP
+#undef ATOMIC_OP_RETURN
+#undef ATOMIC_OP
+
+/*
+ * arch_atomic_sub_if_positive - conditionally subtract integer from atomic variable
+ * @i: integer value to subtract
+ * @v: pointer of type atomic_t
+ *
+ * Atomically test @v and subtract @i if @v is greater or equal than @i.
+ * The function returns the old value of @v minus @i.
+ */
+static inline int arch_atomic_sub_if_positive(int i, atomic_t *v)
+{
+	int result;
+	int temp;
+
+	if (__builtin_constant_p(i)) {
+		__asm__ __volatile__(
+		"1:	ll.w	%1, %2		# atomic_sub_if_positive\n"
+		"	addi.w	%0, %1, %3				\n"
+		"	or	%1, %0, $zero				\n"
+		"	blt	%0, $zero, 2f				\n"
+		"	sc.w	%1, %2					\n"
+		"	beq	$zero, %1, 1b				\n"
+		"2:							\n"
+		: "=&r" (result), "=&r" (temp),
+		  "+" GCC_OFF_SMALL_ASM() (v->counter)
+		: "I" (-i));
+	} else {
+		__asm__ __volatile__(
+		"1:	ll.w	%1, %2		# atomic_sub_if_positive\n"
+		"	sub.w	%0, %1, %3				\n"
+		"	or	%1, %0, $zero				\n"
+		"	blt	%0, $zero, 2f				\n"
+		"	sc.w	%1, %2					\n"
+		"	beq	$zero, %1, 1b				\n"
+		"2:							\n"
+		: "=&r" (result), "=&r" (temp),
+		  "+" GCC_OFF_SMALL_ASM() (v->counter)
+		: "r" (i));
+	}
+
+	return result;
+}
+
+#define arch_atomic_cmpxchg(v, o, n) (arch_cmpxchg(&((v)->counter), (o), (n)))
+#define arch_atomic_xchg(v, new) (arch_xchg(&((v)->counter), (new)))
+
+/*
+ * arch_atomic_dec_if_positive - decrement by 1 if old value positive
+ * @v: pointer of type atomic_t
+ */
+#define arch_atomic_dec_if_positive(v)	arch_atomic_sub_if_positive(1, v)
+
+#ifdef CONFIG_64BIT
+
+#define ATOMIC64_INIT(i)    { (i) }
+
+/*
+ * arch_atomic64_read - read atomic variable
+ * @v: pointer of type atomic64_t
+ *
+ */
+#define arch_atomic64_read(v)	READ_ONCE((v)->counter)
+
+/*
+ * arch_atomic64_set - set atomic variable
+ * @v: pointer of type atomic64_t
+ * @i: required value
+ */
+#define arch_atomic64_set(v, i)	WRITE_ONCE((v)->counter, (i))
+
+#define ATOMIC64_OP(op, I, asm_op)					\
+static inline void arch_atomic64_##op(long i, atomic64_t *v)		\
+{									\
+	__asm__ __volatile__(						\
+	"am"#asm_op"_db.d " " $zero, %1, %0	\n"			\
+	: "+ZB" (v->counter)						\
+	: "r" (I)							\
+	: "memory");							\
+}
+
+#define ATOMIC64_OP_RETURN(op, I, asm_op, c_op)					\
+static inline long arch_atomic64_##op##_return_relaxed(long i, atomic64_t *v)	\
+{										\
+	long result;								\
+	__asm__ __volatile__(							\
+	"am"#asm_op"_db.d " " %1, %2, %0		\n"			\
+	: "+ZB" (v->counter), "=&r" (result)					\
+	: "r" (I)								\
+	: "memory");								\
+										\
+	return result c_op I;							\
+}
+
+#define ATOMIC64_FETCH_OP(op, I, asm_op)					\
+static inline long arch_atomic64_fetch_##op##_relaxed(long i, atomic64_t *v)	\
+{										\
+	long result;								\
+										\
+	__asm__ __volatile__(							\
+	"am"#asm_op"_db.d " " %1, %2, %0		\n"			\
+	: "+ZB" (v->counter), "=&r" (result)					\
+	: "r" (I)								\
+	: "memory");								\
+										\
+	return result;								\
+}
+
+#define ATOMIC64_OPS(op, I, asm_op, c_op)				      \
+	ATOMIC64_OP(op, I, asm_op)					      \
+	ATOMIC64_OP_RETURN(op, I, asm_op, c_op)				      \
+	ATOMIC64_FETCH_OP(op, I, asm_op)
+
+ATOMIC64_OPS(add, i, add, +)
+ATOMIC64_OPS(sub, -i, add, +)
+
+#define arch_atomic64_add_return_relaxed	arch_atomic64_add_return_relaxed
+#define arch_atomic64_sub_return_relaxed	arch_atomic64_sub_return_relaxed
+#define arch_atomic64_fetch_add_relaxed		arch_atomic64_fetch_add_relaxed
+#define arch_atomic64_fetch_sub_relaxed		arch_atomic64_fetch_sub_relaxed
+
+#undef ATOMIC64_OPS
+
+#define ATOMIC64_OPS(op, I, asm_op)					      \
+	ATOMIC64_OP(op, I, asm_op)					      \
+	ATOMIC64_FETCH_OP(op, I, asm_op)
+
+ATOMIC64_OPS(and, i, and)
+ATOMIC64_OPS(or, i, or)
+ATOMIC64_OPS(xor, i, xor)
+
+#define arch_atomic64_fetch_and_relaxed	arch_atomic64_fetch_and_relaxed
+#define arch_atomic64_fetch_or_relaxed	arch_atomic64_fetch_or_relaxed
+#define arch_atomic64_fetch_xor_relaxed	arch_atomic64_fetch_xor_relaxed
+
+#undef ATOMIC64_OPS
+#undef ATOMIC64_FETCH_OP
+#undef ATOMIC64_OP_RETURN
+#undef ATOMIC64_OP
+
+/*
+ * arch_atomic64_sub_if_positive - conditionally subtract integer from atomic variable
+ * @i: integer value to subtract
+ * @v: pointer of type atomic64_t
+ *
+ * Atomically test @v and subtract @i if @v is greater or equal than @i.
+ * The function returns the old value of @v minus @i.
+ */
+static inline long arch_atomic64_sub_if_positive(long i, atomic64_t *v)
+{
+	long result;
+	long temp;
+
+	if (__builtin_constant_p(i)) {
+		__asm__ __volatile__(
+		"1:	ll.d	%1, %2	# atomic64_sub_if_positive	\n"
+		"	addi.d	%0, %1, %3				\n"
+		"	or	%1, %0, $zero				\n"
+		"	blt	%0, $zero, 2f				\n"
+		"	sc.d	%1, %2					\n"
+		"	beq	%1, $zero, 1b				\n"
+		"2:							\n"
+		: "=&r" (result), "=&r" (temp),
+		  "+" GCC_OFF_SMALL_ASM() (v->counter)
+		: "I" (-i));
+	} else {
+		__asm__ __volatile__(
+		"1:	ll.d	%1, %2	# atomic64_sub_if_positive	\n"
+		"	sub.d	%0, %1, %3				\n"
+		"	or	%1, %0, $zero				\n"
+		"	blt	%0, $zero, 2f				\n"
+		"	sc.d	%1, %2					\n"
+		"	beq	%1, $zero, 1b				\n"
+		"2:							\n"
+		: "=&r" (result), "=&r" (temp),
+		  "+" GCC_OFF_SMALL_ASM() (v->counter)
+		: "r" (i));
+	}
+
+	return result;
+}
+
+#define arch_atomic64_cmpxchg(v, o, n) \
+	((__typeof__((v)->counter))arch_cmpxchg(&((v)->counter), (o), (n)))
+#define arch_atomic64_xchg(v, new) (arch_xchg(&((v)->counter), (new)))
+
+/*
+ * arch_atomic64_dec_if_positive - decrement by 1 if old value positive
+ * @v: pointer of type atomic64_t
+ */
+#define arch_atomic64_dec_if_positive(v)	arch_atomic64_sub_if_positive(1, v)
+
+#endif /* CONFIG_64BIT */
+
+#endif /* _ASM_ATOMIC_H */
diff --git a/arch/loongarch/include/asm/barrier.h b/arch/loongarch/include/asm/barrier.h
new file mode 100644
index 000000000000..8ab8d8f15b88
--- /dev/null
+++ b/arch/loongarch/include/asm/barrier.h
@@ -0,0 +1,53 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_BARRIER_H
+#define __ASM_BARRIER_H
+
+#include <asm/addrspace.h>
+
+#define __sync()	__asm__ __volatile__("dbar 0" : : : "memory")
+
+#define fast_wmb()	__sync()
+#define fast_rmb()	__sync()
+#define fast_mb()	__sync()
+#define fast_iob()	__sync()
+#define wbflush()	__sync()
+
+#define wmb()		fast_wmb()
+#define rmb()		fast_rmb()
+#define mb()		fast_mb()
+#define iob()		fast_iob()
+
+/**
+ * array_index_mask_nospec() - generate a ~0 mask when index < size, 0 otherwise
+ * @index: array element index
+ * @size: number of elements in array
+ *
+ * Returns:
+ *     0 - (@index < @size)
+ */
+#define array_index_mask_nospec array_index_mask_nospec
+static inline unsigned long array_index_mask_nospec(unsigned long index,
+						    unsigned long size)
+{
+	unsigned long mask;
+
+	__asm__ __volatile__(
+		"sltu	%0, %1, %2\n\t"
+#if (_LOONGARCH_SZLONG == 32)
+		"sub.w	%0, $r0, %0\n\t"
+#elif (_LOONGARCH_SZLONG == 64)
+		"sub.d	%0, $r0, %0\n\t"
+#endif
+		: "=r" (mask)
+		: "r" (index), "r" (size)
+		:);
+
+	return mask;
+}
+
+#include <asm-generic/barrier.h>
+
+#endif /* __ASM_BARRIER_H */
diff --git a/arch/loongarch/include/asm/bitops.h b/arch/loongarch/include/asm/bitops.h
new file mode 100644
index 000000000000..7ab5664c3337
--- /dev/null
+++ b/arch/loongarch/include/asm/bitops.h
@@ -0,0 +1,253 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_BITOPS_H
+#define _ASM_BITOPS_H
+
+#ifndef _LINUX_BITOPS_H
+#error only <linux/bitops.h> can be included directly
+#endif
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+#include <asm/barrier.h>
+#include <asm/byteorder.h>
+#include <asm/compiler.h>
+#include <asm/cpu-features.h>
+
+#if _LOONGARCH_SZLONG == 32
+#define __LL		"ll.w	"
+#define __SC		"sc.w	"
+#define __AMADD		"amadd.w	"
+#define __AMAND_SYNC	"amand_db.w	"
+#define __AMOR_SYNC	"amor_db.w	"
+#define __AMXOR_SYNC	"amxor_db.w	"
+#elif _LOONGARCH_SZLONG == 64
+#define __LL		"ll.d	"
+#define __SC		"sc.d	"
+#define __AMADD		"amadd.d	"
+#define __AMAND_SYNC	"amand_db.d	"
+#define __AMOR_SYNC	"amor_db.d	"
+#define __AMXOR_SYNC	"amxor_db.d	"
+#endif
+
+/*
+ * set_bit - Atomically set a bit in memory
+ * @nr: the bit to set
+ * @addr: the address to start counting from
+ *
+ * This function is atomic and may not be reordered.  See __set_bit()
+ * if you do not require the atomic guarantees.
+ * Note that @nr may be almost arbitrarily large; this function is not
+ * restricted to acting on a single-word quantity.
+ */
+static inline void set_bit(unsigned long nr, volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMOR_SYNC "$zero, %1, %0        \n"
+	: "+ZB" (*m)
+	: "r" (1UL << bit)
+	: "memory");
+}
+
+/*
+ * clear_bit - Clears a bit in memory
+ * @nr: Bit to clear
+ * @addr: Address to start counting from
+ *
+ * clear_bit() is atomic and may not be reordered.  However, it does
+ * not contain a memory barrier, so if it is used for locking purposes,
+ * you should call smp_mb__before_atomic() and/or smp_mb__after_atomic()
+ * in order to ensure changes are visible on other processors.
+ */
+static inline void clear_bit(unsigned long nr, volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMAND_SYNC "$zero, %1, %0       \n"
+	: "+ZB" (*m)
+	: "r" (~(1UL << bit))
+	: "memory");
+}
+
+/*
+ * clear_bit_unlock - Clears a bit in memory
+ * @nr: Bit to clear
+ * @addr: Address to start counting from
+ *
+ * clear_bit() is atomic and implies release semantics before the memory
+ * operation. It can be used for an unlock.
+ */
+static inline void clear_bit_unlock(unsigned long nr, volatile unsigned long *addr)
+{
+	clear_bit(nr, addr);
+}
+
+/*
+ * change_bit - Toggle a bit in memory
+ * @nr: Bit to change
+ * @addr: Address to start counting from
+ *
+ * change_bit() is atomic and may not be reordered.
+ * Note that @nr may be almost arbitrarily large; this function is not
+ * restricted to acting on a single-word quantity.
+ */
+static inline void change_bit(unsigned long nr, volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMXOR_SYNC "$zero, %1, %0       \n"
+	: "+ZB" (*m)
+	: "r" (1UL << bit)
+	: "memory");
+}
+
+/*
+ * test_and_set_bit - Set a bit and return its old value
+ * @nr: Bit to set
+ * @addr: Address to count from
+ *
+ * This operation is atomic and cannot be reordered.
+ * It also implies a memory barrier.
+ */
+static inline int test_and_set_bit(unsigned long nr,
+	volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	unsigned long res;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMOR_SYNC "%1, %2, %0       \n"
+	: "+ZB" (*m), "=&r" (res)
+	: "r" (1UL << bit)
+	: "memory");
+
+	res = res & (1UL << bit);
+
+	return res != 0;
+}
+
+/*
+ * test_and_set_bit_lock - Set a bit and return its old value
+ * @nr: Bit to set
+ * @addr: Address to count from
+ *
+ * This operation is atomic and implies acquire ordering semantics
+ * after the memory operation.
+ */
+static inline int test_and_set_bit_lock(unsigned long nr,
+	volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	unsigned long res;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMOR_SYNC "%1, %2, %0       \n"
+	: "+ZB" (*m), "=&r" (res)
+	: "r" (1UL << bit)
+	: "memory");
+
+	res = res & (1UL << bit);
+
+	return res != 0;
+}
+/*
+ * test_and_clear_bit - Clear a bit and return its old value
+ * @nr: Bit to clear
+ * @addr: Address to count from
+ *
+ * This operation is atomic and cannot be reordered.
+ * It also implies a memory barrier.
+ */
+static inline int test_and_clear_bit(unsigned long nr,
+	volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	unsigned long res, temp;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMAND_SYNC "%1, %2, %0      \n"
+	: "+ZB" (*m), "=&r" (temp)
+	: "r" (~(1UL << bit))
+	: "memory");
+
+	res = temp & (1UL << bit);
+
+	return res != 0;
+}
+
+/*
+ * test_and_change_bit - Change a bit and return its old value
+ * @nr: Bit to change
+ * @addr: Address to count from
+ *
+ * This operation is atomic and cannot be reordered.
+ * It also implies a memory barrier.
+ */
+static inline int test_and_change_bit(unsigned long nr,
+	volatile unsigned long *addr)
+{
+	int bit = nr % BITS_PER_LONG;
+	unsigned long res;
+	volatile unsigned long *m = &addr[BIT_WORD(nr)];
+
+	__asm__ __volatile__(
+	"   " __AMXOR_SYNC "%1, %2, %0      \n"
+	: "+ZB" (*m), "=&r" (res)
+	: "r" (1UL << bit)
+	: "memory");
+
+	res = res & (1UL << bit);
+
+	return res != 0;
+}
+
+#include <asm-generic/bitops/non-atomic.h>
+
+/*
+ * __clear_bit_unlock - Clears a bit in memory
+ * @nr: Bit to clear
+ * @addr: Address to start counting from
+ *
+ * __clear_bit() is non-atomic and implies release semantics before the memory
+ * operation. It can be used for an unlock if no other CPUs can concurrently
+ * modify other bits in the word.
+ */
+static inline void __clear_bit_unlock(unsigned long nr, volatile unsigned long *addr)
+{
+	clear_bit(nr, addr);
+}
+
+#include <asm-generic/bitops/builtin-ffs.h>
+#include <asm-generic/bitops/builtin-fls.h>
+#include <asm-generic/bitops/builtin-__ffs.h>
+#include <asm-generic/bitops/builtin-__fls.h>
+
+#include <asm-generic/bitops/ffz.h>
+#include <asm-generic/bitops/fls64.h>
+#include <asm-generic/bitops/find.h>
+
+#ifdef __KERNEL__
+
+#include <asm-generic/bitops/sched.h>
+
+#include <asm-generic/bitops/arch_hweight.h>
+#include <asm-generic/bitops/const_hweight.h>
+
+#include <asm-generic/bitops/le.h>
+#include <asm-generic/bitops/ext2-atomic.h>
+
+#endif /* __KERNEL__ */
+
+#endif /* _ASM_BITOPS_H */
diff --git a/arch/loongarch/include/asm/bitrev.h b/arch/loongarch/include/asm/bitrev.h
new file mode 100644
index 000000000000..ed99da2027f2
--- /dev/null
+++ b/arch/loongarch/include/asm/bitrev.h
@@ -0,0 +1,34 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __LOONGARCH_ASM_BITREV_H__
+#define __LOONGARCH_ASM_BITREV_H__
+
+#include <linux/swab.h>
+
+static __always_inline __attribute_const__ u32 __arch_bitrev32(u32 x)
+{
+	u32 ret;
+
+	asm("bitrev.4b	%0, %1" : "=r"(ret) : "r"(__swab32(x)));
+	return ret;
+}
+
+static __always_inline __attribute_const__ u16 __arch_bitrev16(u16 x)
+{
+	u16 ret;
+
+	asm("bitrev.4b	%0, %1" : "=r"(ret) : "r"(__swab16(x)));
+	return ret;
+}
+
+static __always_inline __attribute_const__ u8 __arch_bitrev8(u8 x)
+{
+	u8 ret;
+
+	asm("bitrev.4b	%0, %1" : "=r"(ret) : "r"(x));
+	return ret;
+}
+
+#endif /* __LOONGARCH_ASM_BITREV_H__ */
diff --git a/arch/loongarch/include/asm/bootinfo.h b/arch/loongarch/include/asm/bootinfo.h
new file mode 100644
index 000000000000..41838633d1aa
--- /dev/null
+++ b/arch/loongarch/include/asm/bootinfo.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_BOOTINFO_H
+#define _ASM_BOOTINFO_H
+
+#include <linux/types.h>
+#include <asm/setup.h>
+
+const char *get_system_type(void);
+
+extern void early_memblock_init(void);
+extern void detect_memory_region(phys_addr_t start, phys_addr_t sz_min,  phys_addr_t sz_max);
+
+extern void early_init(void);
+extern void platform_init(void);
+
+extern void free_init_pages(const char *what, unsigned long begin, unsigned long end);
+
+/*
+ * Initial kernel command line, usually setup by fw_init_cmdline()
+ */
+extern char arcs_cmdline[COMMAND_LINE_SIZE];
+
+/*
+ * Registers a0, a1, a3 and a4 as passed to the kernel entry by firmware
+ */
+extern unsigned long fw_arg0, fw_arg1, fw_arg2, fw_arg3;
+
+extern unsigned long initrd_start, initrd_end;
+
+#endif /* _ASM_BOOTINFO_H */
diff --git a/arch/loongarch/include/asm/bugs.h b/arch/loongarch/include/asm/bugs.h
new file mode 100644
index 000000000000..7d4bcbfc49b8
--- /dev/null
+++ b/arch/loongarch/include/asm/bugs.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * This is included by init/main.c to check for architecture-dependent bugs.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_BUGS_H
+#define _ASM_BUGS_H
+
+#include <linux/bug.h>
+#include <linux/delay.h>
+#include <linux/smp.h>
+
+#include <asm/cpu.h>
+#include <asm/cpu-info.h>
+
+static inline void check_bugs(void)
+{
+	unsigned int cpu = smp_processor_id();
+
+	cpu_data[cpu].udelay_val = loops_per_jiffy;
+}
+
+#endif /* _ASM_BUGS_H */
diff --git a/arch/loongarch/include/asm/clocksource.h b/arch/loongarch/include/asm/clocksource.h
new file mode 100644
index 000000000000..d3d943b7c913
--- /dev/null
+++ b/arch/loongarch/include/asm/clocksource.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Author: Huacai Chen <chenhuacai@loongson.cn>
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+
+#ifndef __ASM_CLOCKSOURCE_H
+#define __ASM_CLOCKSOURCE_H
+
+#include <asm/vdso/clocksource.h>
+
+#endif /* __ASM_CLOCKSOURCE_H */
diff --git a/arch/loongarch/include/asm/compiler.h b/arch/loongarch/include/asm/compiler.h
new file mode 100644
index 000000000000..492def19ddee
--- /dev/null
+++ b/arch/loongarch/include/asm/compiler.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_COMPILER_H
+#define _ASM_COMPILER_H
+
+#define GCC_OFF_SMALL_ASM() "R"
+
+#define LOONGARCH_ISA_LEVEL "loongarch"
+#define LOONGARCH_ISA_ARCH_LEVEL "arch=loongarch"
+#define LOONGARCH_ISA_LEVEL_RAW loongarch
+#define LOONGARCH_ISA_ARCH_LEVEL_RAW LOONGARCH_ISA_LEVEL_RAW
+
+#endif /* _ASM_COMPILER_H */
diff --git a/arch/loongarch/include/asm/cpu-features.h b/arch/loongarch/include/asm/cpu-features.h
new file mode 100644
index 000000000000..2d307f30e3c5
--- /dev/null
+++ b/arch/loongarch/include/asm/cpu-features.h
@@ -0,0 +1,67 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_CPU_FEATURES_H
+#define __ASM_CPU_FEATURES_H
+
+#include <asm/cpu.h>
+#include <asm/cpu-info.h>
+
+#define cpu_opt(opt)			(cpu_data[0].options & (opt))
+#define cpu_has(feat)			(cpu_data[0].options & BIT_ULL(feat))
+
+#define cpu_has_loongarch		(cpu_has_loongarch32 | cpu_has_loongarch64)
+#define cpu_has_loongarch32		(cpu_data[0].isa_level & LOONGARCH_CPU_ISA_32BIT)
+#define cpu_has_loongarch64		(cpu_data[0].isa_level & LOONGARCH_CPU_ISA_64BIT)
+
+#define cpu_icache_line_size()		cpu_data[0].icache.linesz
+#define cpu_dcache_line_size()		cpu_data[0].dcache.linesz
+#define cpu_vcache_line_size()		cpu_data[0].vcache.linesz
+#define cpu_scache_line_size()		cpu_data[0].scache.linesz
+
+#ifdef CONFIG_32BIT
+# define cpu_has_64bits			(cpu_data[0].isa_level & LOONGARCH_CPU_ISA_64BIT)
+# define cpu_vabits			31
+# define cpu_pabits			31
+#endif
+
+#ifdef CONFIG_64BIT
+# define cpu_has_64bits			1
+# define cpu_vabits			cpu_data[0].vabits
+# define cpu_pabits			cpu_data[0].pabits
+# define __NEED_ADDRBITS_PROBE
+#endif
+
+/*
+ * SMP assumption: Options of CPU 0 are a superset of all processors.
+ * This is true for all known LoongArch systems.
+ */
+#define cpu_has_cpucfg		cpu_opt(LOONGARCH_CPU_CPUCFG)
+#define cpu_has_lam		cpu_opt(LOONGARCH_CPU_LAM)
+#define cpu_has_ual		cpu_opt(LOONGARCH_CPU_UAL)
+#define cpu_has_fpu		cpu_opt(LOONGARCH_CPU_FPU)
+#define cpu_has_lsx		cpu_opt(LOONGARCH_CPU_LSX)
+#define cpu_has_lasx		cpu_opt(LOONGARCH_CPU_LASX)
+#define cpu_has_complex		cpu_opt(LOONGARCH_CPU_COMPLEX)
+#define cpu_has_crypto		cpu_opt(LOONGARCH_CPU_CRYPTO)
+#define cpu_has_lvz		cpu_opt(LOONGARCH_CPU_LVZ)
+#define cpu_has_lbt_x86		cpu_opt(LOONGARCH_CPU_LBT_X86)
+#define cpu_has_lbt_arm		cpu_opt(LOONGARCH_CPU_LBT_ARM)
+#define cpu_has_lbt_mips	cpu_opt(LOONGARCH_CPU_LBT_MIPS)
+#define cpu_has_lbt		(cpu_has_lbt_x86|cpu_has_lbt_arm|cpu_has_lbt_mips)
+#define cpu_has_csr		cpu_opt(LOONGARCH_CPU_CSR)
+#define cpu_has_tlb		cpu_opt(LOONGARCH_CPU_TLB)
+#define cpu_has_watch		cpu_opt(LOONGARCH_CPU_WATCH)
+#define cpu_has_vint		cpu_opt(LOONGARCH_CPU_VINT)
+#define cpu_has_csripi		cpu_opt(LOONGARCH_CPU_CSRIPI)
+#define cpu_has_extioi		cpu_opt(LOONGARCH_CPU_EXTIOI)
+#define cpu_has_prefetch	cpu_opt(LOONGARCH_CPU_PREFETCH)
+#define cpu_has_pmp		cpu_opt(LOONGARCH_CPU_PMP)
+#define cpu_has_perf		cpu_opt(LOONGARCH_CPU_PMP)
+#define cpu_has_scalefreq	cpu_opt(LOONGARCH_CPU_SCALEFREQ)
+#define cpu_has_guestid		cpu_opt(LOONGARCH_CPU_GUESTID)
+#define cpu_has_hypervisor	cpu_opt(LOONGARCH_CPU_HYPERVISOR)
+
+
+#endif /* __ASM_CPU_FEATURES_H */
diff --git a/arch/loongarch/include/asm/cpu-info.h b/arch/loongarch/include/asm/cpu-info.h
new file mode 100644
index 000000000000..ef1624286b96
--- /dev/null
+++ b/arch/loongarch/include/asm/cpu-info.h
@@ -0,0 +1,138 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_CPU_INFO_H
+#define __ASM_CPU_INFO_H
+
+#include <linux/cache.h>
+#include <linux/types.h>
+
+#include <asm/loongarchregs.h>
+
+/*
+ * Descriptor for a cache
+ */
+struct cache_desc {
+	unsigned int waysize;	/* Bytes per way */
+	unsigned short sets;	/* Number of lines per set */
+	unsigned char ways;	/* Number of ways */
+	unsigned char linesz;	/* Size of line in bytes */
+	unsigned char waybit;	/* Bits to select in a cache set */
+	unsigned char flags;	/* Flags describing cache properties */
+};
+
+struct cpuinfo_loongarch {
+	u64			asid_cache;
+	unsigned long		asid_mask;
+
+	/*
+	 * Capability and feature descriptor structure for LoongArch CPU
+	 */
+	unsigned long		ases;
+	unsigned long long	options;
+	unsigned int		udelay_val;
+	unsigned int		processor_id;
+	unsigned int		fpu_vers;
+	unsigned int		fpu_csr0;
+	unsigned int		fpu_mask;
+	unsigned int		cputype;
+	int			isa_level;
+	int			tlbsize;
+	int			tlbsizemtlb;
+	int			tlbsizestlbsets;
+	int			tlbsizestlbways;
+	struct cache_desc	icache; /* Primary I-cache */
+	struct cache_desc	dcache; /* Primary D or combined I/D cache */
+	struct cache_desc	vcache; /* Victim cache, between pcache and scache */
+	struct cache_desc	scache; /* Secondary cache */
+	struct cache_desc	tcache; /* Tertiary/split secondary cache */
+	int			package;/* physical package number */
+	unsigned int		globalnumber;
+	int			vabits; /* Virtual Address size in bits */
+	int			pabits; /* Physical Address size in bits */
+	void			*data;	/* Additional data */
+	unsigned int		watch_dreg_count;   /* Number data breakpoints */
+	unsigned int		watch_ireg_count;   /* Number instruction breakpoints */
+	unsigned int		watch_reg_use_cnt; /* min(NUM_WATCH_REGS, watch_dreg_count + watch_ireg_count), Usable by ptrace */
+	unsigned int		kscratch_mask; /* Usable KScratch mask. */
+} __aligned(SMP_CACHE_BYTES);
+
+extern struct cpuinfo_loongarch cpu_data[];
+#define boot_cpu_data cpu_data[0]
+#define current_cpu_data cpu_data[smp_processor_id()]
+#define raw_current_cpu_data cpu_data[raw_smp_processor_id()]
+
+extern void cpu_probe(void);
+extern void cpu_report(void);
+
+extern const char *__cpu_family[];
+extern const char *__cpu_full_name[];
+#define cpu_family_string()	__cpu_family[raw_smp_processor_id()]
+#define cpu_full_name_string()	__cpu_full_name[raw_smp_processor_id()]
+
+struct seq_file;
+struct notifier_block;
+
+extern int register_proc_cpuinfo_notifier(struct notifier_block *nb);
+extern int proc_cpuinfo_notifier_call_chain(unsigned long val, void *v);
+
+#define proc_cpuinfo_notifier(fn, pri)					\
+({									\
+	static struct notifier_block fn##_nb = {			\
+		.notifier_call = fn,					\
+		.priority = pri						\
+	};								\
+									\
+	register_proc_cpuinfo_notifier(&fn##_nb);			\
+})
+
+struct proc_cpuinfo_notifier_args {
+	struct seq_file *m;
+	unsigned long n;
+};
+
+static inline unsigned int cpu_cluster(struct cpuinfo_loongarch *cpuinfo)
+{
+	return (cpuinfo->globalnumber & LOONGARCH_GLOBALNUMBER_CLUSTER) >>
+		LOONGARCH_GLOBALNUMBER_CLUSTER_SHF;
+}
+
+static inline unsigned int cpu_core(struct cpuinfo_loongarch *cpuinfo)
+{
+	return (cpuinfo->globalnumber & LOONGARCH_GLOBALNUMBER_CORE) >>
+		LOONGARCH_GLOBALNUMBER_CORE_SHF;
+}
+
+extern void cpu_set_cluster(struct cpuinfo_loongarch *cpuinfo, unsigned int cluster);
+extern void cpu_set_core(struct cpuinfo_loongarch *cpuinfo, unsigned int core);
+
+static inline bool cpus_are_siblings(int cpua, int cpub)
+{
+	struct cpuinfo_loongarch *infoa = &cpu_data[cpua];
+	struct cpuinfo_loongarch *infob = &cpu_data[cpub];
+	unsigned int gnuma, gnumb;
+
+	if (infoa->package != infob->package)
+		return false;
+
+	gnuma = infoa->globalnumber & ~LOONGARCH_GLOBALNUMBER_VP;
+	gnumb = infob->globalnumber & ~LOONGARCH_GLOBALNUMBER_VP;
+	if (gnuma != gnumb)
+		return false;
+
+	return true;
+}
+
+static inline unsigned long cpu_asid_mask(struct cpuinfo_loongarch *cpuinfo)
+{
+	return cpuinfo->asid_mask;
+}
+
+static inline void set_cpu_asid_mask(struct cpuinfo_loongarch *cpuinfo,
+				     unsigned long asid_mask)
+{
+	cpuinfo->asid_mask = asid_mask;
+}
+
+#endif /* __ASM_CPU_INFO_H */
diff --git a/arch/loongarch/include/asm/cpu.h b/arch/loongarch/include/asm/cpu.h
new file mode 100644
index 000000000000..c5fbe2a0716a
--- /dev/null
+++ b/arch/loongarch/include/asm/cpu.h
@@ -0,0 +1,123 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * cpu.h: Values of the PRId register used to match up
+ *	  various LoongArch cpu types.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_CPU_H
+#define _ASM_CPU_H
+
+/*
+ * As of the LoongArch specs from Loongson Technology, the PRId register
+ * (CPUCFG.00) is defined in this (backwards compatible) way:
+ *
+ * +----------------+----------------+----------------+----------------+
+ * | Reserved       | Company ID	    | Processor ID   | Revision	      |
+ * +----------------+----------------+----------------+----------------+
+ *  31		 24 23		  16 15		    8 7              0
+ *
+ */
+
+/*
+ * Assigned Company values for bits 23:16 of the PRId register.
+ */
+
+#define PRID_COMP_MASK		0xff0000
+
+#define PRID_COMP_LOONGSON	0x140000
+
+/*
+ * Assigned Processor ID (implementation) values for bits 15:8 of the PRId
+ * register.  In order to detect a certain CPU type exactly eventually
+ * additional registers may need to be examined.
+ */
+
+#define PRID_IMP_MASK		0xff00
+
+#define PRID_IMP_LOONGSON_32	0x4200  /* Loongson 32bit */
+#define PRID_IMP_LOONGSON_64R	0x6100  /* Reduced Loongson 64bit */
+#define PRID_IMP_LOONGSON_64C	0x6300  /* Classic Loongson 64bit */
+#define PRID_IMP_LOONGSON_64G	0xc000  /* Generic Loongson 64bit */
+#define PRID_IMP_UNKNOWN	0xff00
+
+/*
+ * Particular Revision values for bits 7:0 of the PRId register.
+ */
+
+#define PRID_REV_MASK		0x00ff
+
+#if !defined(__ASSEMBLY__)
+
+enum cpu_type_enum {
+	CPU_UNKNOWN,
+	CPU_LOONGSON32,
+	CPU_LOONGSON64,
+	CPU_LAST
+};
+
+#endif /* !__ASSEMBLY */
+
+/*
+ * ISA Level encodings
+ *
+ */
+
+#define LOONGARCH_CPU_ISA_LA32R 0x00000001
+#define LOONGARCH_CPU_ISA_LA32S 0x00000002
+#define LOONGARCH_CPU_ISA_LA64  0x00000004
+
+#define LOONGARCH_CPU_ISA_32BIT (LOONGARCH_CPU_ISA_LA32R | LOONGARCH_CPU_ISA_LA32S)
+#define LOONGARCH_CPU_ISA_64BIT LOONGARCH_CPU_ISA_LA64
+
+/*
+ * CPU Option encodings
+ */
+#define CPU_FEATURE_CPUCFG		0	/* CPU has CPUCFG */
+#define CPU_FEATURE_LAM			1	/* CPU has Atomic instructions */
+#define CPU_FEATURE_UAL			2	/* CPU has Unaligned Access support */
+#define CPU_FEATURE_FPU			3	/* CPU has FPU */
+#define CPU_FEATURE_LSX			4	/* CPU has 128bit SIMD instructions */
+#define CPU_FEATURE_LASX		5	/* CPU has 256bit SIMD instructions */
+#define CPU_FEATURE_COMPLEX		6	/* CPU has Complex instructions */
+#define CPU_FEATURE_CRYPTO		7	/* CPU has Crypto instructions */
+#define CPU_FEATURE_LVZ			8	/* CPU has Virtualization extension */
+#define CPU_FEATURE_LBT_X86		9	/* CPU has X86 Binary Translation */
+#define CPU_FEATURE_LBT_ARM		10	/* CPU has ARM Binary Translation */
+#define CPU_FEATURE_LBT_MIPS		11	/* CPU has MIPS Binary Translation */
+#define CPU_FEATURE_TLB			12	/* CPU has TLB */
+#define CPU_FEATURE_CSR			13	/* CPU has CSR feature */
+#define CPU_FEATURE_WATCH		14	/* CPU has watchpoint registers */
+#define CPU_FEATURE_VINT		15	/* CPU has vectored interrupts */
+#define CPU_FEATURE_CSRIPI		16	/* CPU has CSR-IPI */
+#define CPU_FEATURE_EXTIOI		17	/* CPU has EXT-IOI */
+#define CPU_FEATURE_PREFETCH		18	/* CPU has prefetch instructions */
+#define CPU_FEATURE_PMP			19	/* CPU has perfermance counter */
+#define CPU_FEATURE_SCALEFREQ		20	/* CPU support scale cpufreq */
+#define CPU_FEATURE_GUESTID		21	/* CPU has GuestID feature */
+#define CPU_FEATURE_HYPERVISOR		22	/* CPU has hypervisor (run in VM) */
+
+#define LOONGARCH_CPU_CPUCFG		BIT_ULL(CPU_FEATURE_CPUCFG)
+#define LOONGARCH_CPU_LAM		BIT_ULL(CPU_FEATURE_LAM)
+#define LOONGARCH_CPU_UAL		BIT_ULL(CPU_FEATURE_UAL)
+#define LOONGARCH_CPU_FPU		BIT_ULL(CPU_FEATURE_FPU)
+#define LOONGARCH_CPU_LSX		BIT_ULL(CPU_FEATURE_LSX)
+#define LOONGARCH_CPU_LASX		BIT_ULL(CPU_FEATURE_LASX)
+#define LOONGARCH_CPU_COMPLEX		BIT_ULL(CPU_FEATURE_COMPLEX)
+#define LOONGARCH_CPU_CRYPTO		BIT_ULL(CPU_FEATURE_CRYPTO)
+#define LOONGARCH_CPU_LVZ		BIT_ULL(CPU_FEATURE_LVZ)
+#define LOONGARCH_CPU_LBT_X86		BIT_ULL(CPU_FEATURE_LBT_X86)
+#define LOONGARCH_CPU_LBT_ARM		BIT_ULL(CPU_FEATURE_LBT_ARM)
+#define LOONGARCH_CPU_LBT_MIPS		BIT_ULL(CPU_FEATURE_LBT_MIPS)
+#define LOONGARCH_CPU_TLB		BIT_ULL(CPU_FEATURE_TLB)
+#define LOONGARCH_CPU_CSR		BIT_ULL(CPU_FEATURE_CSR)
+#define LOONGARCH_CPU_WATCH		BIT_ULL(CPU_FEATURE_WATCH)
+#define LOONGARCH_CPU_VINT		BIT_ULL(CPU_FEATURE_VINT)
+#define LOONGARCH_CPU_CSRIPI		BIT_ULL(CPU_FEATURE_CSRIPI)
+#define LOONGARCH_CPU_EXTIOI		BIT_ULL(CPU_FEATURE_EXTIOI)
+#define LOONGARCH_CPU_PREFETCH		BIT_ULL(CPU_FEATURE_PREFETCH)
+#define LOONGARCH_CPU_PMP		BIT_ULL(CPU_FEATURE_PMP)
+#define LOONGARCH_CPU_SCALEFREQ		BIT_ULL(CPU_FEATURE_SCALEFREQ)
+#define LOONGARCH_CPU_GUESTID		BIT_ULL(CPU_FEATURE_GUESTID)
+#define LOONGARCH_CPU_HYPERVISOR	BIT_ULL(CPU_FEATURE_HYPERVISOR)
+#endif /* _ASM_CPU_H */
diff --git a/arch/loongarch/include/asm/device.h b/arch/loongarch/include/asm/device.h
new file mode 100644
index 000000000000..75693eeba5c6
--- /dev/null
+++ b/arch/loongarch/include/asm/device.h
@@ -0,0 +1,17 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Arch specific extensions to struct device
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_LOONGARCH_DEVICE_H
+#define _ASM_LOONGARCH_DEVICE_H
+
+struct dev_archdata {
+	unsigned long dma_attrs;
+};
+
+struct pdev_archdata {
+};
+
+#endif /* _ASM_LOONGARCH_DEVICE_H*/
diff --git a/arch/loongarch/include/asm/dma-direct.h b/arch/loongarch/include/asm/dma-direct.h
new file mode 100644
index 000000000000..5fdc1988c34b
--- /dev/null
+++ b/arch/loongarch/include/asm/dma-direct.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _LOONGARCH_DMA_DIRECT_H
+#define _LOONGARCH_DMA_DIRECT_H
+
+dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr);
+phys_addr_t dma_to_phys(struct device *dev, dma_addr_t daddr);
+
+#endif /* _LOONGARCH_DMA_DIRECT_H */
diff --git a/arch/loongarch/include/asm/dma.h b/arch/loongarch/include/asm/dma.h
new file mode 100644
index 000000000000..a8a58dc93422
--- /dev/null
+++ b/arch/loongarch/include/asm/dma.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_DMA_H
+#define __ASM_DMA_H
+
+#define MAX_DMA_ADDRESS	PAGE_OFFSET
+#define MAX_DMA32_PFN	(1UL << (32 - PAGE_SHIFT))
+
+extern int isa_dma_bridge_buggy;
+
+#endif
diff --git a/arch/loongarch/include/asm/dmi.h b/arch/loongarch/include/asm/dmi.h
new file mode 100644
index 000000000000..fbe98f0e56e0
--- /dev/null
+++ b/arch/loongarch/include/asm/dmi.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_DMI_H
+#define _ASM_DMI_H
+
+#include <linux/io.h>
+#include <linux/memblock.h>
+
+#define dmi_early_remap(x, l)	dmi_remap(x, l)
+#define dmi_early_unmap(x, l)	dmi_unmap(x)
+#define dmi_alloc(l)		memblock_alloc_low(l, PAGE_SIZE)
+
+static inline void *dmi_remap(u64 phys_addr, unsigned long size)
+{
+	return ((void *)TO_CAC(phys_addr));
+}
+
+static inline void dmi_unmap(void *addr)
+{
+}
+
+#endif /* _ASM_DMI_H */
diff --git a/arch/loongarch/include/asm/efi.h b/arch/loongarch/include/asm/efi.h
new file mode 100644
index 000000000000..c1a826ff9cd3
--- /dev/null
+++ b/arch/loongarch/include/asm/efi.h
@@ -0,0 +1,35 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_LOONGARCH_EFI_H
+#define _ASM_LOONGARCH_EFI_H
+
+#include <linux/efi.h>
+
+extern void __init efi_init(void);
+extern void __init efi_runtime_init(void);
+
+#define ARCH_EFI_IRQ_FLAGS_MASK  0x00000001  /*bit0: CP0 Status.IE*/
+
+static inline void efifb_setup_from_dmi(struct screen_info *si, const char *opt)
+{
+}
+
+#define arch_efi_call_virt_setup()               \
+({                                               \
+})
+
+#define arch_efi_call_virt(p, f, args...)        \
+({                                               \
+	efi_##f##_t * __f;                       \
+	__f = p->f;                              \
+	__f(args);                               \
+})
+
+#define arch_efi_call_virt_teardown()            \
+({                                               \
+})
+
+
+#endif /* _ASM_LOONGARCH_EFI_H */
diff --git a/arch/loongarch/include/asm/fpregdef.h b/arch/loongarch/include/asm/fpregdef.h
new file mode 100644
index 000000000000..17b43c28cd6b
--- /dev/null
+++ b/arch/loongarch/include/asm/fpregdef.h
@@ -0,0 +1,55 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Definitions for the FPU register names
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_FPREGDEF_H
+#define _ASM_FPREGDEF_H
+
+#include <asm/abidefs.h>
+
+#if _LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64
+
+#define fv0	$f0	/* return value */
+#define fv1	$f2
+#define fa0	$f12	/* argument registers */
+#define fa1	$f13
+#define fa2	$f14
+#define fa3	$f15
+#define fa4	$f16
+#define fa5	$f17
+#define fa6	$f18
+#define fa7	$f19
+#define ft0	$f4	/* caller saved */
+#define ft1	$f5
+#define ft2	$f6
+#define ft3	$f7
+#define ft4	$f8
+#define ft5	$f9
+#define ft6	$f10
+#define ft7	$f11
+#define ft8	$f20
+#define ft9	$f21
+#define ft10	$f22
+#define ft11	$f23
+#define ft12	$f1
+#define ft13	$f3
+#define fs0	$f24	/* callee saved */
+#define fs1	$f25
+#define fs2	$f26
+#define fs3	$f27
+#define fs4	$f28
+#define fs5	$f29
+#define fs6	$f30
+#define fs7	$f31
+
+#endif /* _LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64 */
+
+#define fcsr0	$r0
+#define fcsr1	$r1
+#define fcsr2	$r2
+#define fcsr3	$r3
+#define vcsr16	$r16
+
+#endif /* _ASM_FPREGDEF_H */
diff --git a/arch/loongarch/include/asm/fpu.h b/arch/loongarch/include/asm/fpu.h
new file mode 100644
index 000000000000..d64afd9383e9
--- /dev/null
+++ b/arch/loongarch/include/asm/fpu.h
@@ -0,0 +1,135 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Author: Huacai Chen <chenhuacai@loongson.cn>
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_FPU_H
+#define _ASM_FPU_H
+
+#include <linux/sched.h>
+#include <linux/sched/task_stack.h>
+#include <linux/ptrace.h>
+#include <linux/thread_info.h>
+#include <linux/bitops.h>
+
+#include <asm/cpu.h>
+#include <asm/cpu-features.h>
+#include <asm/current.h>
+#include <asm/inst.h>
+#include <asm/loongarchregs.h>
+#include <asm/ptrace.h>
+#include <asm/processor.h>
+
+struct sigcontext;
+
+extern void _init_fpu(unsigned int);
+extern void _save_fp(struct loongarch_fpu *);
+extern void _restore_fp(struct loongarch_fpu *);
+
+/*
+ * Mask the FCSR Cause bits according to the Enable bits, observing
+ * that Unimplemented is always enabled.
+ */
+static inline unsigned long mask_fcsr_x(unsigned long fcsr)
+{
+	return fcsr & ((fcsr & FPU_CSR_ALL_E) <<
+			(ffs(FPU_CSR_ALL_X) - ffs(FPU_CSR_ALL_E)));
+}
+
+static inline int is_fp_enabled(void)
+{
+	return (csr_readl(LOONGARCH_CSR_EUEN) & CSR_EUEN_FPEN) ?
+		1 : 0;
+}
+
+#define enable_fpu()		set_csr_euen(CSR_EUEN_FPEN)
+
+#define disable_fpu()		clear_csr_euen(CSR_EUEN_FPEN)
+
+#define clear_fpu_owner()	clear_thread_flag(TIF_USEDFPU)
+
+static inline int __is_fpu_owner(void)
+{
+	return test_thread_flag(TIF_USEDFPU);
+}
+
+static inline int is_fpu_owner(void)
+{
+	return cpu_has_fpu && __is_fpu_owner();
+}
+
+static inline void __own_fpu(void)
+{
+	enable_fpu();
+	set_thread_flag(TIF_USEDFPU);
+	KSTK_EUEN(current) |= CSR_EUEN_FPEN;
+}
+
+static inline void own_fpu_inatomic(int restore)
+{
+	if (cpu_has_fpu && !__is_fpu_owner()) {
+		__own_fpu();
+		if (restore)
+			_restore_fp(&current->thread.fpu);
+	}
+}
+
+static inline void own_fpu(int restore)
+{
+	preempt_disable();
+	own_fpu_inatomic(restore);
+	preempt_enable();
+}
+
+static inline void lose_fpu_inatomic(int save, struct task_struct *tsk)
+{
+	if (is_fpu_owner()) {
+		if (save)
+			_save_fp(&tsk->thread.fpu);
+		disable_fpu();
+		clear_tsk_thread_flag(tsk, TIF_USEDFPU);
+	}
+	KSTK_EUEN(tsk) = 0;
+}
+
+static inline void lose_fpu(int save)
+{
+	preempt_disable();
+	lose_fpu_inatomic(save, current);
+	preempt_enable();
+}
+
+static inline void init_fpu(void)
+{
+	unsigned int fcsr = current->thread.fpu.fcsr;
+
+	__own_fpu();
+	_init_fpu(fcsr);
+	set_used_math();
+}
+
+static inline void save_fp(struct task_struct *tsk)
+{
+	if (cpu_has_fpu)
+		_save_fp(&tsk->thread.fpu);
+}
+
+static inline void restore_fp(struct task_struct *tsk)
+{
+	if (cpu_has_fpu)
+		_restore_fp(&tsk->thread.fpu);
+}
+
+static inline union fpureg *get_fpu_regs(struct task_struct *tsk)
+{
+	if (tsk == current) {
+		preempt_disable();
+		if (is_fpu_owner())
+			_save_fp(&current->thread.fpu);
+		preempt_enable();
+	}
+
+	return tsk->thread.fpu.fpr;
+}
+
+#endif /* _ASM_FPU_H */
diff --git a/arch/loongarch/include/asm/fw.h b/arch/loongarch/include/asm/fw.h
new file mode 100644
index 000000000000..8a0e8e7c625f
--- /dev/null
+++ b/arch/loongarch/include/asm/fw.h
@@ -0,0 +1,18 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_FW_H_
+#define __ASM_FW_H_
+
+#include <asm/bootinfo.h>
+
+extern int fw_argc;
+extern long *_fw_argv, *_fw_envp;
+
+#define fw_argv(index)		((char *)(long)_fw_argv[(index)])
+#define fw_envp(index)		((char *)(long)_fw_envp[(index)])
+
+extern void fw_init_cmdline(void);
+
+#endif /* __ASM_FW_H_ */
diff --git a/arch/loongarch/include/asm/inst.h b/arch/loongarch/include/asm/inst.h
new file mode 100644
index 000000000000..c8e6ff3508c3
--- /dev/null
+++ b/arch/loongarch/include/asm/inst.h
@@ -0,0 +1,249 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Format of an instruction in memory.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_INST_H
+#define _ASM_INST_H
+
+#include <asm/asm.h>
+#include <uapi/asm/inst.h>
+
+/* In case some other massaging is needed, keep LOONGARCHInst as wrapper */
+
+#define LOONGARCHInst(x) x
+
+#define I_OPCODE_SFT	26
+#define LOONGARCHInst_OPCODE(x) (LOONGARCHInst(x) >> I_OPCODE_SFT)
+
+#define I_JTARGET_SFT	0
+#define LOONGARCHInst_JTARGET(x) (LOONGARCHInst(x) & 0x03ffffff)
+
+#define I_RS_SFT	21
+#define LOONGARCHInst_RS(x) ((LOONGARCHInst(x) & 0x03e00000) >> I_RS_SFT)
+
+#define I_RT_SFT	16
+#define LOONGARCHInst_RT(x) ((LOONGARCHInst(x) & 0x001f0000) >> I_RT_SFT)
+
+#define I_IMM_SFT	0
+#define LOONGARCHInst_SIMM(x) ((int)((short)(LOONGARCHInst(x) & 0xffff)))
+#define LOONGARCHInst_UIMM(x) (LOONGARCHInst(x) & 0xffff)
+
+#define I_CACHEOP_SFT	18
+#define LOONGARCHInst_CACHEOP(x) ((LOONGARCHInst(x) & 0x001c0000) >> I_CACHEOP_SFT)
+
+#define I_CACHESEL_SFT	16
+#define LOONGARCHInst_CACHESEL(x) ((LOONGARCHInst(x) & 0x00030000) >> I_CACHESEL_SFT)
+
+#define I_RD_SFT	11
+#define LOONGARCHInst_RD(x) ((LOONGARCHInst(x) & 0x0000f800) >> I_RD_SFT)
+
+#define I_RE_SFT	6
+#define LOONGARCHInst_RE(x) ((LOONGARCHInst(x) & 0x000007c0) >> I_RE_SFT)
+
+#define I_FUNC_SFT	0
+#define LOONGARCHInst_FUNC(x) (LOONGARCHInst(x) & 0x0000003f)
+
+#define I_FFMT_SFT	21
+#define LOONGARCHInst_FFMT(x) ((LOONGARCHInst(x) & 0x01e00000) >> I_FFMT_SFT)
+
+#define I_FT_SFT	16
+#define LOONGARCHInst_FT(x) ((LOONGARCHInst(x) & 0x001f0000) >> I_FT_SFT)
+
+#define I_FS_SFT	11
+#define LOONGARCHInst_FS(x) ((LOONGARCHInst(x) & 0x0000f800) >> I_FS_SFT)
+
+#define I_FD_SFT	6
+#define LOONGARCHInst_FD(x) ((LOONGARCHInst(x) & 0x000007c0) >> I_FD_SFT)
+
+#define I_FR_SFT	21
+#define LOONGARCHInst_FR(x) ((LOONGARCHInst(x) & 0x03e00000) >> I_FR_SFT)
+
+#define I_FMA_FUNC_SFT	2
+#define LOONGARCHInst_FMA_FUNC(x) ((LOONGARCHInst(x) & 0x0000003c) >> I_FMA_FUNC_SFT)
+
+#define I_FMA_FFMT_SFT	0
+#define LOONGARCHInst_FMA_FFMT(x) (LOONGARCHInst(x) & 0x00000003)
+
+typedef unsigned int loongarch_instruction;
+
+/* Recode table from 16-bit register notation to 32-bit GPR. Do NOT export!!! */
+extern const int reg16to32[];
+
+#define STR(x)	__STR(x)
+#define __STR(x)  #x
+
+#define _LoadHW(addr, value, res)  \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tld.b\t%0, %2, 1\n"             \
+		"2:\tld.bu\t$r19, %2, 0\n\t"        \
+		"slli.w\t%0, %0, 0x8\n\t"           \
+		"or\t%0, %0, $r19\n\t"              \
+		"li.w\t%1, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%1, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=&r" (value), "=r" (res)         \
+		: "r" (addr), "i" (-EFAULT)         \
+		: "$r19");                          \
+} while (0)
+
+#define _LoadW(addr, value, res)   \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tldl.w\t%0, %2, 3\n"            \
+		"2:\tldr.w\t%0, %2, 0\n\t"          \
+		"li.w\t%1, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%1, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=&r" (value), "=r" (res)         \
+		: "r" (addr), "i" (-EFAULT));       \
+} while (0)
+
+#define _LoadHWU(addr, value, res) \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tld.bu\t%0, %2, 1\n"            \
+		"2:\tld.bu\t$r19, %2, 0\n\t"        \
+		"slli.w\t%0, %0, 0x8\n\t"           \
+		"or\t%0, %0, $r19\n\t"              \
+		"li.w\t%1, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%1, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=&r" (value), "=r" (res)         \
+		: "r" (addr), "i" (-EFAULT)         \
+		: "$r19");                          \
+} while (0)
+
+#define _LoadWU(addr, value, res)  \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tldl.w\t%0, %2, 3\n"            \
+		"2:\tldr.w\t%0, %2, 0\n\t"          \
+		"slli.d\t%0, %0, 32\n\t"            \
+		"srli.d\t%0, %0, 32\n\t"            \
+		"li.w\t%1, 0\n"                     \
+		"3:\n\t"                            \
+		"\t.section\t.fixup,\"ax\"\n\t"     \
+		"4:\tli.w\t%1, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=&r" (value), "=r" (res)         \
+		: "r" (addr), "i" (-EFAULT));       \
+} while (0)
+
+#define _LoadDW(addr, value, res)  \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tldl.d\t%0, %2, 7\n"            \
+		"2:\tldr.d\t%0, %2, 0\n\t"          \
+		"li.w\t%1, 0\n"                     \
+		"3:\n\t"                            \
+		"\t.section\t.fixup,\"ax\"\n\t"     \
+		"4:\tli.w\t%1, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=&r" (value), "=r" (res)         \
+		: "r" (addr), "i" (-EFAULT));       \
+} while (0)
+
+#define _StoreHW(addr, value, res) \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tst.b\t%1, %2, 0\n"             \
+		"srli.w\t$r19,%1, 0x8\n"            \
+		"2:\tst.b\t$r19, %2, 1\n"           \
+		"li.w\t%0, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%0, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=r" (res)                        \
+		: "r" (value), "r" (addr), "i" (-EFAULT)  \
+		: "$r19");                          \
+} while (0)
+
+#define _StoreW(addr, value, res)  \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tstl.w\t%1, %2, 3\n"            \
+		"2:\tstr.w\t%1, %2, 0\n\t"          \
+		"li.w\t%0, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%0, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=r" (res)                                \
+		: "r" (value), "r" (addr), "i" (-EFAULT));  \
+} while (0)
+
+#define _StoreDW(addr, value, res) \
+do {                                               \
+	__asm__ __volatile__ (                      \
+		"1:\tstl.d\t%1, %2, 7\n"            \
+		"2:\tstr.d\t%1, %2, 0\n\t"          \
+		"li.w\t%0, 0\n"                     \
+		"3:\n\t"                            \
+		".section\t.fixup,\"ax\"\n\t"       \
+		"4:\tli.w\t%0, %3\n\t"              \
+		"b\t3b\n\t"                         \
+		".previous\n\t"                     \
+		".section\t__ex_table,\"a\"\n\t"    \
+		STR(PTR)"\t1b, 4b\n\t"              \
+		STR(PTR)"\t2b, 4b\n\t"              \
+		".previous"                         \
+		: "=r" (res)                                \
+		: "r" (value), "r" (addr), "i" (-EFAULT));  \
+} while (0)
+
+#define LoadHWU(addr, value, res)	_LoadHWU(addr, value, res)
+#define LoadWU(addr, value, res)	_LoadWU(addr, value, res)
+#define LoadHW(addr, value, res)	_LoadHW(addr, value, res)
+#define LoadW(addr, value, res)		_LoadW(addr, value, res)
+#define LoadDW(addr, value, res)	_LoadDW(addr, value, res)
+
+#define StoreHW(addr, value, res)	_StoreHW(addr, value, res)
+#define StoreW(addr, value, res)	_StoreW(addr, value, res)
+#define StoreDW(addr, value, res)	_StoreDW(addr, value, res)
+
+#endif /* _ASM_INST_H */
diff --git a/arch/loongarch/include/asm/linkage.h b/arch/loongarch/include/asm/linkage.h
new file mode 100644
index 000000000000..283b3389b561
--- /dev/null
+++ b/arch/loongarch/include/asm/linkage.h
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+#ifndef __ASM_LINKAGE_H
+#define __ASM_LINKAGE_H
+
+#define __ALIGN		.align 2
+#define __ALIGN_STR	".align 2"
+
+#define SYM_FUNC_START(name)				\
+	SYM_START(name, SYM_L_GLOBAL, SYM_A_ALIGN)	\
+	.cfi_startproc;
+
+#define SYM_FUNC_START_NOALIGN(name)			\
+	SYM_START(name, SYM_L_GLOBAL, SYM_A_NONE)	\
+	.cfi_startproc;
+
+#define SYM_FUNC_START_LOCAL(name)			\
+	SYM_START(name, SYM_L_LOCAL, SYM_A_ALIGN)	\
+	.cfi_startproc;
+
+#define SYM_FUNC_START_LOCAL_NOALIGN(name)		\
+	SYM_START(name, SYM_L_LOCAL, SYM_A_NONE)	\
+	.cfi_startproc;
+
+#define SYM_FUNC_START_WEAK(name)			\
+	SYM_START(name, SYM_L_WEAK, SYM_A_ALIGN)	\
+	.cfi_startproc;
+
+#define SYM_FUNC_START_WEAK_NOALIGN(name)		\
+	SYM_START(name, SYM_L_WEAK, SYM_A_NONE)		\
+	.cfi_startproc;
+
+#define SYM_FUNC_END(name)				\
+	.cfi_endproc;					\
+	SYM_END(name, SYM_T_FUNC)
+
+#endif
diff --git a/arch/loongarch/include/asm/local.h b/arch/loongarch/include/asm/local.h
new file mode 100644
index 000000000000..1d6faaf04c08
--- /dev/null
+++ b/arch/loongarch/include/asm/local.h
@@ -0,0 +1,138 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ARCH_LOONGARCH_LOCAL_H
+#define _ARCH_LOONGARCH_LOCAL_H
+
+#include <linux/percpu.h>
+#include <linux/bitops.h>
+#include <linux/atomic.h>
+#include <asm/cmpxchg.h>
+#include <asm/compiler.h>
+
+typedef struct {
+	atomic_long_t a;
+} local_t;
+
+#define LOCAL_INIT(i)	{ ATOMIC_LONG_INIT(i) }
+
+#define local_read(l)	atomic_long_read(&(l)->a)
+#define local_set(l, i) atomic_long_set(&(l)->a, (i))
+
+#define local_add(i, l) atomic_long_add((i), (&(l)->a))
+#define local_sub(i, l) atomic_long_sub((i), (&(l)->a))
+#define local_inc(l)	atomic_long_inc(&(l)->a)
+#define local_dec(l)	atomic_long_dec(&(l)->a)
+
+/*
+ * Same as above, but return the result value
+ */
+static inline long local_add_return(long i, local_t *l)
+{
+	unsigned long result;
+
+	__asm__ __volatile__(
+	"   " __AMADD " %1, %2, %0      \n"
+	: "+ZB" (l->a.counter), "=&r" (result)
+	: "r" (i)
+	: "memory");
+	result = result + i;
+
+	return result;
+}
+
+static inline long local_sub_return(long i, local_t *l)
+{
+	unsigned long result;
+
+	__asm__ __volatile__(
+	"   " __AMADD "%1, %2, %0       \n"
+	: "+ZB" (l->a.counter), "=&r" (result)
+	: "r" (-i)
+	: "memory");
+
+	result = result - i;
+
+	return result;
+}
+
+#define local_cmpxchg(l, o, n) \
+	((long)cmpxchg_local(&((l)->a.counter), (o), (n)))
+#define local_xchg(l, n) (atomic_long_xchg((&(l)->a), (n)))
+
+/**
+ * local_add_unless - add unless the number is a given value
+ * @l: pointer of type local_t
+ * @a: the amount to add to l...
+ * @u: ...unless l is equal to u.
+ *
+ * Atomically adds @a to @l, so long as it was not @u.
+ * Returns non-zero if @l was not @u, and zero otherwise.
+ */
+#define local_add_unless(l, a, u)				\
+({								\
+	long c, old;						\
+	c = local_read(l);					\
+	while (c != (u) && (old = local_cmpxchg((l), c, c + (a))) != c) \
+		c = old;					\
+	c != (u);						\
+})
+#define local_inc_not_zero(l) local_add_unless((l), 1, 0)
+
+#define local_dec_return(l) local_sub_return(1, (l))
+#define local_inc_return(l) local_add_return(1, (l))
+
+/*
+ * local_sub_and_test - subtract value from variable and test result
+ * @i: integer value to subtract
+ * @l: pointer of type local_t
+ *
+ * Atomically subtracts @i from @l and returns
+ * true if the result is zero, or false for all
+ * other cases.
+ */
+#define local_sub_and_test(i, l) (local_sub_return((i), (l)) == 0)
+
+/*
+ * local_inc_and_test - increment and test
+ * @l: pointer of type local_t
+ *
+ * Atomically increments @l by 1
+ * and returns true if the result is zero, or false for all
+ * other cases.
+ */
+#define local_inc_and_test(l) (local_inc_return(l) == 0)
+
+/*
+ * local_dec_and_test - decrement by 1 and test
+ * @l: pointer of type local_t
+ *
+ * Atomically decrements @l by 1 and
+ * returns true if the result is 0, or false for all other
+ * cases.
+ */
+#define local_dec_and_test(l) (local_sub_return(1, (l)) == 0)
+
+/*
+ * local_add_negative - add and test if negative
+ * @l: pointer of type local_t
+ * @i: integer value to add
+ *
+ * Atomically adds @i to @l and returns true
+ * if the result is negative, or false when
+ * result is greater than or equal to zero.
+ */
+#define local_add_negative(i, l) (local_add_return(i, (l)) < 0)
+
+/* Use these for per-cpu local_t variables: on some archs they are
+ * much more efficient than these naive implementations.  Note they take
+ * a variable, not an address.
+ */
+
+#define __local_inc(l)		((l)->a.counter++)
+#define __local_dec(l)		((l)->a.counter++)
+#define __local_add(i, l)	((l)->a.counter += (i))
+#define __local_sub(i, l)	((l)->a.counter -= (i))
+
+#endif /* _ARCH_LOONGARCH_LOCAL_H */
diff --git a/arch/loongarch/include/asm/loongarchregs.h b/arch/loongarch/include/asm/loongarchregs.h
new file mode 100644
index 000000000000..23982c49a320
--- /dev/null
+++ b/arch/loongarch/include/asm/loongarchregs.h
@@ -0,0 +1,1554 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_LOONGARCHREGS_H
+#define _ASM_LOONGARCHREGS_H
+
+#include <linux/bits.h>
+#include <linux/linkage.h>
+#include <linux/types.h>
+
+#ifndef __ASSEMBLY__
+#include <larchintrin.h>
+
+/*
+ * parse_r var, r - Helper assembler macro for parsing register names.
+ *
+ * This converts the register name in $n form provided in \r to the
+ * corresponding register number, which is assigned to the variable \var. It is
+ * needed to allow explicit encoding of instructions in inline assembly where
+ * registers are chosen by the compiler in $n form, allowing us to avoid using
+ * fixed register numbers.
+ *
+ * It also allows newer instructions (not implemented by the assembler) to be
+ * transparently implemented using assembler macros, instead of needing separate
+ * cases depending on toolchain support.
+ *
+ * Simple usage example:
+ * __asm__ __volatile__("parse_r __rt, %0\n\t"
+ *			"# di    %0\n\t"
+ *			".word   (0x41606000 | (__rt << 16))"
+ *			: "=r" (status);
+ */
+
+/* Match an individual register number and assign to \var */
+#define _IFC_REG(n)				\
+	".ifc	\\r, $r" #n "\n\t"		\
+	"\\var	= " #n "\n\t"			\
+	".endif\n\t"
+
+__asm__(".macro	parse_r var r\n\t"
+	"\\var	= -1\n\t"
+	_IFC_REG(0)  _IFC_REG(1)  _IFC_REG(2)  _IFC_REG(3)
+	_IFC_REG(4)  _IFC_REG(5)  _IFC_REG(6)  _IFC_REG(7)
+	_IFC_REG(8)  _IFC_REG(9)  _IFC_REG(10) _IFC_REG(11)
+	_IFC_REG(12) _IFC_REG(13) _IFC_REG(14) _IFC_REG(15)
+	_IFC_REG(16) _IFC_REG(17) _IFC_REG(18) _IFC_REG(19)
+	_IFC_REG(20) _IFC_REG(21) _IFC_REG(22) _IFC_REG(23)
+	_IFC_REG(24) _IFC_REG(25) _IFC_REG(26) _IFC_REG(27)
+	_IFC_REG(28) _IFC_REG(29) _IFC_REG(30) _IFC_REG(31)
+	".iflt	\\var\n\t"
+	".error	\"Unable to parse register name \\r\"\n\t"
+	".endif\n\t"
+	".endm");
+
+#undef _IFC_REG
+
+/* CPUCFG */
+static inline u32 read_cpucfg(u32 reg)
+{
+	return __cpucfg(reg);
+}
+
+#endif /* !__ASSEMBLY__ */
+
+/* LoongArch Registers */
+#define REG_RA	0x1
+#define REG_TP	0x2
+#define REG_SP	0x3
+#define REG_A0	0x4
+#define REG_A1	0x5
+#define REG_A2	0x6
+#define REG_A3	0x7
+#define REG_A4	0x8
+#define REG_A5	0x9
+#define REG_A6	0xa
+#define REG_A7	0xb
+#define REG_V0	REG_A0
+#define REG_V1	REG_A1
+#define REG_T0	0xc
+#define REG_T1	0xd
+#define REG_T2	0xe
+#define REG_T3	0xf
+#define REG_T4	0x10
+#define REG_T5	0x11
+#define REG_T6	0x12
+#define REG_T7	0x13
+#define REG_T8	0x14
+#define REG_X0	0x15
+#define REG_FP	0x16
+#define REG_S0	0x17
+#define REG_S1	0x18
+#define REG_S2	0x19
+#define REG_S3	0x1a
+#define REG_S4	0x1b
+#define REG_S5	0x1c
+#define REG_S6	0x1d
+#define REG_S7	0x1e
+#define REG_S8	0x1f
+
+/* Bit Domains for CPUCFG registers */
+#define LOONGARCH_CPUCFG0		0x0
+#define  CPUCFG0_PRID			GENMASK(31, 0)
+
+#define LOONGARCH_CPUCFG1		0x1
+#define  CPUCFG1_ISGR32			BIT(0)
+#define  CPUCFG1_ISGR64			BIT(1)
+#define  CPUCFG1_PAGING			BIT(2)
+#define  CPUCFG1_IOCSR			BIT(3)
+#define  CPUCFG1_PABITS			GENMASK(11, 4)
+#define  CPUCFG1_VABITS			GENMASK(19, 12)
+#define  CPUCFG1_UAL			BIT(20)
+#define  CPUCFG1_RI			BIT(21)
+#define  CPUCFG1_XI			BIT(22)
+#define  CPUCFG1_RPLV			BIT(23)
+#define  CPUCFG1_HUGEPG			BIT(24)
+#define  CPUCFG1_IOCSRBRD		BIT(25)
+#define  CPUCFG1_MSGINT			BIT(26)
+
+#define LOONGARCH_CPUCFG2		0x2
+#define  CPUCFG2_FP			BIT(0)
+#define  CPUCFG2_FPSP			BIT(1)
+#define  CPUCFG2_FPDP			BIT(2)
+#define  CPUCFG2_FPVERS			GENMASK(5, 3)
+#define  CPUCFG2_LSX			BIT(6)
+#define  CPUCFG2_LASX			BIT(7)
+#define  CPUCFG2_COMPLEX		BIT(8)
+#define  CPUCFG2_CRYPTO			BIT(9)
+#define  CPUCFG2_LVZP			BIT(10)
+#define  CPUCFG2_LVZVER			GENMASK(13, 11)
+#define  CPUCFG2_LLFTP			BIT(14)
+#define  CPUCFG2_LLFTPREV		GENMASK(17, 15)
+#define  CPUCFG2_X86BT			BIT(18)
+#define  CPUCFG2_ARMBT			BIT(19)
+#define  CPUCFG2_MIPSBT			BIT(20)
+#define  CPUCFG2_LSPW			BIT(21)
+#define  CPUCFG2_LAM			BIT(22)
+
+#define LOONGARCH_CPUCFG3		0x3
+#define  CPUCFG3_CCDMA			BIT(0)
+#define  CPUCFG3_SFB			BIT(1)
+#define  CPUCFG3_UCACC			BIT(2)
+#define  CPUCFG3_LLEXC			BIT(3)
+#define  CPUCFG3_SCDLY			BIT(4)
+#define  CPUCFG3_LLDBAR			BIT(5)
+#define  CPUCFG3_ITLBT			BIT(6)
+#define  CPUCFG3_ICACHET		BIT(7)
+#define  CPUCFG3_SPW_LVL		GENMASK(10, 8)
+#define  CPUCFG3_SPW_HG_HF		BIT(11)
+#define  CPUCFG3_RVA			BIT(12)
+#define  CPUCFG3_RVAMAX			GENMASK(16, 13)
+
+#define LOONGARCH_CPUCFG4		0x4
+#define  CPUCFG4_CCFREQ			GENMASK(31, 0)
+
+#define LOONGARCH_CPUCFG5		0x5
+#define  CPUCFG5_CCMUL			GENMASK(15, 0)
+#define  CPUCFG5_CCDIV			GENMASK(31, 16)
+
+#define LOONGARCH_CPUCFG6		0x6
+#define  CPUCFG6_PMP			BIT(0)
+#define  CPUCFG6_PAMVER			GENMASK(3, 1)
+#define  CPUCFG6_PMNUM			GENMASK(7, 4)
+#define  CPUCFG6_PMBITS			GENMASK(13, 8)
+#define  CPUCFG6_UPM			BIT(14)
+
+#define LOONGARCH_CPUCFG16		0x10
+#define  CPUCFG16_L1_IUPRE		BIT(0)
+#define  CPUCFG16_L1_UNIFY		BIT(1)
+#define  CPUCFG16_L1_DPRE		BIT(2)
+#define  CPUCFG16_L2_IUPRE		BIT(3)
+#define  CPUCFG16_L2_IUUNIFY		BIT(4)
+#define  CPUCFG16_L2_IUPRIV		BIT(5)
+#define  CPUCFG16_L2_IUINCL		BIT(6)
+#define  CPUCFG16_L2_DPRE		BIT(7)
+#define  CPUCFG16_L2_DPRIV		BIT(8)
+#define  CPUCFG16_L2_DINCL		BIT(9)
+#define  CPUCFG16_L3_IUPRE		BIT(10)
+#define  CPUCFG16_L3_IUUNIFY		BIT(11)
+#define  CPUCFG16_L3_IUPRIV		BIT(12)
+#define  CPUCFG16_L3_IUINCL		BIT(13)
+#define  CPUCFG16_L3_DPRE		BIT(14)
+#define  CPUCFG16_L3_DPRIV		BIT(15)
+#define  CPUCFG16_L3_DINCL		BIT(16)
+
+#define LOONGARCH_CPUCFG17		0x11
+#define  CPUCFG17_L1I_WAYS_M		GENMASK(15, 0)
+#define  CPUCFG17_L1I_SETS_M		GENMASK(23, 16)
+#define  CPUCFG17_L1I_SIZE_M		GENMASK(30, 24)
+#define  CPUCFG17_L1I_WAYS		0
+#define  CPUCFG17_L1I_SETS		16
+#define  CPUCFG17_L1I_SIZE		24
+
+#define LOONGARCH_CPUCFG18		0x12
+#define  CPUCFG18_L1D_WAYS_M		GENMASK(15, 0)
+#define  CPUCFG18_L1D_SETS_M		GENMASK(23, 16)
+#define  CPUCFG18_L1D_SIZE_M		GENMASK(30, 24)
+#define  CPUCFG18_L1D_WAYS		0
+#define  CPUCFG18_L1D_SETS		16
+#define  CPUCFG18_L1D_SIZE		24
+
+#define LOONGARCH_CPUCFG19		0x13
+#define  CPUCFG19_L2_WAYS_M		GENMASK(15, 0)
+#define  CPUCFG19_L2_SETS_M		GENMASK(23, 16)
+#define  CPUCFG19_L2_SIZE_M		GENMASK(30, 24)
+#define  CPUCFG19_L2_WAYS		0
+#define  CPUCFG19_L2_SETS		16
+#define  CPUCFG19_L2_SIZE		24
+
+#define LOONGARCH_CPUCFG20		0x14
+#define  CPUCFG20_L3_WAYS_M		GENMASK(15, 0)
+#define  CPUCFG20_L3_SETS_M		GENMASK(23, 16)
+#define  CPUCFG20_L3_SIZE_M		GENMASK(30, 24)
+#define  CPUCFG20_L3_WAYS		0
+#define  CPUCFG20_L3_SETS		16
+#define  CPUCFG20_L3_SIZE		24
+
+#define LOONGARCH_CPUCFG48		0x30
+#define  CPUCFG48_MCSR_LCK		BIT(0)
+#define  CPUCFG48_NAP_EN		BIT(1)
+#define  CPUCFG48_VFPU_CG		BIT(2)
+#define  CPUCFG48_RAM_CG		BIT(3)
+
+#ifndef __ASSEMBLY__
+
+/* CSR */
+static inline u32 csr_readl(u32 reg)
+{
+	return __csrrd(reg);
+}
+
+static inline u64 csr_readq(u32 reg)
+{
+	return __dcsrrd(reg);
+}
+
+static inline void csr_writel(u32 val, u32 reg)
+{
+	__csrwr(val, reg);
+}
+
+static inline void csr_writeq(u64 val, u32 reg)
+{
+	__dcsrwr(val, reg);
+}
+
+static inline u32 csr_xchgl(u32 val, u32 mask, u32 reg)
+{
+	return __csrxchg(val, mask, reg);
+}
+
+static inline u64 csr_xchgq(u64 val, u64 mask, u32 reg)
+{
+	return __dcsrxchg(val, mask, reg);
+}
+
+/* IOCSR */
+static inline u32 iocsr_readl(u32 reg)
+{
+	return __iocsrrd_w(reg);
+}
+
+static inline u64 iocsr_readq(u32 reg)
+{
+	return __iocsrrd_d(reg);
+}
+
+static inline void iocsr_writel(u32 val, u32 reg)
+{
+	__iocsrwr_w(val, reg);
+}
+
+static inline void iocsr_writeq(u64 val, u32 reg)
+{
+	__iocsrwr_d(val, reg);
+}
+
+#endif /* !__ASSEMBLY__ */
+
+/* CSR register number */
+
+/* Basic CSR registers */
+#define LOONGARCH_CSR_CRMD		0x0	/* Current mode info */
+#define  CSR_CRMD_DACM_SHIFT		7
+#define  CSR_CRMD_DACM_WIDTH		2
+#define  CSR_CRMD_DACM			(_ULCAST_(0x3) << CSR_CRMD_DACM_SHIFT)
+#define  CSR_CRMD_DACF_SHIFT		5
+#define  CSR_CRMD_DACF_WIDTH		2
+#define  CSR_CRMD_DACF			(_ULCAST_(0x3) << CSR_CRMD_DACF_SHIFT)
+#define  CSR_CRMD_PG_SHIFT		4
+#define  CSR_CRMD_PG			(_ULCAST_(0x1) << CSR_CRMD_PG_SHIFT)
+#define  CSR_CRMD_DA_SHIFT		3
+#define  CSR_CRMD_DA			(_ULCAST_(0x1) << CSR_CRMD_DA_SHIFT)
+#define  CSR_CRMD_IE_SHIFT		2
+#define  CSR_CRMD_IE			(_ULCAST_(0x1) << CSR_CRMD_IE_SHIFT)
+#define  CSR_CRMD_PLV_SHIFT		0
+#define  CSR_CRMD_PLV_WIDTH		2
+#define  CSR_CRMD_PLV			(_ULCAST_(0x3) << CSR_CRMD_PLV_SHIFT)
+
+#define PLV_KERN			0
+#define PLV_USER			3
+#define PLV_MASK			0x3
+
+#define LOONGARCH_CSR_PRMD		0x1	/* Prev-exception mode info */
+#define  CSR_PRMD_PIE_SHIFT		2
+#define  CSR_PRMD_PIE			(_ULCAST_(0x1) << CSR_PRMD_PIE_SHIFT)
+#define  CSR_PRMD_PPLV_SHIFT		0
+#define  CSR_PRMD_PPLV_WIDTH		2
+#define  CSR_PRMD_PPLV			(_ULCAST_(0x3) << CSR_PRMD_PPLV_SHIFT)
+
+#define LOONGARCH_CSR_EUEN		0x2	/* Extended unit enable */
+#define  CSR_EUEN_LBTEN_SHIFT		3
+#define  CSR_EUEN_LBTEN			(_ULCAST_(0x1) << CSR_EUEN_LBTEN_SHIFT)
+#define  CSR_EUEN_LASXEN_SHIFT		2
+#define  CSR_EUEN_LASXEN		(_ULCAST_(0x1) << CSR_EUEN_LASXEN_SHIFT)
+#define  CSR_EUEN_LSXEN_SHIFT		1
+#define  CSR_EUEN_LSXEN			(_ULCAST_(0x1) << CSR_EUEN_LSXEN_SHIFT)
+#define  CSR_EUEN_FPEN_SHIFT		0
+#define  CSR_EUEN_FPEN			(_ULCAST_(0x1) << CSR_EUEN_FPEN_SHIFT)
+
+#define LOONGARCH_CSR_MISC		0x3	/* Misc config */
+
+#define LOONGARCH_CSR_ECFG		0x4	/* Exception config */
+#define  CSR_ECFG_VS_SHIFT		16
+#define  CSR_ECFG_VS_WIDTH		3
+#define  CSR_ECFG_VS			(_ULCAST_(0x7) << CSR_ECFG_VS_SHIFT)
+#define  CSR_ECFG_IM_SHIFT		0
+#define  CSR_ECFG_IM_WIDTH		13
+#define  CSR_ECFG_IM			(_ULCAST_(0x1fff) << CSR_ECFG_IM_SHIFT)
+
+#define LOONGARCH_CSR_ESTAT		0x5	/* Exception status */
+#define  CSR_ESTAT_ESUBCODE_SHIFT	22
+#define  CSR_ESTAT_ESUBCODE_WIDTH	9
+#define  CSR_ESTAT_ESUBCODE		(_ULCAST_(0x1ff) << CSR_ESTAT_ESUBCODE_SHIFT)
+#define  CSR_ESTAT_EXC_SHIFT		16
+#define  CSR_ESTAT_EXC_WIDTH		6
+#define  CSR_ESTAT_EXC			(_ULCAST_(0x3f) << CSR_ESTAT_EXC_SHIFT)
+#define  CSR_ESTAT_IS_SHIFT		0
+#define  CSR_ESTAT_IS_WIDTH		15
+#define  CSR_ESTAT_IS			(_ULCAST_(0x7fff) << CSR_ESTAT_IS_SHIFT)
+
+#define LOONGARCH_CSR_EPC		0x6	/* EPC */
+
+#define LOONGARCH_CSR_BADV		0x7	/* Bad virtual address */
+
+#define LOONGARCH_CSR_BADI		0x8	/* Bad instruction */
+
+#define LOONGARCH_CSR_EENTRY		0xc	/* Exception entry */
+
+/* TLB related CSR registers */
+#define LOONGARCH_CSR_TLBIDX		0x10	/* TLB Index, EHINV, PageSize, NP */
+#define  CSR_TLBIDX_EHINV_SHIFT		31
+#define  CSR_TLBIDX_EHINV		(_ULCAST_(1) << CSR_TLBIDX_EHINV_SHIFT)
+#define  CSR_TLBIDX_PS_SHIFT		24
+#define  CSR_TLBIDX_PS_WIDTH		6
+#define  CSR_TLBIDX_PS			(_ULCAST_(0x3f) << CSR_TLBIDX_PS_SHIFT)
+#define  CSR_TLBIDX_IDX_SHIFT		0
+#define  CSR_TLBIDX_IDX_WIDTH		12
+#define  CSR_TLBIDX_IDX			(_ULCAST_(0xfff) << CSR_TLBIDX_IDX_SHIFT)
+#define  CSR_TLBIDX_SIZEM		0x3f000000
+#define  CSR_TLBIDX_SIZE		CSR_TLBIDX_PS_SHIFT
+#define  CSR_TLBIDX_IDXM		0xfff
+#define  CSR_INVALID_ENTRY(e)		(CSR_TLBIDX_EHINV | e)
+
+#define LOONGARCH_CSR_TLBEHI		0x11	/* TLB EntryHi */
+
+#define LOONGARCH_CSR_TLBELO0		0x12	/* TLB EntryLo0 */
+#define  CSR_TLBLO0_RPLV_SHIFT		63
+#define  CSR_TLBLO0_RPLV		(_ULCAST_(0x1) << CSR_TLBLO0_RPLV_SHIFT)
+#define  CSR_TLBLO0_XI_SHIFT		62
+#define  CSR_TLBLO0_XI			(_ULCAST_(0x1) << CSR_TLBLO0_XI_SHIFT)
+#define  CSR_TLBLO0_RI_SHIFT		61
+#define  CSR_TLBLO0_RI			(_ULCAST_(0x1) << CSR_TLBLO0_RI_SHIFT)
+#define  CSR_TLBLO0_PFN_SHIFT		12
+#define  CSR_TLBLO0_PFN_WIDTH		36
+#define  CSR_TLBLO0_PFN			(_ULCAST_(0xfffffffff) << CSR_TLBLO0_PFN_SHIFT)
+#define  CSR_TLBLO0_GLOBAL_SHIFT	6
+#define  CSR_TLBLO0_GLOBAL		(_ULCAST_(0x1) << CSR_TLBLO0_GLOBAL_SHIFT)
+#define  CSR_TLBLO0_CCA_SHIFT		4
+#define  CSR_TLBLO0_CCA_WIDTH		2
+#define  CSR_TLBLO0_CCA			(_ULCAST_(0x3) << CSR_TLBLO0_CCA_SHIFT)
+#define  CSR_TLBLO0_PLV_SHIFT		2
+#define  CSR_TLBLO0_PLV_WIDTH		2
+#define  CSR_TLBLO0_PLV			(_ULCAST_(0x3) << CSR_TLBLO0_PLV_SHIFT)
+#define  CSR_TLBLO0_WE_SHIFT		1
+#define  CSR_TLBLO0_WE			(_ULCAST_(0x1) << CSR_TLBLO0_WE_SHIFT)
+#define  CSR_TLBLO0_V_SHIFT		0
+#define  CSR_TLBLO0_V			(_ULCAST_(0x1) << CSR_TLBLO0_V_SHIFT)
+
+#define LOONGARCH_CSR_TLBELO1		0x13	/* TLB EntryLo1 */
+#define  CSR_TLBLO1_RPLV_SHIFT		63
+#define  CSR_TLBLO1_RPLV		(_ULCAST_(0x1) << CSR_TLBLO1_RPLV_SHIFT)
+#define  CSR_TLBLO1_XI_SHIFT		62
+#define  CSR_TLBLO1_XI			(_ULCAST_(0x1) << CSR_TLBLO1_XI_SHIFT)
+#define  CSR_TLBLO1_RI_SHIFT		61
+#define  CSR_TLBLO1_RI			(_ULCAST_(0x1) << CSR_TLBLO1_RI_SHIFT)
+#define  CSR_TLBLO1_PFN_SHIFT		12
+#define  CSR_TLBLO1_PFN_WIDTH		36
+#define  CSR_TLBLO1_PFN			(_ULCAST_(0xfffffffff) << CSR_TLBLO1_PFN_SHIFT)
+#define  CSR_TLBLO1_GLOBAL_SHIFT	6
+#define  CSR_TLBLO1_GLOBAL		(_ULCAST_(0x1) << CSR_TLBLO1_GLOBAL_SHIFT)
+#define  CSR_TLBLO1_CCA_SHIFT		4
+#define  CSR_TLBLO1_CCA_WIDTH		2
+#define  CSR_TLBLO1_CCA			(_ULCAST_(0x3) << CSR_TLBLO1_CCA_SHIFT)
+#define  CSR_TLBLO1_PLV_SHIFT		2
+#define  CSR_TLBLO1_PLV_WIDTH		2
+#define  CSR_TLBLO1_PLV			(_ULCAST_(0x3) << CSR_TLBLO1_PLV_SHIFT)
+#define  CSR_TLBLO1_WE_SHIFT		1
+#define  CSR_TLBLO1_WE			(_ULCAST_(0x1) << CSR_TLBLO1_WE_SHIFT)
+#define  CSR_TLBLO1_V_SHIFT		0
+#define  CSR_TLBLO1_V			(_ULCAST_(0x1) << CSR_TLBLO1_V_SHIFT)
+
+#define LOONGARCH_CSR_GTLBC		0x15	/* Guest TLB control */
+#define  CSR_GTLBC_RID_SHIFT		16
+#define  CSR_GTLBC_RID_WIDTH		8
+#define  CSR_GTLBC_RID			(_ULCAST_(0xff) << CSR_GTLBC_RID_SHIFT)
+#define  CSR_GTLBC_TOTI_SHIFT		13
+#define  CSR_GTLBC_TOTI			(_ULCAST_(0x1) << CSR_GTLBC_TOTI_SHIFT)
+#define  CSR_GTLBC_USERID_SHIFT		12
+#define  CSR_GTLBC_USERID		(_ULCAST_(0x1) << CSR_GTLBC_USERID_SHIFT)
+#define  CSR_GTLBC_GMTLBSZ_SHIFT	0
+#define  CSR_GTLBC_GMTLBSZ_WIDTH	6
+#define  CSR_GTLBC_GMTLBSZ		(_ULCAST_(0x3f) << CSR_GTLBC_GMTLBSZ_SHIFT)
+
+#define LOONGARCH_CSR_TRGP		0x16	/* TLBR read guest info */
+#define  CSR_TRGP_RID_SHIFT		16
+#define  CSR_TRGP_RID_WIDTH		8
+#define  CSR_TRGP_RID			(_ULCAST_(0xff) << CSR_TRGP_RID_SHIFT)
+#define  CSR_TRGP_GTLB_SHIFT		0
+#define  CSR_TRGP_GTLB			(1 << CSR_TRGP_GTLB_SHIFT)
+
+#define LOONGARCH_CSR_ASID		0x18	/* ASID */
+#define  CSR_ASID_BIT_SHIFT		16	/* ASIDBits */
+#define  CSR_ASID_BIT_WIDTH		8
+#define  CSR_ASID_BIT			(_ULCAST_(0xff) << CSR_ASID_BIT_SHIFT)
+#define  CSR_ASID_ASID_SHIFT		0
+#define  CSR_ASID_ASID_WIDTH		10
+#define  CSR_ASID_ASID			(_ULCAST_(0x3ff) << CSR_ASID_ASID_SHIFT)
+
+#define LOONGARCH_CSR_PGDL		0x19	/* Page table base address when VA[47] = 0 */
+
+#define LOONGARCH_CSR_PGDH		0x1a	/* Page table base address when VA[47] = 1 */
+
+#define LOONGARCH_CSR_PGD		0x1b	/* Page table base */
+
+#define LOONGARCH_CSR_PWCTL0		0x1c	/* PWCtl0 */
+#define  CSR_PWCTL0_PTEW_SHIFT		30
+#define  CSR_PWCTL0_PTEW_WIDTH		2
+#define  CSR_PWCTL0_PTEW		(_ULCAST_(0x3) << CSR_PWCTL0_PTEW_SHIFT)
+#define  CSR_PWCTL0_DIR1WIDTH_SHIFT	25
+#define  CSR_PWCTL0_DIR1WIDTH_WIDTH	5
+#define  CSR_PWCTL0_DIR1WIDTH		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR1WIDTH_SHIFT)
+#define  CSR_PWCTL0_DIR1BASE_SHIFT	20
+#define  CSR_PWCTL0_DIR1BASE_WIDTH	5
+#define  CSR_PWCTL0_DIR1BASE		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR1BASE_SHIFT)
+#define  CSR_PWCTL0_DIR0WIDTH_SHIFT	15
+#define  CSR_PWCTL0_DIR0WIDTH_WIDTH	5
+#define  CSR_PWCTL0_DIR0WIDTH		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR0WIDTH_SHIFT)
+#define  CSR_PWCTL0_DIR0BASE_SHIFT	10
+#define  CSR_PWCTL0_DIR0BASE_WIDTH	5
+#define  CSR_PWCTL0_DIR0BASE		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR0BASE_SHIFT)
+#define  CSR_PWCTL0_PTWIDTH_SHIFT	5
+#define  CSR_PWCTL0_PTWIDTH_WIDTH	5
+#define  CSR_PWCTL0_PTWIDTH		(_ULCAST_(0x1f) << CSR_PWCTL0_PTWIDTH_SHIFT)
+#define  CSR_PWCTL0_PTBASE_SHIFT	0
+#define  CSR_PWCTL0_PTBASE_WIDTH	5
+#define  CSR_PWCTL0_PTBASE		(_ULCAST_(0x1f) << CSR_PWCTL0_PTBASE_SHIFT)
+
+#define LOONGARCH_CSR_PWCTL1		0x1d	/* PWCtl1 */
+#define  CSR_PWCTL1_DIR3WIDTH_SHIFT	18
+#define  CSR_PWCTL1_DIR3WIDTH_WIDTH	5
+#define  CSR_PWCTL1_DIR3WIDTH		(_ULCAST_(0x1f) << CSR_PWCTL1_DIR3WIDTH_SHIFT)
+#define  CSR_PWCTL1_DIR3BASE_SHIFT	12
+#define  CSR_PWCTL1_DIR3BASE_WIDTH	5
+#define  CSR_PWCTL1_DIR3BASE		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR3BASE_SHIFT)
+#define  CSR_PWCTL1_DIR2WIDTH_SHIFT	6
+#define  CSR_PWCTL1_DIR2WIDTH_WIDTH	5
+#define  CSR_PWCTL1_DIR2WIDTH		(_ULCAST_(0x1f) << CSR_PWCTL1_DIR2WIDTH_SHIFT)
+#define  CSR_PWCTL1_DIR2BASE_SHIFT	0
+#define  CSR_PWCTL1_DIR2BASE_WIDTH	5
+#define  CSR_PWCTL1_DIR2BASE		(_ULCAST_(0x1f) << CSR_PWCTL0_DIR2BASE_SHIFT)
+
+#define LOONGARCH_CSR_STLBPGSIZE	0x1e
+#define  CSR_STLBPGSIZE_PS_WIDTH	6
+#define  CSR_STLBPGSIZE_PS		(_ULCAST_(0x3f))
+
+#define LOONGARCH_CSR_RVACFG		0x1f
+#define  CSR_RVACFG_RDVA_WIDTH		4
+#define  CSR_RVACFG_RDVA		(_ULCAST_(0xf))
+
+/* Config CSR registers */
+#define LOONGARCH_CSR_CPUID		0x20	/* CPU core id */
+#define  CSR_CPUID_COREID_WIDTH		9
+#define  CSR_CPUID_COREID		_ULCAST_(0x1ff)
+
+#define LOONGARCH_CSR_PRCFG1		0x21	/* Config1 */
+#define  CSR_CONF1_VSMAX_SHIFT		12
+#define  CSR_CONF1_VSMAX_WIDTH		3
+#define  CSR_CONF1_VSMAX		(_ULCAST_(7) << CSR_CONF1_VSMAX_SHIFT)
+#define  CSR_CONF1_TMRBITS_SHIFT	4
+#define  CSR_CONF1_TMRBITS_WIDTH	8
+#define  CSR_CONF1_TMRBITS		(_ULCAST_(0xff) << CSR_CONF1_TMRBITS_SHIFT)
+#define  CSR_CONF1_KSNUM_WIDTH		4
+#define  CSR_CONF1_KSNUM		_ULCAST_(0xf)
+
+#define LOONGARCH_CSR_PRCFG2		0x22	/* Config2 */
+#define  CSR_CONF2_PGMASK_SUPP		0x3ffff000
+
+#define LOONGARCH_CSR_PRCFG3		0x23	/* Config3 */
+#define  CSR_CONF3_STLBIDX_SHIFT	20
+#define  CSR_CONF3_STLBIDX_WIDTH	6
+#define  CSR_CONF3_STLBIDX		(_ULCAST_(0x3f) << CSR_CONF3_STLBIDX_SHIFT)
+#define  CSR_CONF3_STLBWAYS_SHIFT	12
+#define  CSR_CONF3_STLBWAYS_WIDTH	8
+#define  CSR_CONF3_STLBWAYS		(_ULCAST_(0xff) << CSR_CONF3_STLBWAYS_SHIFT)
+#define  CSR_CONF3_MTLBSIZE_SHIFT	4
+#define  CSR_CONF3_MTLBSIZE_WIDTH	8
+#define  CSR_CONF3_MTLBSIZE		(_ULCAST_(0xff) << CSR_CONF3_MTLBSIZE_SHIFT)
+#define  CSR_CONF3_TLBTYPE_SHIFT	0
+#define  CSR_CONF3_TLBTYPE_WIDTH	4
+#define  CSR_CONF3_TLBTYPE		(_ULCAST_(0xf) << CSR_CONF3_TLBTYPE_SHIFT)
+
+/* Kscratch registers */
+#define LOONGARCH_CSR_KS0		0x30
+#define LOONGARCH_CSR_KS1		0x31
+#define LOONGARCH_CSR_KS2		0x32
+#define LOONGARCH_CSR_KS3		0x33
+#define LOONGARCH_CSR_KS4		0x34
+#define LOONGARCH_CSR_KS5		0x35
+#define LOONGARCH_CSR_KS6		0x36
+#define LOONGARCH_CSR_KS7		0x37
+#define LOONGARCH_CSR_KS8		0x38
+
+/* Exception allocated KS0, KS1 and KS2 statically */
+#define EXCEPTION_KS0			LOONGARCH_CSR_KS0
+#define EXCEPTION_KS1			LOONGARCH_CSR_KS1
+#define EXCEPTION_KS2			LOONGARCH_CSR_KS2
+#define EXC_KSCRATCH_MASK		(1 << 0 | 1 << 1 | 1 << 2)
+
+/* Percpu-data base allocated KS3 statically */
+#define PERCPU_BASE_KS			LOONGARCH_CSR_KS3
+#define PERCPU_KSCRATCH_MASK		(1 << 3)
+
+/* KVM allocated KS4 and KS5 statically */
+#define KVM_VCPU_KS			LOONGARCH_CSR_KS4
+#define KVM_TEMP_KS			LOONGARCH_CSR_KS5
+#define KVM_KSCRATCH_MASK		(1 << 4 | 1 << 5)
+
+/* Timer registers */
+#define LOONGARCH_CSR_TMID		0x40	/* Timer ID */
+
+#define LOONGARCH_CSR_TCFG		0x41	/* Timer config */
+#define  CSR_TCFG_VAL_SHIFT		2
+#define	 CSR_TCFG_VAL_WIDTH		48
+#define  CSR_TCFG_VAL			(_ULCAST_(0x3fffffffffff) << CSR_TCFG_VAL_SHIFT)
+#define  CSR_TCFG_PERIOD_SHIFT		1
+#define  CSR_TCFG_PERIOD		(_ULCAST_(0x1) << CSR_TCFG_PERIOD_SHIFT)
+#define  CSR_TCFG_EN			(_ULCAST_(0x1))
+
+#define LOONGARCH_CSR_TVAL		0x42	/* Timer value */
+
+#define LOONGARCH_CSR_CNTC		0x43	/* Timer offset */
+
+#define LOONGARCH_CSR_TINTCLR		0x44	/* Timer interrupt clear */
+#define  CSR_TINTCLR_TI_SHIFT		0
+#define  CSR_TINTCLR_TI			(1 << CSR_TINTCLR_TI_SHIFT)
+
+/* Guest registers */
+#define LOONGARCH_CSR_GSTAT		0x50	/* Guest status */
+#define  CSR_GSTAT_GID_SHIFT		16
+#define  CSR_GSTAT_GID_WIDTH		8
+#define  CSR_GSTAT_GID			(_ULCAST_(0xff) << CSR_GSTAT_GID_SHIFT)
+#define  CSR_GSTAT_GIDBIT_SHIFT		4
+#define  CSR_GSTAT_GIDBIT_WIDTH		6
+#define  CSR_GSTAT_GIDBIT		(_ULCAST_(0x3f) << CSR_GSTAT_GIDBIT_SHIFT)
+#define  CSR_GSTAT_PVM_SHIFT		1
+#define  CSR_GSTAT_PVM			(_ULCAST_(0x1) << CSR_GSTAT_PVM_SHIFT)
+#define  CSR_GSTAT_VM_SHIFT		0
+#define  CSR_GSTAT_VM			(_ULCAST_(0x1) << CSR_GSTAT_VM_SHIFT)
+
+#define LOONGARCH_CSR_GCFG		0x51	/* Guest config */
+#define  CSR_GCFG_GPERF_SHIFT		24
+#define  CSR_GCFG_GPERF_WIDTH		3
+#define  CSR_GCFG_GPERF			(_ULCAST_(0x7) << CSR_GCFG_GPERF_SHIFT)
+#define  CSR_GCFG_GCI_SHIFT		20
+#define  CSR_GCFG_GCI_WIDTH		2
+#define  CSR_GCFG_GCI			(_ULCAST_(0x3) << CSR_GCFG_GCI_SHIFT)
+#define  CSR_GCFG_GCI_ALL		(_ULCAST_(0x0) << CSR_GCFG_GCI_SHIFT)
+#define  CSR_GCFG_GCI_HIT		(_ULCAST_(0x1) << CSR_GCFG_GCI_SHIFT)
+#define  CSR_GCFG_GCI_SECURE		(_ULCAST_(0x2) << CSR_GCFG_GCI_SHIFT)
+#define  CSR_GCFG_GCIP_SHIFT		16
+#define  CSR_GCFG_GCIP			(_ULCAST_(0xf) << CSR_GCFG_GCIP_SHIFT)
+#define  CSR_GCFG_GCIP_ALL		(_ULCAST_(0x1) << CSR_GCFG_GCIP_SHIFT)
+#define  CSR_GCFG_GCIP_HIT		(_ULCAST_(0x1) << (CSR_GCFG_GCIP_SHIFT + 1))
+#define  CSR_GCFG_GCIP_SECURE		(_ULCAST_(0x1) << (CSR_GCFG_GCIP_SHIFT + 2))
+#define  CSR_GCFG_TORU_SHIFT		15
+#define  CSR_GCFG_TORU			(_ULCAST_(0x1) << CSR_GCFG_TORU_SHIFT)
+#define  CSR_GCFG_TORUP_SHIFT		14
+#define  CSR_GCFG_TORUP			(_ULCAST_(0x1) << CSR_GCFG_TORUP_SHIFT)
+#define  CSR_GCFG_TOP_SHIFT		13
+#define  CSR_GCFG_TOP			(_ULCAST_(0x1) << CSR_GCFG_TOP_SHIFT)
+#define  CSR_GCFG_TOPP_SHIFT		12
+#define  CSR_GCFG_TOPP			(_ULCAST_(0x1) << CSR_GCFG_TOPP_SHIFT)
+#define  CSR_GCFG_TOE_SHIFT		11
+#define  CSR_GCFG_TOE			(_ULCAST_(0x1) << CSR_GCFG_TOE_SHIFT)
+#define  CSR_GCFG_TOEP_SHIFT		10
+#define  CSR_GCFG_TOEP			(_ULCAST_(0x1) << CSR_GCFG_TOEP_SHIFT)
+#define  CSR_GCFG_TIT_SHIFT		9
+#define  CSR_GCFG_TIT			(_ULCAST_(0x1) << CSR_GCFG_TIT_SHIFT)
+#define  CSR_GCFG_TITP_SHIFT		8
+#define  CSR_GCFG_TITP			(_ULCAST_(0x1) << CSR_GCFG_TITP_SHIFT)
+#define  CSR_GCFG_SIT_SHIFT		7
+#define  CSR_GCFG_SIT			(_ULCAST_(0x1) << CSR_GCFG_SIT_SHIFT)
+#define  CSR_GCFG_SITP_SHIFT		6
+#define  CSR_GCFG_SITP			(_ULCAST_(0x1) << CSR_GCFG_SITP_SHIFT)
+#define  CSR_GCFG_MATC_SHITF		4
+#define  CSR_GCFG_MATC_WIDTH		2
+#define  CSR_GCFG_MATC_MASK		(_ULCAST_(0x3) << CSR_GCFG_MATC_SHITF)
+#define  CSR_GCFG_MATC_GUEST		(_ULCAST_(0x0) << CSR_GCFG_MATC_SHITF)
+#define  CSR_GCFG_MATC_ROOT		(_ULCAST_(0x1) << CSR_GCFG_MATC_SHITF)
+#define  CSR_GCFG_MATC_NEST		(_ULCAST_(0x2) << CSR_GCFG_MATC_SHITF)
+
+#define LOONGARCH_CSR_GINTC		0x52	/* Guest interrupt control */
+#define  CSR_GINTC_HC_SHIFT		16
+#define  CSR_GINTC_HC_WIDTH		8
+#define  CSR_GINTC_HC			(_ULCAST_(0xff) << CSR_GINTC_HC_SHIFT)
+#define  CSR_GINTC_PIP_SHIFT		8
+#define  CSR_GINTC_PIP_WIDTH		8
+#define  CSR_GINTC_PIP			(_ULCAST_(0xff) << CSR_GINTC_PIP_SHIFT)
+#define  CSR_GINTC_VIP_SHIFT		0
+#define  CSR_GINTC_VIP_WIDTH		8
+#define  CSR_GINTC_VIP			(_ULCAST_(0xff))
+
+#define LOONGARCH_CSR_GCNTC		0x53	/* Guest timer offset */
+
+/* LLBCTL register */
+#define LOONGARCH_CSR_LLBCTL		0x60	/* LLBit control */
+#define  CSR_LLBCTL_ROLLB_SHIFT		0
+#define  CSR_LLBCTL_ROLLB		(_ULCAST_(1) << CSR_LLBCTL_ROLLB_SHIFT)
+#define  CSR_LLBCTL_WCLLB_SHIFT		1
+#define  CSR_LLBCTL_WCLLB		(_ULCAST_(1) << CSR_LLBCTL_WCLLB_SHIFT)
+#define  CSR_LLBCTL_KLO_SHIFT		2
+#define  CSR_LLBCTL_KLO			(_ULCAST_(1) << CSR_LLBCTL_KLO_SHIFT)
+
+/* Implement dependent */
+#define LOONGARCH_CSR_IMPCTL1		0x80	/* Loongson config1 */
+#define  CSR_MISPEC_SHIFT		20
+#define  CSR_MISPEC_WIDTH		8
+#define  CSR_MISPEC			(_ULCAST_(0xff) << CSR_MISPEC_SHIFT)
+#define  CSR_SSEN_SHIFT			18
+#define  CSR_SSEN			(_ULCAST_(1) << CSR_SSEN_SHIFT)
+#define  CSR_SCRAND_SHIFT		17
+#define  CSR_SCRAND			(_ULCAST_(1) << CSR_SCRAND_SHIFT)
+#define  CSR_LLEXCL_SHIFT		16
+#define  CSR_LLEXCL			(_ULCAST_(1) << CSR_LLEXCL_SHIFT)
+#define  CSR_DISVC_SHIFT		15
+#define  CSR_DISVC			(_ULCAST_(1) << CSR_DISVC_SHIFT)
+#define  CSR_VCLRU_SHIFT		14
+#define  CSR_VCLRU			(_ULCAST_(1) << CSR_VCLRU_SHIFT)
+#define  CSR_DCLRU_SHIFT		13
+#define  CSR_DCLRU			(_ULCAST_(1) << CSR_DCLRU_SHIFT)
+#define  CSR_FASTLDQ_SHIFT		12
+#define  CSR_FASTLDQ			(_ULCAST_(1) << CSR_FASTLDQ_SHIFT)
+#define  CSR_USERCAC_SHIFT		11
+#define  CSR_USERCAC			(_ULCAST_(1) << CSR_USERCAC_SHIFT)
+#define  CSR_ANTI_MISPEC_SHIFT		10
+#define  CSR_ANTI_MISPEC		(_ULCAST_(1) << CSR_ANTI_MISPEC_SHIFT)
+#define  CSR_AUTO_FLUSHSFB_SHIFT	9
+#define  CSR_AUTO_FLUSHSFB		(_ULCAST_(1) << CSR_AUTO_FLUSHSFB_SHIFT)
+#define  CSR_STFILL_SHIFT		8
+#define  CSR_STFILL			(_ULCAST_(1) << CSR_STFILL_SHIFT)
+#define  CSR_LIFEP_SHIFT		7
+#define  CSR_LIFEP			(_ULCAST_(1) << CSR_LIFEP_SHIFT)
+#define  CSR_LLSYNC_SHIFT		6
+#define  CSR_LLSYNC			(_ULCAST_(1) << CSR_LLSYNC_SHIFT)
+#define  CSR_BRBTDIS_SHIFT		5
+#define  CSR_BRBTDIS			(_ULCAST_(1) << CSR_BRBTDIS_SHIFT)
+#define  CSR_RASDIS_SHIFT		4
+#define  CSR_RASDIS			(_ULCAST_(1) << CSR_RASDIS_SHIFT)
+#define  CSR_STPRE_SHIFT		2
+#define  CSR_STPRE_WIDTH		2
+#define  CSR_STPRE			(_ULCAST_(3) << CSR_STPRE_SHIFT)
+#define  CSR_INSTPRE_SHIFT		1
+#define  CSR_INSTPRE			(_ULCAST_(1) << CSR_INSTPRE_SHIFT)
+#define  CSR_DATAPRE_SHIFT		0
+#define  CSR_DATAPRE			(_ULCAST_(1) << CSR_DATAPRE_SHIFT)
+
+#define LOONGARCH_CSR_IMPCTL2		0x81	/* Loongson config2 */
+#define  CSR_FLUSH_MTLB_SHIFT		0
+#define  CSR_FLUSH_MTLB			(_ULCAST_(1) << CSR_FLUSH_MTLB_SHIFT)
+#define  CSR_FLUSH_STLB_SHIFT		1
+#define  CSR_FLUSH_STLB			(_ULCAST_(1) << CSR_FLUSH_STLB_SHIFT)
+#define  CSR_FLUSH_DTLB_SHIFT		2
+#define  CSR_FLUSH_DTLB			(_ULCAST_(1) << CSR_FLUSH_DTLB_SHIFT)
+#define  CSR_FLUSH_ITLB_SHIFT		3
+#define  CSR_FLUSH_ITLB			(_ULCAST_(1) << CSR_FLUSH_ITLB_SHIFT)
+#define  CSR_FLUSH_BTAC_SHIFT		4
+#define  CSR_FLUSH_BTAC			(_ULCAST_(1) << CSR_FLUSH_BTAC_SHIFT)
+
+#define LOONGARCH_CSR_GNMI		0x82
+
+/* TLB Refill registers */
+#define LOONGARCH_CSR_TLBRENTRY		0x88	/* TLB refill exception entry */
+#define LOONGARCH_CSR_TLBRBADV		0x89	/* TLB refill badvaddr */
+#define LOONGARCH_CSR_TLBREPC		0x8a	/* TLB refill EPC */
+#define LOONGARCH_CSR_TLBRSAVE		0x8b	/* KScratch for TLB refill exception */
+#define LOONGARCH_CSR_TLBRELO0		0x8c	/* TLB refill entrylo0 */
+#define LOONGARCH_CSR_TLBRELO1		0x8d	/* TLB refill entrylo1 */
+#define LOONGARCH_CSR_TLBREHI		0x8e	/* TLB refill entryhi */
+#define LOONGARCH_CSR_TLBRPRMD		0x8f	/* TLB refill mode info */
+
+/* Machine Error registers */
+#define LOONGARCH_CSR_MERRCTL		0x90	/* MERRCTL */
+#define LOONGARCH_CSR_MERRINFO1		0x91	/* MError info1 */
+#define LOONGARCH_CSR_MERRINFO2		0x92	/* MError info2 */
+#define LOONGARCH_CSR_MERRENTRY		0x93	/* MError exception entry */
+#define LOONGARCH_CSR_MERREPC		0x94	/* MError exception PC */
+#define LOONGARCH_CSR_MERRSAVE		0x95	/* KScratch for machine error exception */
+
+#define LOONGARCH_CSR_CTAG		0x98	/* TagLo + TagHi */
+
+#define LOONGARCH_CSR_PRID		0xc0
+
+/* Shadow MCSR : 0xc0 ~ 0xff */
+#define LOONGARCH_CSR_MCSR0		0xc0	/* CPUCFG0 and CPUCFG1 */
+#define  MCSR0_INT_IMPL_SHIFT		58
+#define  MCSR0_INT_IMPL			0
+#define  MCSR0_IOCSR_BRD_SHIFT		57
+#define  MCSR0_IOCSR_BRD		(_ULCAST_(1) << MCSR0_IOCSR_BRD_SHIFT)
+#define  MCSR0_HUGEPG_SHIFT		56
+#define  MCSR0_HUGEPG			(_ULCAST_(1) << MCSR0_HUGEPG_SHIFT)
+#define  MCSR0_RPLMTLB_SHIFT		55
+#define  MCSR0_RPLMTLB			(_ULCAST_(1) << MCSR0_RPLMTLB_SHIFT)
+#define  MCSR0_EXEPROT_SHIFT		54
+#define  MCSR0_EXEPROT			(_ULCAST_(1) << MCSR0_EXEPROT_SHIFT)
+#define  MCSR0_RI_SHIFT			53
+#define  MCSR0_RI			(_ULCAST_(1) << MCSR0_RI_SHIFT)
+#define  MCSR0_UAL_SHIFT		52
+#define  MCSR0_UAL			(_ULCAST_(1) << MCSR0_UAL_SHIFT)
+#define  MCSR0_VABIT_SHIFT		44
+#define  MCSR0_VABIT_WIDTH		8
+#define  MCSR0_VABIT			(_ULCAST_(0xff) << MCSR0_VABIT_SHIFT)
+#define  VABIT_DEFAULT			0x2f
+#define  MCSR0_PABIT_SHIFT		36
+#define  MCSR0_PABIT_WIDTH		8
+#define  MCSR0_PABIT			(_ULCAST_(0xff) << MCSR0_PABIT_SHIFT)
+#define  PABIT_DEFAULT			0x2f
+#define  MCSR0_IOCSR_SHIFT		35
+#define  MCSR0_IOCSR			(_ULCAST_(1) << MCSR0_IOCSR_SHIFT)
+#define  MCSR0_PAGING_SHIFT		34
+#define  MCSR0_PAGING			(_ULCAST_(1) << MCSR0_PAGING_SHIFT)
+#define  MCSR0_GR64_SHIFT		33
+#define  MCSR0_GR64			(_ULCAST_(1) << MCSR0_GR64_SHIFT)
+#define  GR64_DEFAULT			1
+#define  MCSR0_GR32_SHIFT		32
+#define  MCSR0_GR32			(_ULCAST_(1) << MCSR0_GR32_SHIFT)
+#define  GR32_DEFAULT			0
+#define  MCSR0_PRID_WIDTH		32
+#define  MCSR0_PRID			0x14C010
+
+#define LOONGARCH_CSR_MCSR1		0xc1	/* CPUCFG2 and CPUCFG3 */
+#define  MCSR1_HPFOLD_SHIFT		43
+#define  MCSR1_HPFOLD			(_ULCAST_(1) << MCSR1_HPFOLD_SHIFT)
+#define  MCSR1_SPW_LVL_SHIFT		40
+#define  MCSR1_SPW_LVL_WIDTH		3
+#define  MCSR1_SPW_LVL			(_ULCAST_(7) << MCSR1_SPW_LVL_SHIFT)
+#define  MCSR1_ICACHET_SHIFT		39
+#define  MCSR1_ICACHET			(_ULCAST_(1) << MCSR1_ICACHET_SHIFT)
+#define  MCSR1_ITLBT_SHIFT		38
+#define  MCSR1_ITLBT			(_ULCAST_(1) << MCSR1_ITLBT_SHIFT)
+#define  MCSR1_LLDBAR_SHIFT		37
+#define  MCSR1_LLDBAR			(_ULCAST_(1) << MCSR1_LLDBAR_SHIFT)
+#define  MCSR1_SCDLY_SHIFT		36
+#define  MCSR1_SCDLY			(_ULCAST_(1) << MCSR1_SCDLY_SHIFT)
+#define  MCSR1_LLEXC_SHIFT		35
+#define  MCSR1_LLEXC			(_ULCAST_(1) << MCSR1_LLEXC_SHIFT)
+#define  MCSR1_UCACC_SHIFT		34
+#define  MCSR1_UCACC			(_ULCAST_(1) << MCSR1_UCACC_SHIFT)
+#define  MCSR1_SFB_SHIFT		33
+#define  MCSR1_SFB			(_ULCAST_(1) << MCSR1_SFB_SHIFT)
+#define  MCSR1_CCDMA_SHIFT		32
+#define  MCSR1_CCDMA			(_ULCAST_(1) << MCSR1_CCDMA_SHIFT)
+#define  MCSR1_LAMO_SHIFT		22
+#define  MCSR1_LAMO			(_ULCAST_(1) << MCSR1_LAMO_SHIFT)
+#define  MCSR1_LSPW_SHIFT		21
+#define  MCSR1_LSPW			(_ULCAST_(1) << MCSR1_LSPW_SHIFT)
+#define  MCSR1_LOONGARCHBT_SHIFT	20
+#define  MCSR1_LOONGARCHBT		(_ULCAST_(1) << MCSR1_LOONGARCHBT_SHIFT)
+#define  MCSR1_ARMBT_SHIFT		19
+#define  MCSR1_ARMBT			(_ULCAST_(1) << MCSR1_ARMBT_SHIFT)
+#define  MCSR1_X86BT_SHIFT		18
+#define  MCSR1_X86BT			(_ULCAST_(1) << MCSR1_X86BT_SHIFT)
+#define  MCSR1_LLFTPVERS_SHIFT		15
+#define  MCSR1_LLFTPVERS_WIDTH		3
+#define  MCSR1_LLFTPVERS		(_ULCAST_(7) << MCSR1_LLFTPVERS_SHIFT)
+#define  MCSR1_LLFTP_SHIFT		14
+#define  MCSR1_LLFTP			(_ULCAST_(1) << MCSR1_LLFTP_SHIFT)
+#define  MCSR1_VZVERS_SHIFT		11
+#define  MCSR1_VZVERS_WIDTH		3
+#define  MCSR1_VZVERS			(_ULCAST_(7) << MCSR1_VZVERS_SHIFT)
+#define  MCSR1_VZ_SHIFT			10
+#define  MCSR1_VZ			(_ULCAST_(1) << MCSR1_VZ_SHIFT)
+#define  MCSR1_CRYPTO_SHIFT		9
+#define  MCSR1_CRYPTO			(_ULCAST_(1) << MCSR1_CRYPTO_SHIFT)
+#define  MCSR1_COMPLEX_SHIFT		8
+#define  MCSR1_COMPLEX			(_ULCAST_(1) << MCSR1_COMPLEX_SHIFT)
+#define  MCSR1_LASX_SHIFT		7
+#define  MCSR1_LASX			(_ULCAST_(1) << MCSR1_LASX_SHIFT)
+#define  MCSR1_LSX_SHIFT		6
+#define  MCSR1_LSX			(_ULCAST_(1) << MCSR1_LSX_SHIFT)
+#define  MCSR1_FPVERS_SHIFT		3
+#define  MCSR1_FPVERS_WIDTH		3
+#define  MCSR1_FPVERS			(_ULCAST_(7) << MCSR1_FPVERS_SHIFT)
+#define  MCSR1_FPDP_SHIFT		2
+#define  MCSR1_FPDP			(_ULCAST_(1) << MCSR1_FPDP_SHIFT)
+#define  MCSR1_FPSP_SHIFT		1
+#define  MCSR1_FPSP			(_ULCAST_(1) << MCSR1_FPSP_SHIFT)
+#define  MCSR1_FP_SHIFT			0
+#define  MCSR1_FP			(_ULCAST_(1) << MCSR1_FP_SHIFT)
+
+#define LOONGARCH_CSR_MCSR2		0xc2	/* CPUCFG4 and CPUCFG5 */
+#define  MCSR2_CCDIV_SHIFT		48
+#define  MCSR2_CCDIV_WIDTH		16
+#define  MCSR2_CCDIV			(_ULCAST_(0xffff) << MCSR2_CCDIV_SHIFT)
+#define  MCSR2_CCMUL_SHIFT		32
+#define  MCSR2_CCMUL_WIDTH		16
+#define  MCSR2_CCMUL			(_ULCAST_(0xffff) << MCSR2_CCMUL_SHIFT)
+#define  MCSR2_CCFREQ_WIDTH		32
+#define  MCSR2_CCFREQ			(_ULCAST_(0xffffffff))
+#define  CCFREQ_DEFAULT			0x5f5e100	/* 100MHz */
+
+#define LOONGARCH_CSR_MCSR3		0xc3	/* CPUCFG6 */
+#define  MCSR3_UPM_SHIFT		14
+#define  MCSR3_UPM			(_ULCAST_(1) << MCSR3_UPM_SHIFT)
+#define  MCSR3_PMBITS_SHIFT		8
+#define  MCSR3_PMBITS_WIDTH		6
+#define  MCSR3_PMBITS			(_ULCAST_(0x3f) << MCSR3_PMBITS_SHIFT)
+#define  PMBITS_DEFAULT			0x40
+#define  MCSR3_PMNUM_SHIFT		4
+#define  MCSR3_PMNUM_WIDTH		4
+#define  MCSR3_PMNUM			(_ULCAST_(0xf) << MCSR3_PMNUM_SHIFT)
+#define  MCSR3_PAMVER_SHIFT		1
+#define  MCSR3_PAMVER_WIDTH		3
+#define  MCSR3_PAMVER			(_ULCAST_(0x7) << MCSR3_PAMVER_SHIFT)
+#define  MCSR3_PMP_SHIFT		0
+#define  MCSR3_PMP			(_ULCAST_(1) << MCSR3_PMP_SHIFT)
+
+#define LOONGARCH_CSR_MCSR8		0xc8	/* CPUCFG16 and CPUCFG17 */
+#define  MCSR8_L1I_SIZE_SHIFT		56
+#define  MCSR8_L1I_SIZE_WIDTH		7
+#define  MCSR8_L1I_SIZE			(_ULCAST_(0x7f) << MCSR8_L1I_SIZE_SHIFT)
+#define  MCSR8_L1I_IDX_SHIFT		48
+#define  MCSR8_L1I_IDX_WIDTH		8
+#define  MCSR8_L1I_IDX			(_ULCAST_(0xff) << MCSR8_L1I_IDX_SHIFT)
+#define  MCSR8_L1I_WAY_SHIFT		32
+#define  MCSR8_L1I_WAY_WIDTH		16
+#define  MCSR8_L1I_WAY			(_ULCAST_(0xffff) << MCSR8_L1I_WAY_SHIFT)
+#define  MCSR8_L3DINCL_SHIFT		16
+#define  MCSR8_L3DINCL			(_ULCAST_(1) << MCSR8_L3DINCL_SHIFT)
+#define  MCSR8_L3DPRIV_SHIFT		15
+#define  MCSR8_L3DPRIV			(_ULCAST_(1) << MCSR8_L3DPRIV_SHIFT)
+#define  MCSR8_L3DPRE_SHIFT		14
+#define  MCSR8_L3DPRE			(_ULCAST_(1) << MCSR8_L3DPRE_SHIFT)
+#define  MCSR8_L3IUINCL_SHIFT		13
+#define  MCSR8_L3IUINCL			(_ULCAST_(1) << MCSR8_L3IUINCL_SHIFT)
+#define  MCSR8_L3IUPRIV_SHIFT		12
+#define  MCSR8_L3IUPRIV			(_ULCAST_(1) << MCSR8_L3IUPRIV_SHIFT)
+#define  MCSR8_L3IUUNIFY_SHIFT		11
+#define  MCSR8_L3IUUNIFY		(_ULCAST_(1) << MCSR8_L3IUUNIFY_SHIFT)
+#define  MCSR8_L3IUPRE_SHIFT		10
+#define  MCSR8_L3IUPRE			(_ULCAST_(1) << MCSR8_L3IUPRE_SHIFT)
+#define  MCSR8_L2DINCL_SHIFT		9
+#define  MCSR8_L2DINCL			(_ULCAST_(1) << MCSR8_L2DINCL_SHIFT)
+#define  MCSR8_L2DPRIV_SHIFT		8
+#define  MCSR8_L2DPRIV			(_ULCAST_(1) << MCSR8_L2DPRIV_SHIFT)
+#define  MCSR8_L2DPRE_SHIFT		7
+#define  MCSR8_L2DPRE			(_ULCAST_(1) << MCSR8_L2DPRE_SHIFT)
+#define  MCSR8_L2IUINCL_SHIFT		6
+#define  MCSR8_L2IUINCL			(_ULCAST_(1) << MCSR8_L2IUINCL_SHIFT)
+#define  MCSR8_L2IUPRIV_SHIFT		5
+#define  MCSR8_L2IUPRIV			(_ULCAST_(1) << MCSR8_L2IUPRIV_SHIFT)
+#define  MCSR8_L2IUUNIFY_SHIFT		4
+#define  MCSR8_L2IUUNIFY		(_ULCAST_(1) << MCSR8_L2IUUNIFY_SHIFT)
+#define  MCSR8_L2IUPRE_SHIFT		3
+#define  MCSR8_L2IUPRE			(_ULCAST_(1) << MCSR8_L2IUPRE_SHIFT)
+#define  MCSR8_L1DPRE_SHIFT		2
+#define  MCSR8_L1DPRE			(_ULCAST_(1) << MCSR8_L1DPRE_SHIFT)
+#define  MCSR8_L1IUUNIFY_SHIFT		1
+#define  MCSR8_L1IUUNIFY		(_ULCAST_(1) << MCSR8_L1IUUNIFY_SHIFT)
+#define  MCSR8_L1IUPRE_SHIFT		0
+#define  MCSR8_L1IUPRE			(_ULCAST_(1) << MCSR8_L1IUPRE_SHIFT)
+
+#define LOONGARCH_CSR_MCSR9		0xc9	/* CPUCFG18 and CPUCFG19 */
+#define  MCSR9_L2U_SIZE_SHIFT		56
+#define  MCSR9_L2U_SIZE_WIDTH		7
+#define  MCSR9_L2U_SIZE			(_ULCAST_(0x7f) << MCSR9_L2U_SIZE_SHIFT)
+#define  MCSR9_L2U_IDX_SHIFT		48
+#define  MCSR9_L2U_IDX_WIDTH		8
+#define  MCSR9_L2U_IDX			(_ULCAST_(0xff) << MCSR9_IDX_LOG_SHIFT)
+#define  MCSR9_L2U_WAY_SHIFT		32
+#define  MCSR9_L2U_WAY_WIDTH		16
+#define  MCSR9_L2U_WAY			(_ULCAST_(0xffff) << MCSR9_L2U_WAY_SHIFT)
+#define  MCSR9_L1D_SIZE_SHIFT		24
+#define  MCSR9_L1D_SIZE_WIDTH		7
+#define  MCSR9_L1D_SIZE			(_ULCAST_(0x7f) << MCSR9_L1D_SIZE_SHIFT)
+#define  MCSR9_L1D_IDX_SHIFT		16
+#define  MCSR9_L1D_IDX_WIDTH		8
+#define  MCSR9_L1D_IDX			(_ULCAST_(0xff) << MCSR9_L1D_IDX_SHIFT)
+#define  MCSR9_L1D_WAY_SHIFT		0
+#define  MCSR9_L1D_WAY_WIDTH		16
+#define  MCSR9_L1D_WAY			(_ULCAST_(0xffff) << MCSR9_L1D_WAY_SHIFT)
+
+#define LOONGARCH_CSR_MCSR10		0xca	/* CPUCFG20 */
+#define  MCSR10_L3U_SIZE_SHIFT		24
+#define  MCSR10_L3U_SIZE_WIDTH		7
+#define  MCSR10_L3U_SIZE		(_ULCAST_(0x7f) << MCSR10_L3U_SIZE_SHIFT)
+#define  MCSR10_L3U_IDX_SHIFT		16
+#define  MCSR10_L3U_IDX_WIDTH		8
+#define  MCSR10_L3U_IDX			(_ULCAST_(0xff) << MCSR10_L3U_IDX_SHIFT)
+#define  MCSR10_L3U_WAY_SHIFT		0
+#define  MCSR10_L3U_WAY_WIDTH		16
+#define  MCSR10_L3U_WAY			(_ULCAST_(0xffff) << MCSR10_L3U_WAY_SHIFT)
+
+#define LOONGARCH_CSR_MCSR24		0xf0	/* cpucfg48 */
+#define  MCSR24_RAMCG_SHIFT		3
+#define  MCSR24_RAMCG			(_ULCAST_(1) << MCSR24_RAMCG_SHIFT)
+#define  MCSR24_VFPUCG_SHIFT		2
+#define  MCSR24_VFPUCG			(_ULCAST_(1) << MCSR24_VFPUCG_SHIFT)
+#define  MCSR24_NAPEN_SHIFT		1
+#define  MCSR24_NAPEN			(_ULCAST_(1) << MCSR24_NAPEN_SHIFT)
+#define  MCSR24_MCSRLOCK_SHIFT		0
+#define  MCSR24_MCSRLOCK		(_ULCAST_(1) << MCSR24_MCSRLOCK_SHIFT)
+
+/* Uncached accelerate windows registers */
+#define LOONGARCH_CSR_UCAWIN		0x100
+#define LOONGARCH_CSR_UCAWIN0_LO	0x102
+#define LOONGARCH_CSR_UCAWIN0_HI	0x103
+#define LOONGARCH_CSR_UCAWIN1_LO	0x104
+#define LOONGARCH_CSR_UCAWIN1_HI	0x105
+#define LOONGARCH_CSR_UCAWIN2_LO	0x106
+#define LOONGARCH_CSR_UCAWIN2_HI	0x107
+#define LOONGARCH_CSR_UCAWIN3_LO	0x108
+#define LOONGARCH_CSR_UCAWIN3_HI	0x109
+
+/* Direct Map windows registers */
+#define LOONGARCH_CSR_DMWIN0		0x180	/* 64 direct map win0: MEM & IF */
+#define LOONGARCH_CSR_DMWIN1		0x181	/* 64 direct map win1: MEM & IF */
+#define LOONGARCH_CSR_DMWIN2		0x182	/* 64 direct map win2: MEM */
+#define LOONGARCH_CSR_DMWIN3		0x183	/* 64 direct map win3: MEM */
+
+/* Direct Map window 0/1 */
+#define CSR_DMW0_PLV0		_CONST64_(1 << 0)
+#define CSR_DMW0_VSEG		_CONST64_(0x8000)
+#define CSR_DMW0_BASE		(CSR_DMW0_VSEG << DMW_PABITS)
+#define CSR_DMW0_INIT		(CSR_DMW0_BASE | CSR_DMW0_PLV0)
+
+#define CSR_DMW1_PLV0		_CONST64_(1 << 0)
+#define CSR_DMW1_MAT		_CONST64_(1 << 4)
+#define CSR_DMW1_VSEG		_CONST64_(0x9000)
+#define CSR_DMW1_BASE		(CSR_DMW1_VSEG << DMW_PABITS)
+#define CSR_DMW1_INIT		(CSR_DMW1_BASE | CSR_DMW1_MAT | CSR_DMW1_PLV0)
+
+/* Performance Counter registers */
+#define LOONGARCH_CSR_PERFCTRL0		0x200	/* 32 perf event 0 config */
+#define LOONGARCH_CSR_PERFCNTR0		0x201	/* 64 perf event 0 count value */
+#define LOONGARCH_CSR_PERFCTRL1		0x202	/* 32 perf event 1 config */
+#define LOONGARCH_CSR_PERFCNTR1		0x203	/* 64 perf event 1 count value */
+#define LOONGARCH_CSR_PERFCTRL2		0x204	/* 32 perf event 2 config */
+#define LOONGARCH_CSR_PERFCNTR2		0x205	/* 64 perf event 2 count value */
+#define LOONGARCH_CSR_PERFCTRL3		0x206	/* 32 perf event 3 config */
+#define LOONGARCH_CSR_PERFCNTR3		0x207	/* 64 perf event 3 count value */
+#define  CSR_PERFCTRL_PLV0		(_ULCAST_(1) << 16)
+#define  CSR_PERFCTRL_PLV1		(_ULCAST_(1) << 17)
+#define  CSR_PERFCTRL_PLV2		(_ULCAST_(1) << 18)
+#define  CSR_PERFCTRL_PLV3		(_ULCAST_(1) << 19)
+#define  CSR_PERFCTRL_IE		(_ULCAST_(1) << 20)
+#define  CSR_PERFCTRL_EVENT		0x3ff
+
+/* Debug registers */
+#define LOONGARCH_CSR_MWPC		0x300	/* data breakpoint config */
+#define LOONGARCH_CSR_MWPS		0x301	/* data breakpoint status */
+
+#define LOONGARCH_CSR_DB0ADDR		0x310	/* data breakpoint 0 address */
+#define LOONGARCH_CSR_DB0MASK		0x311	/* data breakpoint 0 mask */
+#define LOONGARCH_CSR_DB0CTL		0x312	/* data breakpoint 0 control */
+#define LOONGARCH_CSR_DB0ASID		0x313	/* data breakpoint 0 asid */
+
+#define LOONGARCH_CSR_DB1ADDR		0x318	/* data breakpoint 1 address */
+#define LOONGARCH_CSR_DB1MASK		0x319	/* data breakpoint 1 mask */
+#define LOONGARCH_CSR_DB1CTL		0x31a	/* data breakpoint 1 control */
+#define LOONGARCH_CSR_DB1ASID		0x31b	/* data breakpoint 1 asid */
+
+#define LOONGARCH_CSR_DB2ADDR		0x320	/* data breakpoint 2 address */
+#define LOONGARCH_CSR_DB2MASK		0x321	/* data breakpoint 2 mask */
+#define LOONGARCH_CSR_DB2CTL		0x322	/* data breakpoint 2 control */
+#define LOONGARCH_CSR_DB2ASID		0x323	/* data breakpoint 2 asid */
+
+#define LOONGARCH_CSR_DB3ADDR		0x328	/* data breakpoint 3 address */
+#define LOONGARCH_CSR_DB3MASK		0x329	/* data breakpoint 3 mask */
+#define LOONGARCH_CSR_DB3CTL		0x32a	/* data breakpoint 3 control */
+#define LOONGARCH_CSR_DB3ASID		0x32b	/* data breakpoint 3 asid */
+
+#define LOONGARCH_CSR_DB4ADDR		0x330	/* data breakpoint 4 address */
+#define LOONGARCH_CSR_DB4MASK		0x331	/* data breakpoint 4 maks */
+#define LOONGARCH_CSR_DB4CTL		0x332	/* data breakpoint 4 control */
+#define LOONGARCH_CSR_DB4ASID		0x333	/* data breakpoint 4 asid */
+
+#define LOONGARCH_CSR_DB5ADDR		0x338	/* data breakpoint 5 address */
+#define LOONGARCH_CSR_DB5MASK		0x339	/* data breakpoint 5 mask */
+#define LOONGARCH_CSR_DB5CTL		0x33a	/* data breakpoint 5 control */
+#define LOONGARCH_CSR_DB5ASID		0x33b	/* data breakpoint 5 asid */
+
+#define LOONGARCH_CSR_DB6ADDR		0x340	/* data breakpoint 6 address */
+#define LOONGARCH_CSR_DB6MASK		0x341	/* data breakpoint 6 mask */
+#define LOONGARCH_CSR_DB6CTL		0x342	/* data breakpoint 6 control */
+#define LOONGARCH_CSR_DB6ASID		0x343	/* data breakpoint 6 asid */
+
+#define LOONGARCH_CSR_DB7ADDR		0x348	/* data breakpoint 7 address */
+#define LOONGARCH_CSR_DB7MASK		0x349	/* data breakpoint 7 mask */
+#define LOONGARCH_CSR_DB7CTL		0x34a	/* data breakpoint 7 control */
+#define LOONGARCH_CSR_DB7ASID		0x34b	/* data breakpoint 7 asid */
+
+#define LOONGARCH_CSR_FWPC		0x380	/* instruction breakpoint config */
+#define LOONGARCH_CSR_FWPS		0x381	/* instruction breakpoint status */
+
+#define LOONGARCH_CSR_IB0ADDR		0x390	/* inst breakpoint 0 address */
+#define LOONGARCH_CSR_IB0MASK		0x391	/* inst breakpoint 0 mask */
+#define LOONGARCH_CSR_IB0CTL		0x392	/* inst breakpoint 0 control */
+#define LOONGARCH_CSR_IB0ASID		0x393	/* inst breakpoint 0 asid */
+
+#define LOONGARCH_CSR_IB1ADDR		0x398	/* inst breakpoint 1 address */
+#define LOONGARCH_CSR_IB1MASK		0x399	/* inst breakpoint 1 mask */
+#define LOONGARCH_CSR_IB1CTL		0x39a	/* inst breakpoint 1 control */
+#define LOONGARCH_CSR_IB1ASID		0x39b	/* inst breakpoint 1 asid */
+
+#define LOONGARCH_CSR_IB2ADDR		0x3a0	/* inst breakpoint 2 address */
+#define LOONGARCH_CSR_IB2MASK		0x3a1	/* inst breakpoint 2 mask */
+#define LOONGARCH_CSR_IB2CTL		0x3a2	/* inst breakpoint 2 control */
+#define LOONGARCH_CSR_IB2ASID		0x3a3	/* inst breakpoint 2 asid */
+
+#define LOONGARCH_CSR_IB3ADDR		0x3a8	/* inst breakpoint 3 address */
+#define LOONGARCH_CSR_IB3MASK		0x3a9	/* breakpoint 3 mask */
+#define LOONGARCH_CSR_IB3CTL		0x3aa	/* inst breakpoint 3 control */
+#define LOONGARCH_CSR_IB3ASID		0x3ab	/* inst breakpoint 3 asid */
+
+#define LOONGARCH_CSR_IB4ADDR		0x3b0	/* inst breakpoint 4 address */
+#define LOONGARCH_CSR_IB4MASK		0x3b1	/* inst breakpoint 4 mask */
+#define LOONGARCH_CSR_IB4CTL		0x3b2	/* inst breakpoint 4 control */
+#define LOONGARCH_CSR_IB4ASID		0x3b3	/* inst breakpoint 4 asid */
+
+#define LOONGARCH_CSR_IB5ADDR		0x3b8	/* inst breakpoint 5 address */
+#define LOONGARCH_CSR_IB5MASK		0x3b9	/* inst breakpoint 5 mask */
+#define LOONGARCH_CSR_IB5CTL		0x3ba	/* inst breakpoint 5 control */
+#define LOONGARCH_CSR_IB5ASID		0x3bb	/* inst breakpoint 5 asid */
+
+#define LOONGARCH_CSR_IB6ADDR		0x3c0	/* inst breakpoint 6 address */
+#define LOONGARCH_CSR_IB6MASK		0x3c1	/* inst breakpoint 6 mask */
+#define LOONGARCH_CSR_IB6CTL		0x3c2	/* inst breakpoint 6 control */
+#define LOONGARCH_CSR_IB6ASID		0x3c3	/* inst breakpoint 6 asid */
+
+#define LOONGARCH_CSR_IB7ADDR		0x3c8	/* inst breakpoint 7 address */
+#define LOONGARCH_CSR_IB7MASK		0x3c9	/* inst breakpoint 7 mask */
+#define LOONGARCH_CSR_IB7CTL		0x3ca	/* inst breakpoint 7 control */
+#define LOONGARCH_CSR_IB7ASID		0x3cb	/* inst breakpoint 7 asid */
+
+#define LOONGARCH_CSR_DEBUG		0x500	/* debug config */
+#define LOONGARCH_CSR_DEPC		0x501	/* debug epc */
+#define LOONGARCH_CSR_DESAVE		0x502	/* debug save */
+
+/*
+ * CSR_ECFG IM
+ */
+#define ECFG0_IM		0x00001fff
+#define ECFGB_SIP0		0
+#define ECFGF_SIP0		(_ULCAST_(1) << ECFGB_SIP0)
+#define ECFGB_SIP1		1
+#define ECFGF_SIP1		(_ULCAST_(1) << ECFGB_SIP1)
+#define ECFGB_IP0		2
+#define ECFGF_IP0		(_ULCAST_(1) << ECFGB_IP0)
+#define ECFGB_IP1		3
+#define ECFGF_IP1		(_ULCAST_(1) << ECFGB_IP1)
+#define ECFGB_IP2		4
+#define ECFGF_IP2		(_ULCAST_(1) << ECFGB_IP2)
+#define ECFGB_IP3		5
+#define ECFGF_IP3		(_ULCAST_(1) << ECFGB_IP3)
+#define ECFGB_IP4		6
+#define ECFGF_IP4		(_ULCAST_(1) << ECFGB_IP4)
+#define ECFGB_IP5		7
+#define ECFGF_IP5		(_ULCAST_(1) << ECFGB_IP5)
+#define ECFGB_IP6		8
+#define ECFGF_IP6		(_ULCAST_(1) << ECFGB_IP6)
+#define ECFGB_IP7		9
+#define ECFGF_IP7		(_ULCAST_(1) << ECFGB_IP7)
+#define ECFGB_PC		10
+#define ECFGF_PC		(_ULCAST_(1) << ECFGB_PC)
+#define ECFGB_TIMER		11
+#define ECFGF_TIMER		(_ULCAST_(1) << ECFGB_TIMER)
+#define ECFGB_IPI		12
+#define ECFGF_IPI		(_ULCAST_(1) << ECFGB_IPI)
+#define ECFGF(hwirq)		(_ULCAST_(1) << hwirq)
+
+#define ESTATF_IP		0x00001fff
+
+#define LOONGARCH_IOCSR_FEATURES	0x8
+#define  IOCSRF_TEMP			BIT_ULL(0)
+#define  IOCSRF_NODECNT			BIT_ULL(1)
+#define  IOCSRF_MSI			BIT_ULL(2)
+#define  IOCSRF_EXTIOI			BIT_ULL(3)
+#define  IOCSRF_CSRIPI			BIT_ULL(4)
+#define  IOCSRF_FREQCSR			BIT_ULL(5)
+#define  IOCSRF_FREQSCALE		BIT_ULL(6)
+#define  IOCSRF_DVFSV1			BIT_ULL(7)
+#define  IOCSRF_GMOD			BIT_ULL(9)
+#define  IOCSRF_VM			BIT_ULL(11)
+
+#define LOONGARCH_IOCSR_VENDOR		0x10
+
+#define LOONGARCH_IOCSR_CPUNAME		0x20
+
+#define LOONGARCH_IOCSR_NODECNT		0x408
+
+#define LOONGARCH_IOCSR_MISC_FUNC	0x420
+#define  IOCSR_MISC_FUNC_TIMER_RESET	BIT_ULL(21)
+#define  IOCSR_MISC_FUNC_EXT_IOI_EN	BIT_ULL(48)
+
+#define LOONGARCH_IOCSR_CPUTEMP		0x428
+
+/* PerCore CSR, only accessible by local cores */
+#define LOONGARCH_IOCSR_IPI_STATUS	0x1000
+#define LOONGARCH_IOCSR_IPI_EN		0x1004
+#define LOONGARCH_IOCSR_IPI_SET		0x1008
+#define LOONGARCH_IOCSR_IPI_CLEAR	0x100c
+#define LOONGARCH_IOCSR_MBUF0		0x1020
+#define LOONGARCH_IOCSR_MBUF1		0x1028
+#define LOONGARCH_IOCSR_MBUF2		0x1030
+#define LOONGARCH_IOCSR_MBUF3		0x1038
+
+#define LOONGARCH_IOCSR_IPI_SEND	0x1040
+#define  IOCSR_IPI_SEND_IP_SHIFT	0
+#define  IOCSR_IPI_SEND_CPU_SHIFT	16
+#define  IOCSR_IPI_SEND_BLOCKING	BIT(31)
+
+#define LOONGARCH_IOCSR_MBUF_SEND	0x1048
+#define  IOCSR_MBUF_SEND_BLOCKING	BIT_ULL(31)
+#define  IOCSR_MBUF_SEND_BOX_SHIFT	2
+#define  IOCSR_MBUF_SEND_BOX_LO(box)	(box << 1)
+#define  IOCSR_MBUF_SEND_BOX_HI(box)	((box << 1) + 1)
+#define  IOCSR_MBUF_SEND_CPU_SHIFT	16
+#define  IOCSR_MBUF_SEND_BUF_SHIFT	32
+#define  IOCSR_MBUF_SEND_H32_MASK	0xFFFFFFFF00000000ULL
+
+#define LOONGARCH_IOCSR_ANY_SEND	0x1158
+#define  IOCSR_ANY_SEND_BLOCKING	BIT_ULL(31)
+#define  IOCSR_ANY_SEND_NODE_SHIFT	18
+#define  IOCSR_ANY_SEND_MASK_SHIFT	27
+#define  IOCSR_ANY_SEND_BUF_SHIFT	32
+#define  IOCSR_ANY_SEND_H32_MASK	0xFFFFFFFF00000000ULL
+
+/* Register offset and bit definition for CSR access */
+#define LOONGARCH_IOCSR_TIMER_CFG       0x1060
+#define LOONGARCH_IOCSR_TIMER_TICK      0x1070
+#define  IOCSR_TIMER_CFG_RESERVED       (_ULCAST_(1) << 63)
+#define  IOCSR_TIMER_CFG_PERIODIC       (_ULCAST_(1) << 62)
+#define  IOCSR_TIMER_CFG_EN             (_ULCAST_(1) << 61)
+#define  IOCSR_TIMER_MASK		0x0ffffffffffffULL
+#define  IOCSR_TIMER_INITVAL_RST        (_ULCAST_(0xffff) << 48)
+
+#define LOONGARCH_IOCSR_EXTIOI_NODEMAP_BASE	0x14a0
+#define LOONGARCH_IOCSR_EXTIOI_IPMAP_BASE	0x14c0
+#define LOONGARCH_IOCSR_EXTIOI_EN_BASE		0x1600
+#define LOONGARCH_IOCSR_EXTIOI_BOUNCE_BASE	0x1680
+#define LOONGARCH_IOCSR_EXTIOI_ISR_BASE		0x1800
+#define LOONGARCH_IOCSR_EXTIOI_ROUTE_BASE	0x1c00
+#define IOCSR_EXTIOI_VECTOR_NUM			256
+
+#ifndef __ASSEMBLY__
+
+static inline u64 drdtime(void)
+{
+	int rID = 0;
+	u64 val = 0;
+
+	__asm__ __volatile__(
+		"rdtime.d %0, %1 \n\t"
+		: "=r"(val), "=r"(rID)
+		:
+		);
+	return val;
+}
+
+static inline unsigned int get_csr_cpuid(void)
+{
+	return csr_readl(LOONGARCH_CSR_CPUID);
+}
+
+static inline void csr_any_send(unsigned int addr, unsigned int data,
+				unsigned int data_mask, unsigned int node)
+{
+	uint64_t val = 0;
+
+	val = IOCSR_ANY_SEND_BLOCKING | addr;
+	val |= (node << IOCSR_ANY_SEND_NODE_SHIFT);
+	val |= (data_mask << IOCSR_ANY_SEND_MASK_SHIFT);
+	val |= ((uint64_t)data << IOCSR_ANY_SEND_BUF_SHIFT);
+	__iocsrwr_d(val, LOONGARCH_IOCSR_ANY_SEND);
+}
+
+static inline unsigned int read_csr_excode(void)
+{
+	return (csr_readl(LOONGARCH_CSR_ESTAT) & CSR_ESTAT_EXC) >> CSR_ESTAT_EXC_SHIFT;
+}
+
+static inline void write_csr_index(unsigned int idx)
+{
+	__csrxchg(idx, CSR_TLBIDX_IDXM, LOONGARCH_CSR_TLBIDX);
+}
+
+static inline unsigned int read_csr_pagesize(void)
+{
+	return (__csrrd(LOONGARCH_CSR_TLBIDX) & CSR_TLBIDX_SIZEM) >> CSR_TLBIDX_SIZE;
+}
+
+static inline void write_csr_pagesize(unsigned int size)
+{
+	__csrxchg(size << CSR_TLBIDX_SIZE, CSR_TLBIDX_SIZEM, LOONGARCH_CSR_TLBIDX);
+}
+
+#define read_csr_asid()			__csrrd(LOONGARCH_CSR_ASID)
+#define write_csr_asid(val)		__csrwr(val, LOONGARCH_CSR_ASID)
+#define read_csr_entryhi()		__dcsrrd(LOONGARCH_CSR_TLBEHI)
+#define write_csr_entryhi(val)		__dcsrwr(val, LOONGARCH_CSR_TLBEHI)
+#define read_csr_entrylo0()		__dcsrrd(LOONGARCH_CSR_TLBELO0)
+#define write_csr_entrylo0(val)		__dcsrwr(val, LOONGARCH_CSR_TLBELO0)
+#define read_csr_entrylo1()		__dcsrrd(LOONGARCH_CSR_TLBELO1)
+#define write_csr_entrylo1(val)		__dcsrwr(val, LOONGARCH_CSR_TLBELO1)
+#define read_csr_ecfg()			__csrrd(LOONGARCH_CSR_ECFG)
+#define write_csr_ecfg(val)		__csrwr(val, LOONGARCH_CSR_ECFG)
+#define read_csr_estat()		__csrrd(LOONGARCH_CSR_ESTAT)
+#define write_csr_estat(val)		__csrwr(val, LOONGARCH_CSR_ESTAT)
+#define read_csr_tlbidx()		__csrrd(LOONGARCH_CSR_TLBIDX)
+#define write_csr_tlbidx(val)		__csrwr(val, LOONGARCH_CSR_TLBIDX)
+#define read_csr_euen()			__csrrd(LOONGARCH_CSR_EUEN)
+#define write_csr_euen(val)		__csrwr(val, LOONGARCH_CSR_EUEN)
+#define read_csr_cpuid()		__csrrd(LOONGARCH_CSR_CPUID)
+#define read_csr_prcfg1()		__dcsrrd(LOONGARCH_CSR_PRCFG1)
+#define write_csr_prcfg1(val)		__dcsrwr(val, LOONGARCH_CSR_PRCFG1)
+#define read_csr_prcfg2()		__dcsrrd(LOONGARCH_CSR_PRCFG2)
+#define write_csr_prcfg2(val)		__dcsrwr(val, LOONGARCH_CSR_PRCFG2)
+#define read_csr_prcfg3()		__dcsrrd(LOONGARCH_CSR_PRCFG3)
+#define write_csr_prcfg3(val)		__dcsrwr(val, LOONGARCH_CSR_PRCFG3)
+#define read_csr_stlbpgsize()		__dcsrrd(LOONGARCH_CSR_STLBPGSIZE)
+#define write_csr_stlbpgsize(val)	__dcsrwr(val, LOONGARCH_CSR_STLBPGSIZE)
+#define read_csr_rvacfg()		__dcsrrd(LOONGARCH_CSR_RVACFG)
+#define write_csr_rvacfg(val)		__dcsrwr(val, LOONGARCH_CSR_RVACFG)
+#define write_csr_tintclear(val)	__dcsrwr(val, LOONGARCH_CSR_TINTCLR)
+#define read_csr_impctl1()		__dcsrrd(LOONGARCH_CSR_IMPCTL1)
+#define write_csr_impctl1(val)		__dcsrwr(val, LOONGARCH_CSR_IMPCTL1)
+#define write_csr_impctl2(val)		__dcsrwr(val, LOONGARCH_CSR_IMPCTL2)
+
+#define read_csr_perfctrl0()		__dcsrrd(LOONGARCH_CSR_PERFCTRL0)
+#define read_csr_perfcntr0()		__dcsrrd(LOONGARCH_CSR_PERFCNTR0)
+#define read_csr_perfctrl1()		__dcsrrd(LOONGARCH_CSR_PERFCTRL1)
+#define read_csr_perfcntr1()		__dcsrrd(LOONGARCH_CSR_PERFCNTR1)
+#define read_csr_perfctrl2()		__dcsrrd(LOONGARCH_CSR_PERFCTRL2)
+#define read_csr_perfcntr2()		__dcsrrd(LOONGARCH_CSR_PERFCNTR2)
+#define read_csr_perfctrl3()		__dcsrrd(LOONGARCH_CSR_PERFCTRL3)
+#define read_csr_perfcntr3()		__dcsrrd(LOONGARCH_CSR_PERFCNTR3)
+#define write_csr_perfctrl0(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCTRL0)
+#define write_csr_perfcntr0(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCNTR0)
+#define write_csr_perfctrl1(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCTRL1)
+#define write_csr_perfcntr1(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCNTR1)
+#define write_csr_perfctrl2(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCTRL2)
+#define write_csr_perfcntr2(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCNTR2)
+#define write_csr_perfctrl3(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCTRL3)
+#define write_csr_perfcntr3(val)	__dcsrwr(val, LOONGARCH_CSR_PERFCNTR3)
+
+/*
+ * Manipulate bits in a register.
+ */
+#define __BUILD_CSR_COMMON(name)				\
+static inline unsigned long					\
+set_##name(unsigned long set)					\
+{								\
+	unsigned long res, new;					\
+								\
+	res = read_##name();					\
+	new = res | set;					\
+	write_##name(new);					\
+								\
+	return res;						\
+}								\
+								\
+static inline unsigned long					\
+clear_##name(unsigned long clear)				\
+{								\
+	unsigned long res, new;					\
+								\
+	res = read_##name();					\
+	new = res & ~clear;					\
+	write_##name(new);					\
+								\
+	return res;						\
+}								\
+								\
+static inline unsigned long					\
+change_##name(unsigned long change, unsigned long val)		\
+{								\
+	unsigned long res, new;					\
+								\
+	res = read_##name();					\
+	new = res & ~change;					\
+	new |= (val & change);					\
+	write_##name(new);					\
+								\
+	return res;						\
+}
+
+#define __BUILD_CSR_OP(name)	__BUILD_CSR_COMMON(csr_##name)
+
+__BUILD_CSR_OP(euen)
+__BUILD_CSR_OP(ecfg)
+__BUILD_CSR_OP(tlbidx)
+
+#define set_csr_estat(val)	\
+	__dcsrxchg(val, val, LOONGARCH_CSR_ESTAT)
+#define clear_csr_estat(val)	\
+	__dcsrxchg(~(val), val, LOONGARCH_CSR_ESTAT)
+
+#endif /* __ASSEMBLY__ */
+
+/* Generic EntryLo bit definitions */
+#define ENTRYLO_V		(_ULCAST_(1) << 0)
+#define ENTRYLO_D		(_ULCAST_(1) << 1)
+#define ENTRYLO_PLV_SHIFT	2
+#define ENTRYLO_PLV		(_ULCAST_(3) << ENTRYLO_PLV_SHIFT)
+#define ENTRYLO_C_SHIFT		4
+#define ENTRYLO_C		(_ULCAST_(3) << ENTRYLO_C_SHIFT)
+#define ENTRYLO_G		(_ULCAST_(1) << 6)
+#define ENTRYLO_RI		(_ULCAST_(1) << 61)
+#define ENTRYLO_XI		(_ULCAST_(1) << 62)
+
+/* LoongArch GlobalNumber definitions */
+#define LOONGARCH_GLOBALNUMBER_VP_SHF	0
+#define LOONGARCH_GLOBALNUMBER_VP		(_ULCAST_(0xff) << LOONGARCH_GLOBALNUMBER_VP_SHF)
+#define LOONGARCH_GLOBALNUMBER_CORE_SHF	8
+#define LOONGARCH_GLOBALNUMBER_CORE		(_ULCAST_(0xff) << LOONGARCH_GLOBALNUMBER_CORE_SHF)
+#define LOONGARCH_GLOBALNUMBER_CLUSTER_SHF	16
+#define LOONGARCH_GLOBALNUMBER_CLUSTER	(_ULCAST_(0xf) << LOONGARCH_GLOBALNUMBER_CLUSTER_SHF)
+
+/* Values for PageMask register */
+#define PM_4K		0x00000000
+#define PM_8K		0x00002000
+#define PM_16K		0x00006000
+#define PM_32K		0x0000e000
+#define PM_64K		0x0001e000
+#define PM_128K		0x0003e000
+#define PM_256K		0x0007e000
+#define PM_512K		0x000fe000
+#define PM_1M		0x001fe000
+#define PM_2M		0x003fe000
+#define PM_4M		0x007fe000
+#define PM_8M		0x00ffe000
+#define PM_16M		0x01ffe000
+#define PM_32M		0x03ffe000
+#define PM_64M		0x07ffe000
+#define PM_256M		0x1fffe000
+#define PM_1G		0x7fffe000
+
+#define PS_4K		0x0000000c
+#define PS_8K		0x0000000d
+#define PS_16K		0x0000000e
+#define PS_32K		0x0000000f
+#define PS_64K		0x00000010
+#define PS_128K		0x00000011
+#define PS_256K		0x00000012
+#define PS_512K		0x00000013
+#define PS_1M		0x00000014
+#define PS_2M		0x00000015
+#define PS_4M		0x00000016
+#define PS_8M		0x00000017
+#define PS_16M		0x00000018
+#define PS_32M		0x00000019
+#define PS_64M		0x0000001a
+#define PS_256M		0x0000001b
+#define PS_1G		0x0000001c
+
+#define PS_MASK		0x3f000000
+#define PS_SHIFT	24
+
+/* Default page size for a given kernel configuration */
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PM_DEFAULT_MASK PM_4K
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+#define PM_DEFAULT_MASK PM_16K
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+#define PM_DEFAULT_MASK PM_64K
+#else
+#error Bad page size configuration!
+#endif
+
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PS_DEFAULT_SIZE PS_4K
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+#define PS_DEFAULT_SIZE PS_16K
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+#define PS_DEFAULT_SIZE PS_64K
+#else
+#error Bad page size configuration!
+#endif
+
+/* Default huge tlb size for a given kernel configuration */
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PM_HUGE_MASK   PM_1M
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+#define PM_HUGE_MASK   PM_16M
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+#define PM_HUGE_MASK   PM_256M
+#else
+#error Bad page size configuration for hugetlbfs!
+#endif
+
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PS_HUGE_SIZE   PS_1M
+#elif defined(CONFIG_PAGE_SIZE_16KB)
+#define PS_HUGE_SIZE   PS_16M
+#elif defined(CONFIG_PAGE_SIZE_64KB)
+#define PS_HUGE_SIZE   PS_256M
+#else
+#error Bad page size configuration for hugetlbfs!
+#endif
+
+/* Values used for computation of new tlb entries */
+#define PL_4K		12
+#define PL_16K		14
+#define PL_64K		16
+#define PL_256K		18
+#define PL_1M		20
+#define PL_4M		22
+#define PL_16M		24
+#define PL_64M		26
+#define PL_256M		28
+
+/* ExStatus.ExcCode */
+#define EXCCODE_RSV		0	/* Reserved */
+#define EXCCODE_TLBL		1	/* TLB miss on a load */
+#define EXCCODE_TLBS		2	/* TLB miss on a store */
+#define EXCCODE_TLBI		3	/* TLB miss on a ifetch */
+#define EXCCODE_TLBM		4	/* TLB modified fault */
+#define EXCCODE_TLBRI		5	/* TLB Read-Inhibit exception */
+#define EXCCODE_TLBXI		6	/* TLB Execution-Inhibit exception */
+#define EXCCODE_TLBPE		7	/* TLB Privilege Error */
+#define EXCCODE_ADE		8	/* Address Error */
+	#define EXSUBCODE_ADEF		0	/* Fetch Instruction */
+	#define EXSUBCODE_ADEM		1	/* Access Memory*/
+#define EXCCODE_ALE		9	/* Unalign Access */
+#define EXCCODE_OOB		10	/* Out of bounds */
+#define EXCCODE_SYS		11	/* System call */
+#define EXCCODE_BP		12	/* Breakpoint */
+#define EXCCODE_INE		13	/* Inst. Not Exist */
+#define EXCCODE_IPE		14	/* Inst. Privileged Error */
+#define EXCCODE_FPDIS		15	/* FPU Disabled */
+#define EXCCODE_LSXDIS		16	/* LSX Disabled */
+#define EXCCODE_LASXDIS		17	/* LASX Disabled */
+#define EXCCODE_FPE		18	/* Floating Point Exception */
+	#define EXCSUBCODE_FPE		0	/* Floating Point Exception */
+	#define EXCSUBCODE_VFPE		1	/* Vector Exception */
+#define EXCCODE_WATCH		19	/* Watch address reference */
+#define EXCCODE_BTDIS		20	/* Binary Trans. Disabled */
+#define EXCCODE_BTE		21	/* Binary Trans. Exception */
+#define EXCCODE_PSI		22	/* Guest Privileged Error */
+#define EXCCODE_HYP		23	/* Hypercall */
+#define EXCCODE_GCM		24	/* Guest CSR modified */
+	#define EXCSUBCODE_GCSC		0	/* Software caused */
+	#define EXCSUBCODE_GCHC		1	/* Hardware caused */
+#define EXCCODE_SE		25	/* Security */
+
+#define EXCCODE_INT_START   64
+#define EXCCODE_SIP0        64
+#define EXCCODE_SIP1        65
+#define EXCCODE_IP0         66
+#define EXCCODE_IP1         67
+#define EXCCODE_IP2         68
+#define EXCCODE_IP3         69
+#define EXCCODE_IP4         70
+#define EXCCODE_IP5         71
+#define EXCCODE_IP6         72
+#define EXCCODE_IP7         73
+#define EXCCODE_PC          74 /* Performance Counter */
+#define EXCCODE_TIMER       75
+#define EXCCODE_IPI         76
+#define EXCCODE_NMI         77
+#define EXCCODE_INT_END     78
+#define EXCCODE_INT_NUM	    (EXCCODE_INT_END - EXCCODE_INT_START)
+
+/* FPU register names */
+#define LOONGARCH_FCSR0	$r0
+#define LOONGARCH_FCSR1	$r1
+#define LOONGARCH_FCSR2	$r2
+#define LOONGARCH_FCSR3	$r3
+
+/* FPU Status Register Values */
+#define FPU_CSR_RSVD	0xe0e0fce0
+
+/*
+ * X the exception cause indicator
+ * E the exception enable
+ * S the sticky/flag bit
+ */
+#define FPU_CSR_ALL_X	0x1f000000
+#define FPU_CSR_INV_X	0x10000000
+#define FPU_CSR_DIV_X	0x08000000
+#define FPU_CSR_OVF_X	0x04000000
+#define FPU_CSR_UDF_X	0x02000000
+#define FPU_CSR_INE_X	0x01000000
+
+#define FPU_CSR_ALL_S	0x001f0000
+#define FPU_CSR_INV_S	0x00100000
+#define FPU_CSR_DIV_S	0x00080000
+#define FPU_CSR_OVF_S	0x00040000
+#define FPU_CSR_UDF_S	0x00020000
+#define FPU_CSR_INE_S	0x00010000
+
+#define FPU_CSR_ALL_E	0x0000001f
+#define FPU_CSR_INV_E	0x00000010
+#define FPU_CSR_DIV_E	0x00000008
+#define FPU_CSR_OVF_E	0x00000004
+#define FPU_CSR_UDF_E	0x00000002
+#define FPU_CSR_INE_E	0x00000001
+
+/* Bits 8 and 9 of FPU Status Register specify the rounding mode */
+#define FPU_CSR_RM	0x300
+#define FPU_CSR_RN	0x000	/* nearest */
+#define FPU_CSR_RZ	0x100	/* towards zero */
+#define FPU_CSR_RU	0x200	/* towards +Infinity */
+#define FPU_CSR_RD	0x300	/* towards -Infinity */
+
+#define write_fcsr(dest, val) \
+do {	\
+	__asm__ __volatile__(	\
+	"	movgr2fcsr	%0, "STR(dest)"	\n"	\
+	: : "r" (val));	\
+} while (0)
+
+#define read_fcsr(source)	\
+({	\
+	unsigned int __res;	\
+\
+	__asm__ __volatile__(	\
+	"	movfcsr2gr	%0, "STR(source)"	\n"	\
+	: "=r" (__res));	\
+	__res;	\
+})
+
+#endif /* _ASM_LOONGARCHREGS_H */
diff --git a/arch/loongarch/include/asm/page.h b/arch/loongarch/include/asm/page.h
new file mode 100644
index 000000000000..50f4b3032ca4
--- /dev/null
+++ b/arch/loongarch/include/asm/page.h
@@ -0,0 +1,133 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_PAGE_H
+#define _ASM_PAGE_H
+
+#include <linux/const.h>
+
+/*
+ * PAGE_SHIFT determines the page size
+ */
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PAGE_SHIFT	12
+#endif
+#ifdef CONFIG_PAGE_SIZE_16KB
+#define PAGE_SHIFT	14
+#endif
+#ifdef CONFIG_PAGE_SIZE_64KB
+#define PAGE_SHIFT	16
+#endif
+#define PAGE_SIZE	(_AC(1, UL) << PAGE_SHIFT)
+#define PAGE_MASK	(~(PAGE_SIZE - 1))
+
+#define HPAGE_SHIFT	(PAGE_SHIFT + PAGE_SHIFT - 3)
+#define HPAGE_SIZE	(_AC(1, UL) << HPAGE_SHIFT)
+#define HPAGE_MASK	(~(HPAGE_SIZE - 1))
+#define HUGETLB_PAGE_ORDER	(HPAGE_SHIFT - PAGE_SHIFT)
+
+#ifndef __ASSEMBLY__
+
+#include <linux/kernel.h>
+#include <linux/pfn.h>
+
+/*
+ * It's normally defined only for FLATMEM config but it's
+ * used in our early mem init code for all memory models.
+ * So always define it.
+ */
+#define ARCH_PFN_OFFSET	PFN_UP(PHYS_OFFSET)
+
+extern void clear_page(void *page);
+extern void copy_page(void *to, void *from);
+
+extern unsigned long shm_align_mask;
+
+struct page;
+
+static inline void clear_user_page(void *addr, unsigned long vaddr,
+	struct page *page)
+{
+	clear_page(addr);
+}
+
+struct vm_area_struct;
+extern void copy_user_highpage(struct page *to, struct page *from,
+	unsigned long vaddr, struct vm_area_struct *vma);
+
+#define __HAVE_ARCH_COPY_USER_HIGHPAGE
+
+/*
+ * These are used to make use of C type-checking..
+ */
+typedef struct { unsigned long pte; } pte_t;
+#define pte_val(x)	((x).pte)
+#define __pte(x)	((pte_t) { (x) })
+typedef struct page *pgtable_t;
+
+/*
+ * Finall the top of the hierarchy, the pgd
+ */
+typedef struct { unsigned long pgd; } pgd_t;
+#define pgd_val(x)	((x).pgd)
+#define __pgd(x)	((pgd_t) { (x) })
+
+/*
+ * Manipulate page protection bits
+ */
+typedef struct { unsigned long pgprot; } pgprot_t;
+#define pgprot_val(x)	((x).pgprot)
+#define __pgprot(x)	((pgprot_t) { (x) })
+#define pte_pgprot(x)	__pgprot(pte_val(x) & ~_PFN_MASK)
+
+#define ptep_buddy(x)	((pte_t *)((unsigned long)(x) ^ sizeof(pte_t)))
+
+/*
+ * __pa()/__va() should be used only during mem init.
+ */
+#define __pa(x)		PHYSADDR(x)
+#define __va(x)		((void *)((unsigned long)(x) + PAGE_OFFSET - PHYS_OFFSET))
+
+#ifndef __pa_symbol
+#define __pa_symbol(x)	__pa(RELOC_HIDE((unsigned long)(x), 0))
+#endif
+
+#define pfn_to_kaddr(pfn)	__va((pfn) << PAGE_SHIFT)
+
+#ifdef CONFIG_FLATMEM
+
+static inline int pfn_valid(unsigned long pfn)
+{
+	/* avoid <linux/mm.h> include hell */
+	extern unsigned long max_mapnr;
+	unsigned long pfn_offset = ARCH_PFN_OFFSET;
+
+	return pfn >= pfn_offset && pfn < max_mapnr;
+}
+
+#endif
+
+#define virt_to_pfn(kaddr)	PFN_DOWN(virt_to_phys((void *)(kaddr)))
+#define virt_to_page(kaddr)	pfn_to_page(virt_to_pfn(kaddr))
+
+extern int __virt_addr_valid(volatile void *kaddr);
+#define virt_addr_valid(kaddr)	__virt_addr_valid((volatile void *)(kaddr))
+
+#define VM_DATA_DEFAULT_FLAGS \
+	(VM_READ | VM_WRITE | \
+	 ((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0) | \
+	 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
+
+extern unsigned long __kaslr_offset;
+static inline unsigned long kaslr_offset(void)
+{
+	return __kaslr_offset;
+}
+
+#include <asm-generic/memory_model.h>
+#include <asm-generic/getorder.h>
+
+#endif /* !__ASSEMBLY__ */
+
+#endif /* _ASM_PAGE_H */
diff --git a/arch/loongarch/include/asm/percpu.h b/arch/loongarch/include/asm/percpu.h
new file mode 100644
index 000000000000..ea5979872485
--- /dev/null
+++ b/arch/loongarch/include/asm/percpu.h
@@ -0,0 +1,20 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_PERCPU_H
+#define __ASM_PERCPU_H
+
+/* Use r21 for fast access */
+register unsigned long __my_cpu_offset __asm__("$r21");
+
+static inline void set_my_cpu_offset(unsigned long off)
+{
+	__my_cpu_offset = off;
+	csr_writeq(off, PERCPU_BASE_KS);
+}
+#define __my_cpu_offset __my_cpu_offset
+
+#include <asm-generic/percpu.h>
+
+#endif /* __ASM_PERCPU_H */
diff --git a/arch/loongarch/include/asm/perf_event.h b/arch/loongarch/include/asm/perf_event.h
new file mode 100644
index 000000000000..10aee5b066e0
--- /dev/null
+++ b/arch/loongarch/include/asm/perf_event.h
@@ -0,0 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Author: Huacai Chen <chenhuacai@loongson.cn>
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+
+#ifndef __LOONGARCH_PERF_EVENT_H__
+#define __LOONGARCH_PERF_EVENT_H__
+/* Leave it empty here. The file is required by linux/perf_event.h */
+#endif /* __LOONGARCH_PERF_EVENT_H__ */
diff --git a/arch/loongarch/include/asm/pgalloc.h b/arch/loongarch/include/asm/pgalloc.h
new file mode 100644
index 000000000000..03e87961dd65
--- /dev/null
+++ b/arch/loongarch/include/asm/pgalloc.h
@@ -0,0 +1,97 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_PGALLOC_H
+#define _ASM_PGALLOC_H
+
+#include <linux/highmem.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+
+#define __HAVE_ARCH_PMD_ALLOC_ONE
+#define __HAVE_ARCH_PUD_ALLOC_ONE
+#include <asm-generic/pgalloc.h>
+
+static inline void pmd_populate_kernel(struct mm_struct *mm, pmd_t *pmd,
+	pte_t *pte)
+{
+	set_pmd(pmd, __pmd((unsigned long)pte));
+}
+
+static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,
+	pgtable_t pte)
+{
+	set_pmd(pmd, __pmd((unsigned long)page_address(pte)));
+}
+
+/*
+ * Initialize a new pmd table with invalid pointers.
+ */
+extern void pmd_init(unsigned long page, unsigned long pagetable);
+
+#ifndef __PAGETABLE_PMD_FOLDED
+
+static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)
+{
+	set_pud(pud, __pud((unsigned long)pmd));
+}
+#endif
+
+/*
+ * Initialize a new pgd / pmd table with invalid pointers.
+ */
+extern void pgd_init(unsigned long page);
+extern pgd_t *pgd_alloc(struct mm_struct *mm);
+
+#define __pte_free_tlb(tlb, pte, address)			\
+do {							\
+	pgtable_pte_page_dtor(pte);			\
+	tlb_remove_page((tlb), pte);			\
+} while (0)
+
+#ifndef __PAGETABLE_PMD_FOLDED
+
+static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long address)
+{
+	pmd_t *pmd;
+
+	pmd = (pmd_t *) __get_free_pages(GFP_KERNEL, PMD_ORDER);
+	if (pmd)
+		pmd_init((unsigned long)pmd, (unsigned long)invalid_pte_table);
+	return pmd;
+}
+
+#define __pmd_free_tlb(tlb, x, addr)	pmd_free((tlb)->mm, x)
+
+#endif
+
+#ifndef __PAGETABLE_PUD_FOLDED
+
+static inline pud_t *pud_alloc_one(struct mm_struct *mm, unsigned long address)
+{
+	pud_t *pud;
+
+	pud = (pud_t *) __get_free_pages(GFP_KERNEL, PUD_ORDER);
+	if (pud)
+		pud_init((unsigned long)pud, (unsigned long)invalid_pmd_table);
+	return pud;
+}
+
+static inline void pud_free(struct mm_struct *mm, pud_t *pud)
+{
+	free_pages((unsigned long)pud, PUD_ORDER);
+}
+
+static inline void p4d_populate(struct mm_struct *mm, p4d_t *p4d, pud_t *pud)
+{
+	set_p4d(p4d, __p4d((unsigned long)pud));
+}
+
+#define __pud_free_tlb(tlb, x, addr)	pud_free((tlb)->mm, x)
+
+#endif /* __PAGETABLE_PUD_FOLDED */
+
+extern void pagetable_init(void);
+
+#endif /* _ASM_PGALLOC_H */
diff --git a/arch/loongarch/include/asm/pgtable-64.h b/arch/loongarch/include/asm/pgtable-64.h
new file mode 100644
index 000000000000..be9284879161
--- /dev/null
+++ b/arch/loongarch/include/asm/pgtable-64.h
@@ -0,0 +1,291 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ *
+ * Derived from MIPS:
+ * Copyright (C) 1994, 95, 96, 97, 98, 99, 2000, 2003 Ralf Baechle
+ * Copyright (C) 1999, 2000, 2001 Silicon Graphics, Inc.
+ */
+#ifndef _ASM_PGTABLE_64_H
+#define _ASM_PGTABLE_64_H
+
+#include <linux/compiler.h>
+#include <linux/linkage.h>
+
+#include <asm/addrspace.h>
+
+#if CONFIG_PGTABLE_LEVELS == 2
+#include <asm-generic/pgtable-nopmd.h>
+#elif CONFIG_PGTABLE_LEVELS == 3
+#include <asm-generic/pgtable-nopud.h>
+#else
+#include <asm-generic/pgtable-nop4d.h>
+#endif
+
+#if CONFIG_PGTABLE_LEVELS == 2
+#define PGDIR_SHIFT	(PAGE_SHIFT + (PAGE_SHIFT + PTE_ORDER - 3))
+#elif CONFIG_PGTABLE_LEVELS == 3
+#define PMD_SHIFT	(PAGE_SHIFT + (PAGE_SHIFT + PTE_ORDER - 3))
+#define PMD_SIZE	(1UL << PMD_SHIFT)
+#define PMD_MASK	(~(PMD_SIZE-1))
+#define PGDIR_SHIFT	(PMD_SHIFT + (PAGE_SHIFT + PMD_ORDER - 3))
+#elif CONFIG_PGTABLE_LEVELS == 4
+#define PMD_SHIFT	(PAGE_SHIFT + (PAGE_SHIFT + PTE_ORDER - 3))
+#define PMD_SIZE	(1UL << PMD_SHIFT)
+#define PMD_MASK	(~(PMD_SIZE-1))
+#define PUD_SHIFT	(PMD_SHIFT + (PAGE_SHIFT + PMD_ORDER - 3))
+#define PUD_SIZE	(1UL << PUD_SHIFT)
+#define PUD_MASK	(~(PUD_SIZE-1))
+#define PGDIR_SHIFT	(PUD_SHIFT + (PAGE_SHIFT + PUD_ORDER - 3))
+#endif
+
+#define PGDIR_SIZE	(1UL << PGDIR_SHIFT)
+#define PGDIR_MASK	(~(PGDIR_SIZE-1))
+
+#ifdef CONFIG_VA_BITS_40
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PGD_ORDER		1
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
+#ifdef CONFIG_PAGE_SIZE_16KB
+#define PGD_ORDER               0
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
+#ifdef CONFIG_PAGE_SIZE_64KB
+#define PGD_ORDER		0
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		aieeee_attempt_to_allocate_pmd
+#define PTE_ORDER		0
+#endif
+#endif
+
+#ifdef CONFIG_VA_BITS_48
+#ifdef CONFIG_PAGE_SIZE_4KB
+#define PGD_ORDER		0
+#define PUD_ORDER		0
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
+#ifdef CONFIG_PAGE_SIZE_16KB
+#define PGD_ORDER               1
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
+#ifdef CONFIG_PAGE_SIZE_64KB
+#define PGD_ORDER		0
+#define PUD_ORDER		aieeee_attempt_to_allocate_pud
+#define PMD_ORDER		0
+#define PTE_ORDER		0
+#endif
+#endif
+
+#define PTRS_PER_PGD	((PAGE_SIZE << PGD_ORDER) >> 3)
+#if CONFIG_PGTABLE_LEVELS > 3
+#define PTRS_PER_PUD	((PAGE_SIZE << PUD_ORDER) >> 3)
+#endif
+#if CONFIG_PGTABLE_LEVELS > 2
+#define PTRS_PER_PMD	((PAGE_SIZE << PMD_ORDER) >> 3)
+#endif
+#define PTRS_PER_PTE	((PAGE_SIZE << PTE_ORDER) >> 3)
+
+#define USER_PTRS_PER_PGD       ((TASK_SIZE64 / PGDIR_SIZE)?(TASK_SIZE64 / PGDIR_SIZE):1)
+#define FIRST_USER_ADDRESS	0UL
+
+#ifndef __ASSEMBLY__
+
+#include <asm/fixmap.h>
+
+/*
+ * TLB refill handlers may also map the vmalloc area into xkvrange.
+ * Avoid the first couple of pages so NULL pointer dereferences will
+ * still reliably trap.
+ */
+#define VMALLOC_START		(vm_map_base + (2 * PAGE_SIZE))
+#define VMALLOC_END	\
+	(vm_map_base + \
+	 min(PTRS_PER_PGD * PTRS_PER_PUD * PTRS_PER_PMD * PTRS_PER_PTE * PAGE_SIZE, (1UL << cpu_vabits)) - PMD_SIZE)
+
+#define pte_ERROR(e) \
+	pr_err("%s:%d: bad pte %016lx.\n", __FILE__, __LINE__, pte_val(e))
+#ifndef __PAGETABLE_PMD_FOLDED
+#define pmd_ERROR(e) \
+	pr_err("%s:%d: bad pmd %016lx.\n", __FILE__, __LINE__, pmd_val(e))
+#endif
+#ifndef __PAGETABLE_PUD_FOLDED
+#define pud_ERROR(e) \
+	pr_err("%s:%d: bad pud %016lx.\n", __FILE__, __LINE__, pud_val(e))
+#endif
+#define pgd_ERROR(e) \
+	pr_err("%s:%d: bad pgd %016lx.\n", __FILE__, __LINE__, pgd_val(e))
+
+extern pte_t invalid_pte_table[PTRS_PER_PTE];
+
+#ifndef __PAGETABLE_PUD_FOLDED
+/*
+ * For 4-level pagetables we defines these ourselves, for 3-level the
+ * definitions are below, for 2-level the
+ * definitions are supplied by <asm-generic/pgtable-nopmd.h>.
+ */
+typedef struct { unsigned long pud; } pud_t;
+#define pud_val(x)	((x).pud)
+#define __pud(x)	((pud_t) { (x) })
+
+extern pud_t invalid_pud_table[PTRS_PER_PUD];
+
+/*
+ * Empty pgd entries point to the invalid_pud_table.
+ */
+static inline int p4d_none(p4d_t p4d)
+{
+	return p4d_val(p4d) == (unsigned long)invalid_pud_table;
+}
+
+static inline int p4d_bad(p4d_t p4d)
+{
+	if (unlikely(p4d_val(p4d) & ~PAGE_MASK))
+		return 1;
+
+	return 0;
+}
+
+static inline int p4d_present(p4d_t p4d)
+{
+	return p4d_val(p4d) != (unsigned long)invalid_pud_table;
+}
+
+static inline void p4d_clear(pgd_t *p4dp)
+{
+	p4d_val(*p4dp) = (unsigned long)invalid_pud_table;
+}
+
+static inline pud_t *p4d_pgtable(p4d_t p4d)
+{
+	return (pud_t *)p4d_val(p4d);
+}
+
+static inline void set_p4d(pgd_t *p4d, pgd_t p4dval)
+{
+	*p4d = p4dval;
+}
+
+#endif
+
+#ifndef __PAGETABLE_PMD_FOLDED
+/*
+ * For 3-level pagetables we defines these ourselves, for 2-level the
+ * definitions are supplied by <asm-generic/pgtable-nopmd.h>.
+ */
+typedef struct { unsigned long pmd; } pmd_t;
+#define pmd_val(x)	((x).pmd)
+#define __pmd(x)	((pmd_t) { (x) })
+
+
+extern pmd_t invalid_pmd_table[PTRS_PER_PMD];
+#endif
+
+/*
+ * Empty pgd/pmd entries point to the invalid_pte_table.
+ */
+static inline int pmd_none(pmd_t pmd)
+{
+	return pmd_val(pmd) == (unsigned long) invalid_pte_table;
+}
+
+static inline int pmd_bad(pmd_t pmd)
+{
+	/* pmd_huge(pmd) but inline */
+	if (unlikely(pmd_val(pmd) & _PAGE_HUGE))
+		return 0;
+
+	if (unlikely(pmd_val(pmd) & ~PAGE_MASK))
+		return 1;
+
+	return 0;
+}
+
+static inline int pmd_present(pmd_t pmd)
+{
+	if (unlikely(pmd_val(pmd) & _PAGE_HUGE))
+		return !!(pmd_val(pmd) & (_PAGE_PRESENT | _PAGE_PROTNONE));
+
+	return pmd_val(pmd) != (unsigned long) invalid_pte_table;
+}
+
+static inline void pmd_clear(pmd_t *pmdp)
+{
+	pmd_val(*pmdp) = ((unsigned long) invalid_pte_table);
+}
+#ifndef __PAGETABLE_PMD_FOLDED
+
+/*
+ * Empty pud entries point to the invalid_pmd_table.
+ */
+static inline int pud_none(pud_t pud)
+{
+	return pud_val(pud) == (unsigned long) invalid_pmd_table;
+}
+
+static inline int pud_bad(pud_t pud)
+{
+	return pud_val(pud) & ~PAGE_MASK;
+}
+
+static inline int pud_present(pud_t pud)
+{
+	return pud_val(pud) != (unsigned long) invalid_pmd_table;
+}
+
+static inline void pud_clear(pud_t *pudp)
+{
+	pud_val(*pudp) = ((unsigned long) invalid_pmd_table);
+}
+#endif
+
+#define pte_page(x)		pfn_to_page(pte_pfn(x))
+
+#define pte_pfn(x)		((unsigned long)(((x).pte & _PFN_MASK) >> _PFN_SHIFT))
+#define pfn_pte(pfn, prot)	__pte(((pfn) << _PFN_SHIFT) | pgprot_val(prot))
+#define pfn_pmd(pfn, prot)	__pmd(((pfn) << _PFN_SHIFT) | pgprot_val(prot))
+
+#define __pgd_offset(address)	pgd_index(address)
+#define __pud_offset(address)	(((address) >> PUD_SHIFT) & (PTRS_PER_PUD-1))
+#define __pmd_offset(address)	pmd_index(address)
+
+#ifndef __PAGETABLE_PMD_FOLDED
+static inline pmd_t *pud_pgtable(pud_t pud)
+{
+	return (pmd_t *)pud_val(pud);
+}
+#define pud_phys(pud)		virt_to_phys((void *)pud_val(pud))
+#define pud_page(pud)		(pfn_to_page(pud_phys(pud) >> PAGE_SHIFT))
+
+#endif
+
+/*
+ * Initialize a new pgd / pmd table with invalid pointers.
+ */
+extern void pgd_init(unsigned long page);
+extern void pud_init(unsigned long page, unsigned long pagetable);
+extern void pmd_init(unsigned long page, unsigned long pagetable);
+
+/*
+ * Non-present pages:  high 40 bits are offset, next 8 bits type,
+ * low 16 bits zero.
+ */
+static inline pte_t mk_swap_pte(unsigned long type, unsigned long offset)
+{ pte_t pte; pte_val(pte) = (type << 16) | (offset << 24); return pte; }
+
+#define __swp_type(x)		(((x).val >> 16) & 0xff)
+#define __swp_offset(x)		((x).val >> 24)
+#define __swp_entry(type, offset) ((swp_entry_t) { pte_val(mk_swap_pte((type), (offset))) })
+#define __pte_to_swp_entry(pte) ((swp_entry_t) { pte_val(pte) })
+#define __swp_entry_to_pte(x)	((pte_t) { (x).val })
+
+#endif /* !__ASSEMBLY__ */
+
+#endif /* _ASM_PGTABLE_64_H */
diff --git a/arch/loongarch/include/asm/pgtable-bits.h b/arch/loongarch/include/asm/pgtable-bits.h
new file mode 100644
index 000000000000..a196c8b3760a
--- /dev/null
+++ b/arch/loongarch/include/asm/pgtable-bits.h
@@ -0,0 +1,135 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_PGTABLE_BITS_H
+#define _ASM_PGTABLE_BITS_H
+
+/* Page table bits */
+
+#define	_PAGE_VALID_SHIFT	0
+#define	_PAGE_DIRTY_SHIFT	1
+#define	_PAGE_PLV_SHIFT		2  /* 2~3, two bits */
+#define	_CACHE_SHIFT		4  /* 4~5, two bits */
+#define	_PAGE_GLOBAL_SHIFT	6
+#define	_PAGE_HUGE_SHIFT	6  /* HUGE is a PMD bit */
+#define	_PAGE_PRESENT_SHIFT	7
+#define	_PAGE_WRITE_SHIFT	8
+#define	_PAGE_PROTNONE_SHIFT	9
+#define	_PAGE_SPECIAL_SHIFT	10
+#define	_PAGE_HGLOBAL_SHIFT	12 /* HGlobal is a PMD bit */
+#define	_PAGE_PFN_SHIFT		12
+#define	_PAGE_PFN_END_SHIFT	48
+#define	_PAGE_NO_READ_SHIFT	61
+#define	_PAGE_NO_EXEC_SHIFT	62
+#define	_PAGE_RPLV_SHIFT	63
+
+/* Used only by software */
+#define _PAGE_PRESENT		(_ULCAST_(1) << _PAGE_PRESENT_SHIFT)
+#define _PAGE_WRITE		(_ULCAST_(1) << _PAGE_WRITE_SHIFT)
+#define _PAGE_PROTNONE		(_ULCAST_(1) << _PAGE_PROTNONE_SHIFT)
+#define _PAGE_SPECIAL		(_ULCAST_(1) << _PAGE_SPECIAL_SHIFT)
+
+/* Used by TLB hardware (placed in EntryLo*) */
+#define _PAGE_VALID		(_ULCAST_(1) << _PAGE_VALID_SHIFT)
+#define _PAGE_DIRTY		(_ULCAST_(1) << _PAGE_DIRTY_SHIFT)
+#define _PAGE_PLV		(_ULCAST_(3) << _PAGE_PLV_SHIFT)
+#define _PAGE_GLOBAL		(_ULCAST_(1) << _PAGE_GLOBAL_SHIFT)
+#define _PAGE_HUGE		(_ULCAST_(1) << _PAGE_HUGE_SHIFT)
+#define _PAGE_HGLOBAL		(_ULCAST_(1) << _PAGE_HGLOBAL_SHIFT)
+#define _PAGE_NO_READ		(_ULCAST_(1) << _PAGE_NO_READ_SHIFT)
+#define _PAGE_NO_EXEC		(_ULCAST_(1) << _PAGE_NO_EXEC_SHIFT)
+#define _PAGE_RPLV		(_ULCAST_(1) << _PAGE_RPLV_SHIFT)
+#define _CACHE_MASK		(_ULCAST_(3) << _CACHE_SHIFT)
+#define _PFN_SHIFT		(PAGE_SHIFT - 12 + _PAGE_PFN_SHIFT)
+
+#define _PAGE_USER	(PLV_USER << _PAGE_PLV_SHIFT)
+#define _PAGE_KERN	(PLV_KERN << _PAGE_PLV_SHIFT)
+
+#define _PFN_MASK (~((_ULCAST_(1) << (_PFN_SHIFT)) - 1) & \
+		  ((_ULCAST_(1) << (_PAGE_PFN_END_SHIFT)) - 1))
+
+/*
+ * Cache attributes
+ */
+
+#ifndef _CACHE_SUC
+#define _CACHE_SUC			(0<<_CACHE_SHIFT) /* Strong-ordered UnCached */
+#endif
+#ifndef _CACHE_CC
+#define _CACHE_CC			(1<<_CACHE_SHIFT) /* Coherent Cached */
+#endif
+#ifndef _CACHE_WUC
+#define _CACHE_WUC			(2<<_CACHE_SHIFT) /* Weak-ordered UnCached */
+#endif
+
+#define __READABLE	(_PAGE_VALID)
+#define __WRITEABLE	(_PAGE_DIRTY | _PAGE_WRITE)
+
+#define _PAGE_CHG_MASK	(_PAGE_SPECIAL | _PFN_MASK | _CACHE_MASK | _PAGE_PLV)
+#define _HPAGE_CHG_MASK	(_PAGE_SPECIAL | _PFN_MASK | _CACHE_MASK | _PAGE_PLV | _PAGE_HUGE)
+
+#define PAGE_NONE	__pgprot(_PAGE_PROTNONE | _PAGE_NO_READ | \
+				 _PAGE_USER | _CACHE_CC)
+#define PAGE_SHARED	__pgprot(_PAGE_PRESENT | _PAGE_WRITE | \
+				 _PAGE_USER | _CACHE_CC)
+#define PAGE_READONLY	__pgprot(_PAGE_PRESENT | _PAGE_USER | _CACHE_CC)
+
+#define PAGE_KERNEL	__pgprot(_PAGE_PRESENT | __READABLE | __WRITEABLE | \
+				 _PAGE_GLOBAL | _PAGE_KERN | _CACHE_CC)
+#define PAGE_KERNEL_SUC __pgprot(_PAGE_PRESENT | __READABLE | __WRITEABLE | \
+				 _PAGE_GLOBAL | _PAGE_KERN |  _CACHE_SUC)
+#define PAGE_KERNEL_WUC __pgprot(_PAGE_PRESENT | __READABLE | __WRITEABLE | \
+				 _PAGE_GLOBAL | _PAGE_KERN |  _CACHE_WUC)
+
+#define __P000 __pgprot(_CACHE_CC | _PAGE_PROTNONE | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __P001 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __P010 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __P011 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __P100 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+#define __P101 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+#define __P110 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+#define __P111 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+
+#define __S000 __pgprot(_CACHE_CC | _PAGE_PROTNONE | _PAGE_NO_EXEC | _PAGE_NO_READ)
+#define __S001 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC)
+#define __S010 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_WRITE | _PAGE_NO_READ)
+#define __S011 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_NO_EXEC | _PAGE_WRITE)
+#define __S100 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+#define __S101 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT)
+#define __S110 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_WRITE)
+#define __S111 __pgprot(_CACHE_CC | _PAGE_USER | _PAGE_PRESENT | _PAGE_WRITE)
+
+#ifndef __ASSEMBLY__
+
+/*
+ * Macro to make mark a page protection value as "uncacheable".	 Note
+ * that "protection" is really a misnomer here as the protection value
+ * contains the memory attribute bits, dirty bits, and various other
+ * bits as well.
+ */
+#define pgprot_noncached pgprot_noncached
+
+static inline pgprot_t pgprot_noncached(pgprot_t _prot)
+{
+	unsigned long prot = pgprot_val(_prot);
+
+	prot = (prot & ~_CACHE_MASK) | _CACHE_SUC;
+
+	return __pgprot(prot);
+}
+
+#define pgprot_writecombine pgprot_writecombine
+
+static inline pgprot_t pgprot_writecombine(pgprot_t _prot)
+{
+	unsigned long prot = pgprot_val(_prot);
+
+	prot = (prot & ~_CACHE_MASK) | _CACHE_WUC;
+
+	return __pgprot(prot);
+}
+
+#endif /* !__ASSEMBLY__ */
+
+#endif /* _ASM_PGTABLE_BITS_H */
diff --git a/arch/loongarch/include/asm/pgtable.h b/arch/loongarch/include/asm/pgtable.h
new file mode 100644
index 000000000000..0a6ecd01bd8e
--- /dev/null
+++ b/arch/loongarch/include/asm/pgtable.h
@@ -0,0 +1,341 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_PGTABLE_H
+#define _ASM_PGTABLE_H
+
+#include <linux/mm_types.h>
+#include <linux/mmzone.h>
+#include <asm/io.h>
+#include <asm/pgtable-bits.h>
+#include <asm/pgtable-64.h>
+
+struct mm_struct;
+struct vm_area_struct;
+
+/*
+ * ZERO_PAGE is a global shared page that is always zero; used
+ * for zero-mapped memory areas etc..
+ */
+
+extern unsigned long empty_zero_page;
+extern unsigned long zero_page_mask;
+
+#define ZERO_PAGE(vaddr) \
+	(virt_to_page((void *)(empty_zero_page + (((unsigned long)(vaddr)) & zero_page_mask))))
+#define __HAVE_COLOR_ZERO_PAGE
+
+extern void paging_init(void);
+
+/*
+ * Conversion functions: convert a page and protection to a page entry,
+ * and a page entry and page directory to the page they refer to.
+ */
+#define pmd_phys(pmd)		virt_to_phys((void *)pmd_val(pmd))
+
+#define __pmd_page(pmd)		(pfn_to_page(pmd_phys(pmd) >> PAGE_SHIFT))
+#ifndef CONFIG_TRANSPARENT_HUGEPAGE
+#define pmd_page(pmd)		__pmd_page(pmd)
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE  */
+
+#define pmd_page_vaddr(pmd)	pmd_val(pmd)
+
+static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep, pte_t pteval);
+
+#define pte_none(pte)		(!(pte_val(pte) & ~_PAGE_GLOBAL))
+#define pte_present(pte)	(pte_val(pte) & (_PAGE_PRESENT | _PAGE_PROTNONE))
+#define pte_no_exec(pte)	(pte_val(pte) & _PAGE_NO_EXEC)
+
+static inline void set_pte(pte_t *ptep, pte_t pteval)
+{
+	*ptep = pteval;
+	if (pte_val(pteval) & _PAGE_GLOBAL) {
+		pte_t *buddy = ptep_buddy(ptep);
+		/*
+		 * Make sure the buddy is global too (if it's !none,
+		 * it better already be global)
+		 */
+		if (pte_none(*buddy))
+			pte_val(*buddy) = pte_val(*buddy) | _PAGE_GLOBAL;
+	}
+}
+
+static inline void pte_clear(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
+{
+	/* Preserve global status for the pair */
+	if (pte_val(*ptep_buddy(ptep)) & _PAGE_GLOBAL)
+		set_pte_at(mm, addr, ptep, __pte(_PAGE_GLOBAL));
+	else
+		set_pte_at(mm, addr, ptep, __pte(0));
+}
+
+static inline void set_pte_at(struct mm_struct *mm, unsigned long addr,
+			      pte_t *ptep, pte_t pteval)
+{
+	extern void __update_cache(unsigned long address, pte_t pte);
+
+	if (!pte_present(pteval))
+		goto cache_sync_done;
+
+	if (pte_present(*ptep) && (pte_pfn(*ptep) == pte_pfn(pteval)))
+		goto cache_sync_done;
+
+	__update_cache(addr, pteval);
+cache_sync_done:
+	set_pte(ptep, pteval);
+}
+
+/*
+ * (pmds are folded into puds so this doesn't get actually called,
+ * but the define is needed for a generic inline function.)
+ */
+#define set_pmd(pmdptr, pmdval) do { *(pmdptr) = (pmdval); } while (0)
+
+#ifndef __PAGETABLE_PMD_FOLDED
+/*
+ * (puds are folded into pgds so this doesn't get actually called,
+ * but the define is needed for a generic inline function.)
+ */
+#define set_pud(pudptr, pudval) do { *(pudptr) = (pudval); } while (0)
+#endif
+
+#define PGD_T_LOG2	(__builtin_ffs(sizeof(pgd_t)) - 1)
+#define PMD_T_LOG2	(__builtin_ffs(sizeof(pmd_t)) - 1)
+#define PTE_T_LOG2	(__builtin_ffs(sizeof(pte_t)) - 1)
+
+extern pgd_t swapper_pg_dir[];
+extern pgd_t invalid_pg_dir[];
+
+/*
+ * The following only work if pte_present() is true.
+ * Undefined behaviour if not..
+ */
+static inline int pte_write(pte_t pte)	{ return pte_val(pte) & _PAGE_WRITE; }
+static inline int pte_dirty(pte_t pte)	{ return pte_val(pte) & _PAGE_DIRTY; }
+static inline int pte_young(pte_t pte)	{ return pte_val(pte) & _PAGE_VALID; }
+
+static inline pte_t pte_mkclean(pte_t pte)
+{
+	pte_val(pte) &= ~_PAGE_DIRTY;
+	return pte;
+}
+
+static inline pte_t pte_mkdirty(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_DIRTY;
+	return pte;
+}
+
+static inline pte_t pte_mkold(pte_t pte)
+{
+	pte_val(pte) &= ~_PAGE_VALID;
+	return pte;
+}
+
+static inline pte_t pte_mkyoung(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_VALID;
+	return pte;
+}
+
+static inline pte_t pte_mkwrite(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_WRITE;
+	return pte;
+}
+
+static inline pte_t pte_wrprotect(pte_t pte)
+{
+	pte_val(pte) &= ~(_PAGE_WRITE | _PAGE_DIRTY);
+	return pte;
+}
+
+static inline int pte_huge(pte_t pte)	{ return pte_val(pte) & _PAGE_HUGE; }
+
+static inline pte_t pte_mkhuge(pte_t pte)
+{
+	pte_val(pte) |= _PAGE_HUGE;
+	return pte;
+}
+
+#if defined(CONFIG_ARCH_HAS_PTE_SPECIAL)
+static inline int pte_special(pte_t pte)	{ return pte_val(pte) & _PAGE_SPECIAL; }
+static inline pte_t pte_mkspecial(pte_t pte)	{ pte_val(pte) |= _PAGE_SPECIAL; return pte; }
+#endif /* CONFIG_ARCH_HAS_PTE_SPECIAL */
+
+#define pte_accessible pte_accessible
+static inline unsigned long pte_accessible(struct mm_struct *mm, pte_t a)
+{
+	if (pte_val(a) & _PAGE_PRESENT)
+		return true;
+
+	if ((pte_val(a) & _PAGE_PROTNONE) &&
+			mm_tlb_flush_pending(mm))
+		return true;
+
+	return false;
+}
+
+/*
+ * Conversion functions: convert a page and protection to a page entry,
+ * and a page entry and page directory to the page they refer to.
+ */
+#define mk_pte(page, pgprot)	pfn_pte(page_to_pfn(page), (pgprot))
+
+static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)
+{
+	return __pte((pte_val(pte) & _PAGE_CHG_MASK) |
+		     (pgprot_val(newprot) & ~_PAGE_CHG_MASK));
+}
+
+extern void __update_tlb(struct vm_area_struct *vma, unsigned long address,
+	pte_t *ptep);
+
+static inline void update_mmu_cache(struct vm_area_struct *vma,
+	unsigned long address, pte_t *ptep)
+{
+	__update_tlb(vma, address, ptep);
+}
+
+static inline void update_mmu_cache_pmd(struct vm_area_struct *vma,
+	unsigned long address, pmd_t *pmdp)
+{
+	__update_tlb(vma, address, (pte_t *)pmdp);
+}
+
+#define kern_addr_valid(addr)	(1)
+
+#ifdef CONFIG_TRANSPARENT_HUGEPAGE
+
+/* We don't have hardware dirty/accessed bits, generic_pmdp_establish is fine.*/
+#define pmdp_establish generic_pmdp_establish
+
+#define has_transparent_hugepage has_transparent_hugepage
+extern int has_transparent_hugepage(void);
+
+static inline int pmd_trans_huge(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_HUGE);
+}
+
+static inline pmd_t pmd_mkhuge(pmd_t pmd)
+{
+	pmd_val(pmd) = (pmd_val(pmd) & ~(_PAGE_GLOBAL)) |
+		((pmd_val(pmd) & _PAGE_GLOBAL) << (_PAGE_HGLOBAL_SHIFT - _PAGE_GLOBAL_SHIFT));
+	pmd_val(pmd) |= _PAGE_HUGE;
+
+	return pmd;
+}
+
+extern void set_pmd_at(struct mm_struct *mm, unsigned long addr,
+		       pmd_t *pmdp, pmd_t pmd);
+
+#define pmd_write pmd_write
+static inline int pmd_write(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_WRITE);
+}
+
+static inline pmd_t pmd_mkwrite(pmd_t pmd)
+{
+	pmd_val(pmd) |= _PAGE_WRITE;
+	return pmd;
+}
+
+static inline pmd_t pmd_wrprotect(pmd_t pmd)
+{
+	pmd_val(pmd) &= ~(_PAGE_WRITE | _PAGE_DIRTY);
+	return pmd;
+}
+
+static inline int pmd_dirty(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_DIRTY);
+}
+
+static inline pmd_t pmd_mkclean(pmd_t pmd)
+{
+	pmd_val(pmd) &= ~_PAGE_DIRTY;
+	return pmd;
+}
+
+static inline pmd_t pmd_mkdirty(pmd_t pmd)
+{
+	pmd_val(pmd) |= _PAGE_DIRTY;
+	return pmd;
+}
+
+static inline int pmd_young(pmd_t pmd)
+{
+	return !!(pmd_val(pmd) & _PAGE_VALID);
+}
+
+static inline pmd_t pmd_mkold(pmd_t pmd)
+{
+	pmd_val(pmd) &= ~_PAGE_VALID;
+	return pmd;
+}
+
+static inline pmd_t pmd_mkyoung(pmd_t pmd)
+{
+	pmd_val(pmd) |= _PAGE_VALID;
+	return pmd;
+}
+
+/* Extern to avoid header file madness */
+extern pmd_t mk_pmd(struct page *page, pgprot_t prot);
+
+static inline unsigned long pmd_pfn(pmd_t pmd)
+{
+	return (pmd_val(pmd) & _PFN_MASK) >> _PFN_SHIFT;
+}
+
+static inline struct page *pmd_page(pmd_t pmd)
+{
+	if (pmd_trans_huge(pmd))
+		return pfn_to_page(pmd_pfn(pmd));
+
+	return pfn_to_page(pmd_phys(pmd) >> PAGE_SHIFT);
+}
+
+static inline pmd_t pmd_modify(pmd_t pmd, pgprot_t newprot)
+{
+	pmd_val(pmd) = (pmd_val(pmd) & _HPAGE_CHG_MASK) |
+				(pgprot_val(newprot) & ~_HPAGE_CHG_MASK);
+	return pmd;
+}
+
+static inline pmd_t pmd_mkinvalid(pmd_t pmd)
+{
+	pmd_val(pmd) &= ~(_PAGE_PRESENT | _PAGE_VALID | _PAGE_DIRTY | _PAGE_PROTNONE);
+
+	return pmd;
+}
+
+/*
+ * The generic version pmdp_huge_get_and_clear uses a version of pmd_clear() with a
+ * different prototype.
+ */
+#define __HAVE_ARCH_PMDP_HUGE_GET_AND_CLEAR
+static inline pmd_t pmdp_huge_get_and_clear(struct mm_struct *mm,
+					    unsigned long address, pmd_t *pmdp)
+{
+	pmd_t old = *pmdp;
+
+	pmd_clear(pmdp);
+
+	return old;
+}
+
+#endif /* CONFIG_TRANSPARENT_HUGEPAGE */
+
+/*
+ * We provide our own get_unmapped area to cope with the virtual aliasing
+ * constraints placed on us by the cache architecture.
+ */
+#define HAVE_ARCH_UNMAPPED_AREA
+#define HAVE_ARCH_UNMAPPED_AREA_TOPDOWN
+
+#endif /* _ASM_PGTABLE_H */
diff --git a/arch/loongarch/include/asm/prefetch.h b/arch/loongarch/include/asm/prefetch.h
new file mode 100644
index 000000000000..cc584180ac1c
--- /dev/null
+++ b/arch/loongarch/include/asm/prefetch.h
@@ -0,0 +1,29 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_PREFETCH_H
+#define __ASM_PREFETCH_H
+
+#define Pref_Load	0
+#define Pref_Store	8
+
+#ifdef __ASSEMBLY__
+
+	.macro	__pref hint addr
+#ifdef CONFIG_CPU_HAS_PREFETCH
+	preld	\hint, \addr, 0
+#endif
+	.endm
+
+	.macro	pref_load addr
+	__pref	Pref_Load, \addr
+	.endm
+
+	.macro	pref_store addr
+	__pref	Pref_Store, \addr
+	.endm
+
+#endif
+
+#endif /* __ASM_PREFETCH_H */
diff --git a/arch/loongarch/include/asm/processor.h b/arch/loongarch/include/asm/processor.h
new file mode 100644
index 000000000000..f573d36c22b8
--- /dev/null
+++ b/arch/loongarch/include/asm/processor.h
@@ -0,0 +1,219 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_PROCESSOR_H
+#define _ASM_PROCESSOR_H
+
+#include <linux/atomic.h>
+#include <linux/cpumask.h>
+#include <linux/sizes.h>
+#include <linux/threads.h>
+
+#include <asm/cpu.h>
+#include <asm/cpu-info.h>
+#include <asm/loongarchregs.h>
+#include <asm/vdso/processor.h>
+#include <uapi/asm/ptrace.h>
+
+/*
+ * LoongArch does have an arch_pick_mmap_layout()
+ */
+#define HAVE_ARCH_PICK_MMAP_LAYOUT 1
+
+#ifdef CONFIG_32BIT
+#define TASK_SIZE	0x80000000UL
+#define STACK_TOP_MAX	TASK_SIZE
+
+#define TASK_IS_32BIT_ADDR 1
+
+#endif
+
+#ifdef CONFIG_64BIT
+#define TASK_SIZE32	0x80000000UL
+
+#ifdef CONFIG_VA_BITS_40
+#define TASK_SIZE64     (0x1UL << ((cpu_vabits > 40)?40:cpu_vabits))
+#endif
+#ifdef CONFIG_VA_BITS_48
+#define TASK_SIZE64     (0x1UL << ((cpu_vabits > 48)?48:cpu_vabits))
+#endif
+
+#define TASK_SIZE (test_thread_flag(TIF_32BIT_ADDR) ? TASK_SIZE32 : TASK_SIZE64)
+#define STACK_TOP_MAX	TASK_SIZE64
+
+#define TASK_SIZE_OF(tsk)						\
+	(test_tsk_thread_flag(tsk, TIF_32BIT_ADDR) ? TASK_SIZE32 : TASK_SIZE64)
+
+#define TASK_IS_32BIT_ADDR test_thread_flag(TIF_32BIT_ADDR)
+
+#endif
+
+#define VDSO_RANDOMIZE_SIZE	(TASK_IS_32BIT_ADDR ? SZ_1M : SZ_64M)
+
+unsigned long stack_top(void);
+#define STACK_TOP stack_top()
+
+/*
+ * This decides where the kernel will search for a free chunk of vm
+ * space during mmap's.
+ */
+#define TASK_UNMAPPED_BASE PAGE_ALIGN(TASK_SIZE / 3)
+
+#define NUM_FPU_REGS	32
+#define FPU_REG_WIDTH	256
+
+union fpureg {
+	__u32	val32[FPU_REG_WIDTH / 32];
+	__u64	val64[FPU_REG_WIDTH / 64];
+};
+
+#define FPR_IDX(width, idx)	(idx)
+
+#define BUILD_FPR_ACCESS(width) \
+static inline u##width get_fpr##width(union fpureg *fpr, unsigned idx)	\
+{									\
+	return fpr->val##width[FPR_IDX(width, idx)];			\
+}									\
+									\
+static inline void set_fpr##width(union fpureg *fpr, unsigned int idx,	\
+				  u##width val)				\
+{									\
+	fpr->val##width[FPR_IDX(width, idx)] = val;			\
+}
+
+BUILD_FPR_ACCESS(32)
+BUILD_FPR_ACCESS(64)
+
+struct loongarch_fpu {
+	unsigned int	fcsr;
+	unsigned int	vcsr;
+	uint64_t	fcc;	/* 8x8 */
+	union fpureg	fpr[NUM_FPU_REGS];
+};
+
+#define INIT_CPUMASK { \
+	{0,} \
+}
+
+#define ARCH_MIN_TASKALIGN	32
+#define FPU_ALIGN		__aligned(32)
+
+struct loongarch_abi;
+
+/*
+ * If you change thread_struct remember to change the #defines below too!
+ */
+struct thread_struct {
+	/* Saved main processor registers. */
+	unsigned long reg01, reg03, reg22; /* ra sp fp */
+	unsigned long reg23, reg24, reg25, reg26; /* s0-s3 */
+	unsigned long reg27, reg28, reg29, reg30, reg31; /* s4-s8 */
+
+	/* Saved csr registers */
+	unsigned long csr_prmd;
+	unsigned long csr_crmd;
+	unsigned long csr_euen;
+	unsigned long csr_ecfg;
+
+	/* Saved scratch registers */
+	unsigned long scr0;
+	unsigned long scr1;
+	unsigned long scr2;
+	unsigned long scr3;
+
+	/* Saved eflag register */
+	unsigned long eflag;
+
+	/* Other stuff associated with the thread. */
+	unsigned long csr_badvaddr;	/* Last user fault */
+	unsigned long csr_baduaddr;	/* Last kernel fault accessing user space */
+	unsigned long error_code;
+	unsigned long trap_nr;
+	struct loongarch_abi *abi;
+
+	/*
+	 * Saved fpu register stuff, must be at last because
+	 * it is conditionally copied at fork.
+	 */
+	struct loongarch_fpu fpu FPU_ALIGN;
+};
+
+#define INIT_THREAD  {						\
+	/*							\
+	 * Saved main processor registers			\
+	 */							\
+	.reg01			= 0,				\
+	.reg03			= 0,				\
+	.reg22			= 0,				\
+	.reg23			= 0,				\
+	.reg24			= 0,				\
+	.reg25			= 0,				\
+	.reg26			= 0,				\
+	.reg27			= 0,				\
+	.reg28			= 0,				\
+	.reg29			= 0,				\
+	.reg30			= 0,				\
+	.reg31			= 0,				\
+	.csr_crmd		= 0,				\
+	.csr_prmd		= 0,				\
+	.csr_euen		= 0,				\
+	.csr_ecfg		= 0,				\
+	/*							\
+	 * Other stuff associated with the process		\
+	 */							\
+	.csr_badvaddr		= 0,				\
+	.csr_baduaddr		= 0,				\
+	.error_code		= 0,				\
+	.trap_nr		= 0,				\
+	/*							\
+	 * Saved fpu register stuff				\
+	 */							\
+	.fpu			= {				\
+		.fcsr		= 0,				\
+		.vcsr		= 0,				\
+		.fcc		= 0,				\
+		.fpr		= {{{0,},},},			\
+	},							\
+}
+
+struct task_struct;
+
+/* Free all resources held by a thread. */
+#define release_thread(thread) do { } while (0)
+
+enum idle_boot_override {IDLE_NO_OVERRIDE = 0, IDLE_HALT, IDLE_NOMWAIT, IDLE_POLL};
+
+extern unsigned long		boot_option_idle_override;
+/*
+ * Do necessary setup to start up a newly executed thread.
+ */
+extern void start_thread(struct pt_regs *regs, unsigned long pc, unsigned long sp);
+
+static inline void flush_thread(void)
+{
+}
+
+unsigned long get_wchan(struct task_struct *p);
+
+#define __KSTK_TOS(tsk) ((unsigned long)task_stack_page(tsk) + \
+			 THREAD_SIZE - 32 - sizeof(struct pt_regs))
+#define task_pt_regs(tsk) ((struct pt_regs *)__KSTK_TOS(tsk))
+#define KSTK_EIP(tsk) (task_pt_regs(tsk)->csr_epc)
+#define KSTK_ESP(tsk) (task_pt_regs(tsk)->regs[3])
+#define KSTK_EUEN(tsk) (task_pt_regs(tsk)->csr_euen)
+#define KSTK_ECFG(tsk) (task_pt_regs(tsk)->csr_ecfg)
+
+#define return_address() ({__asm__ __volatile__("":::"$1"); __builtin_return_address(0);})
+
+#ifdef CONFIG_CPU_HAS_PREFETCH
+
+#define ARCH_HAS_PREFETCH
+#define prefetch(x) __builtin_prefetch((x), 0, 1)
+
+#define ARCH_HAS_PREFETCHW
+#define prefetchw(x) __builtin_prefetch((x), 1, 1)
+
+#endif
+
+#endif /* _ASM_PROCESSOR_H */
diff --git a/arch/loongarch/include/asm/reboot.h b/arch/loongarch/include/asm/reboot.h
new file mode 100644
index 000000000000..0e96b0f42993
--- /dev/null
+++ b/arch/loongarch/include/asm/reboot.h
@@ -0,0 +1,10 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_REBOOT_H
+#define _ASM_REBOOT_H
+
+extern void (*pm_restart)(void);
+
+#endif /* _ASM_REBOOT_H */
diff --git a/arch/loongarch/include/asm/regdef.h b/arch/loongarch/include/asm/regdef.h
new file mode 100644
index 000000000000..7d98e38a5a2f
--- /dev/null
+++ b/arch/loongarch/include/asm/regdef.h
@@ -0,0 +1,43 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_REGDEF_H
+#define _ASM_REGDEF_H
+
+#define zero	$r0	/* wired zero */
+#define ra	$r1	/* return address */
+#define tp	$r2
+#define sp	$r3	/* stack pointer */
+#define v0	$r4	/* return value - caller saved */
+#define v1	$r5
+#define a0	$r4	/* argument registers */
+#define a1	$r5
+#define a2	$r6
+#define a3	$r7
+#define a4	$r8
+#define a5	$r9
+#define a6	$r10
+#define a7	$r11
+#define t0	$r12	/* caller saved */
+#define t1	$r13
+#define t2	$r14
+#define t3	$r15
+#define t4	$r16
+#define t5	$r17
+#define t6	$r18
+#define t7	$r19
+#define t8	$r20
+#define x0	$r21
+#define fp	$r22	/* frame pointer */
+#define s0	$r23	/* callee saved */
+#define s1	$r24
+#define s2	$r25
+#define s3	$r26
+#define s4	$r27
+#define s5	$r28
+#define s6	$r29
+#define s7	$r30
+#define s8	$r31
+
+#endif /* _ASM_REGDEF_H */
diff --git a/arch/loongarch/include/asm/serial.h b/arch/loongarch/include/asm/serial.h
new file mode 100644
index 000000000000..b031b921d924
--- /dev/null
+++ b/arch/loongarch/include/asm/serial.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM__SERIAL_H
+#define __ASM__SERIAL_H
+
+#include <asm-generic/serial.h>
+#define STD_COM_FLAGS (ASYNC_BOOT_AUTOCONF | ASYNC_SKIP_TEST)
+
+#endif /* __ASM__SERIAL_H */
diff --git a/arch/loongarch/include/asm/setup.h b/arch/loongarch/include/asm/setup.h
new file mode 100644
index 000000000000..33b8d9aa5fd0
--- /dev/null
+++ b/arch/loongarch/include/asm/setup.h
@@ -0,0 +1,24 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+
+#ifndef _LOONGARCH_SETUP_H
+#define _LOONGARCH_SETUP_H
+
+#include <linux/types.h>
+#include <uapi/asm/setup.h>
+
+extern void set_handler(unsigned long offset, void *addr, unsigned long len);
+extern void set_merr_handler(unsigned long offset, void *addr, unsigned long len);
+
+typedef void (*vi_handler_t)(int irq);
+extern void set_vi_handler(int n, vi_handler_t addr);
+
+extern unsigned long eentry;
+extern unsigned long tlbrentry;
+extern void cpu_cache_init(void);
+extern void boot_cpu_trap_init(void);
+extern void nonboot_cpu_trap_init(void);
+
+#endif /* __SETUP_H */
diff --git a/arch/loongarch/include/asm/spinlock.h b/arch/loongarch/include/asm/spinlock.h
new file mode 100644
index 000000000000..2544ee546596
--- /dev/null
+++ b/arch/loongarch/include/asm/spinlock.h
@@ -0,0 +1,12 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_SPINLOCK_H
+#define _ASM_SPINLOCK_H
+
+#include <asm/processor.h>
+#include <asm/qspinlock.h>
+#include <asm/qrwlock.h>
+
+#endif /* _ASM_SPINLOCK_H */
diff --git a/arch/loongarch/include/asm/spinlock_types.h b/arch/loongarch/include/asm/spinlock_types.h
new file mode 100644
index 000000000000..91f258401ef9
--- /dev/null
+++ b/arch/loongarch/include/asm/spinlock_types.h
@@ -0,0 +1,11 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_SPINLOCK_TYPES_H
+#define _ASM_SPINLOCK_TYPES_H
+
+#include <asm-generic/qspinlock_types.h>
+#include <asm-generic/qrwlock_types.h>
+
+#endif
diff --git a/arch/loongarch/include/asm/time.h b/arch/loongarch/include/asm/time.h
new file mode 100644
index 000000000000..4d97151c16aa
--- /dev/null
+++ b/arch/loongarch/include/asm/time.h
@@ -0,0 +1,50 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_TIME_H
+#define _ASM_TIME_H
+
+#include <linux/clockchips.h>
+#include <linux/clocksource.h>
+#include <asm/loongarchregs.h>
+
+extern u64 cpu_clock_freq;
+extern u64 const_clock_freq;
+
+extern void sync_counter(void);
+
+static inline unsigned int calc_const_freq(void)
+{
+	unsigned int res;
+	unsigned int base_freq;
+	unsigned int cfm, cfd;
+
+	res = read_cpucfg(LOONGARCH_CPUCFG2);
+	if (!(res & CPUCFG2_LLFTP))
+		return 0;
+
+	base_freq = read_cpucfg(LOONGARCH_CPUCFG4);
+	res = read_cpucfg(LOONGARCH_CPUCFG5);
+	cfm = res & 0xffff;
+	cfd = (res >> 16) & 0xffff;
+
+	if (!base_freq || !cfm || !cfd)
+		return 0;
+	else
+		return (base_freq * cfm / cfd);
+}
+
+/*
+ * Initialize the calling CPU's timer interrupt as clockevent device
+ */
+extern int constant_clockevent_init(void);
+extern int constant_clocksource_init(void);
+
+static inline void clockevent_set_clock(struct clock_event_device *cd,
+					unsigned int clock)
+{
+	clockevents_calc_mult_shift(cd, clock, 4);
+}
+
+#endif /* _ASM_TIME_H */
diff --git a/arch/loongarch/include/asm/timex.h b/arch/loongarch/include/asm/timex.h
new file mode 100644
index 000000000000..9880fa7f4150
--- /dev/null
+++ b/arch/loongarch/include/asm/timex.h
@@ -0,0 +1,36 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_TIMEX_H
+#define _ASM_TIMEX_H
+
+#ifdef __KERNEL__
+
+#include <linux/compiler.h>
+
+#include <asm/cpu.h>
+#include <asm/cpu-features.h>
+#include <asm/loongarchregs.h>
+
+/*
+ * Standard way to access the cycle counter.
+ * Currently only used on SMP for scheduling.
+ *
+ * We know that all SMP capable CPUs have cycle counters.
+ */
+
+typedef unsigned long cycles_t;
+
+static inline cycles_t get_cycles(void)
+{
+	return drdtime();
+}
+
+static inline unsigned long random_get_entropy(void)
+{
+	return drdtime();
+}
+#endif /* __KERNEL__ */
+
+#endif /*  _ASM_TIMEX_H */
diff --git a/arch/loongarch/include/asm/topology.h b/arch/loongarch/include/asm/topology.h
new file mode 100644
index 000000000000..bf39f3911424
--- /dev/null
+++ b/arch/loongarch/include/asm/topology.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_TOPOLOGY_H
+#define __ASM_TOPOLOGY_H
+
+#include <linux/smp.h>
+
+#define cpu_logical_map(cpu)  0
+
+#include <asm-generic/topology.h>
+
+static inline void arch_fix_phys_package_id(int num, u32 slot) { }
+#endif /* __ASM_TOPOLOGY_H */
diff --git a/arch/loongarch/include/asm/types.h b/arch/loongarch/include/asm/types.h
new file mode 100644
index 000000000000..6f8f14249d8a
--- /dev/null
+++ b/arch/loongarch/include/asm/types.h
@@ -0,0 +1,33 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_TYPES_H
+#define _ASM_TYPES_H
+
+#include <asm-generic/int-ll64.h>
+#include <uapi/asm/types.h>
+
+/*
+ * The following macros are especially useful for __asm__
+ * inline assembler.
+ */
+#ifndef __STR
+#define __STR(x) #x
+#endif
+#ifndef STR
+#define STR(x) __STR(x)
+#endif
+
+/*
+ *  Configure language
+ */
+#ifdef __ASSEMBLY__
+#define _ULCAST_
+#define _U64CAST_
+#else
+#define _ULCAST_ (unsigned long)
+#define _U64CAST_ (u64)
+#endif
+
+#endif /* _ASM_TYPES_H */
diff --git a/arch/loongarch/include/uapi/asm/abidefs.h b/arch/loongarch/include/uapi/asm/abidefs.h
new file mode 100644
index 000000000000..fcbf5d352efb
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/abidefs.h
@@ -0,0 +1,21 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+/*
+ * Author: Hanlu Li <lihanlu@loongson.cn>
+ *         Huacai Chen <chenhuacai@loongson.cn>
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __ASM_ABIDEFS_H
+#define __ASM_ABIDEFS_H
+
+#define _LOONGARCH_ISA_LOONGARCH32	1
+#define _LOONGARCH_ISA_LOONGARCH64	2
+
+/*
+ * Subprogram calling convention
+ */
+#define _LOONGARCH_SIM_ABILP32		1
+#define _LOONGARCH_SIM_ABILPX32		2
+#define _LOONGARCH_SIM_ABILP64		3
+
+#endif /* __ASM_ABIDEFS_H */
diff --git a/arch/loongarch/include/uapi/asm/bitfield.h b/arch/loongarch/include/uapi/asm/bitfield.h
new file mode 100644
index 000000000000..ad619f7203c4
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/bitfield.h
@@ -0,0 +1,15 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+/*
+ * Author: Hanlu Li <lihanlu@loongson.cn>
+ *         Huacai Chen <chenhuacai@loongson.cn>
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef __UAPI_ASM_BITFIELD_H
+#define __UAPI_ASM_BITFIELD_H
+
+#define __BITFIELD_FIELD(field, more)					\
+	more								\
+	field;
+
+#endif /* __UAPI_ASM_BITFIELD_H */
diff --git a/arch/loongarch/include/uapi/asm/bitsperlong.h b/arch/loongarch/include/uapi/asm/bitsperlong.h
new file mode 100644
index 000000000000..5c2c8779a695
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/bitsperlong.h
@@ -0,0 +1,9 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+#ifndef __ASM_LOONGARCH_BITSPERLONG_H
+#define __ASM_LOONGARCH_BITSPERLONG_H
+
+#define __BITS_PER_LONG _LOONGARCH_SZLONG
+
+#include <asm-generic/bitsperlong.h>
+
+#endif /* __ASM_LOONGARCH_BITSPERLONG_H */
diff --git a/arch/loongarch/include/uapi/asm/byteorder.h b/arch/loongarch/include/uapi/asm/byteorder.h
new file mode 100644
index 000000000000..5e1faa828a43
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/byteorder.h
@@ -0,0 +1,13 @@
+/* SPDX-License-Identifier: GPL-2.0+ WITH Linux-syscall-note */
+/*
+ * Author: Hanlu Li <lihanlu@loongson.cn>
+ *         Huacai Chen <chenhuacai@loongson.cn>
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _ASM_BYTEORDER_H
+#define _ASM_BYTEORDER_H
+
+#include <linux/byteorder/little_endian.h>
+
+#endif /* _ASM_BYTEORDER_H */
diff --git a/arch/loongarch/include/uapi/asm/inst.h b/arch/loongarch/include/uapi/asm/inst.h
new file mode 100644
index 000000000000..3e3843b48460
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/inst.h
@@ -0,0 +1,467 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Format of an instruction in memory.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+#ifndef _UAPI_ASM_INST_H
+#define _UAPI_ASM_INST_H
+
+#include <asm/bitfield.h>
+
+enum reg0_op {
+	tlbclr_op = 0x19208, gtlbclr_op = 0x19208,
+	tlbflush_op = 0x19209, gtlbflush_op = 0x19209,
+	tlbsrch_op = 0x1920a, gtlbsrch_op = 0x1920a,
+	tlbrd_op = 0x1920b, gtlbrd_op = 0x1920b,
+	tlbwr_op = 0x1920c, gtlbwr_op = 0x1920c,
+	tlbfill_op = 0x1920d, gtlbfill_op = 0x1920d,
+	ertn_op = 0x1920e,
+};
+
+enum reg0i15_op {
+	break_op = 0x54, dbcl_op, syscall_op, hypcall_op,
+	idle_op = 0xc91, dbar_op = 0x70e4, ibar_op,
+};
+
+enum reg1i20_op {
+	lu12iw_op = 0xa, lu32id_op, pcaddi_op, pcalau12i_op,
+	pcaddu12i_op, pcaddu18i_op,
+};
+
+enum reg1i21_op {
+	beqz_op = 0x10, bnez_op, bceqz_op, bcnez_op = 0x12, jiscr0_op = 0x12, jiscr1_op = 0x12,
+	jirl_op, b_op, bl_op, beq_op, bne_op, blt_op, bge_op, bltu_op, bgeu_op,
+};
+
+enum reg2_op {
+	gr2scr_op = 0x2, scr2gr_op, clow_op,
+	clzw_op, ctow_op, ctzw_op, clod_op,
+	clzd_op, ctod_op, ctzd_op, revb2h_op,
+	revb4h_op, revb2w_op, revbd_op, revh2w_op,
+	revhd_op, bitrev4b_op, bitrev8b_op, bitrevw_op,
+	bitrevd_op, extwh_op, extwb_op, rdtimelw_op,
+	rdtimehw_op, rdtimed_op, cpucfg_op,
+	iocsrrdb_op = 0x19200, iocsrrdh_op, iocsrrdw_op, iocsrrdd_op,
+	iocsrwrb_op, iocsrwrh_op, iocsrwrw_op, iocsrwrd_op,
+	movgr2fcsr_op = 0x4530, movfcsr2gr_op = 0x4532,
+	movgr2cf_op = 0x4536, movcf2gr_op = 0x4537,
+};
+
+enum reg2ui3_op {
+	rotrib_op = 0x261, rcrib_op = 0x281,
+};
+
+enum reg2ui4_op {
+	rotrih_op = 0x131, rcrih_op = 0x141,
+};
+
+enum reg2ui5_op {
+	slliw_op = 0x81, srliw_op = 0x89, sraiw_op = 0x91, rotriw_op = 0x99,
+	rcriw_op = 0xa1,
+};
+
+enum reg2ui6_op {
+	sllid_op = 0x41, srlid_op = 0x45, sraid_op = 0x49, rotrid_op = 0x4d,
+	rcrid_op = 0x51,
+};
+
+enum reg2lsbw_op {
+	bstrinsw_op = 0x3, bstrpickw_op = 0x3,
+};
+
+enum reg2lsbd_op {
+	bstrinsd_op = 0x2, bstrpickd_op = 0x3,
+};
+
+enum reg2i8_op {
+	lddir_op = 0x190, ldpte_op,
+};
+
+enum reg2i8idx1_op {
+	vstelmd_op = 0x622,
+};
+
+enum reg2i8idx2_op {
+	vstelmw_op = 0x312, xvstelmd_op = 0x331,
+};
+
+enum reg2i8idx3_op {
+	vstelmh_op = 0x18a, xvstelmw_op = 0x199,
+};
+
+enum reg2i8idx4_op {
+	vstelmb_op = 0xc6, xvstelmh_op = 0xcd,
+};
+
+enum reg2i8idx5_op {
+	xvstelmb_op = 0x67,
+};
+
+enum reg2i9_op {
+	vldrepld_op = 0x602, xvldrepld_op = 0x642,
+};
+
+enum reg2i10_op {
+	vldreplw_op = 0x302, xvldreplw_op = 0x322,
+};
+
+enum reg2i11_op {
+	vldreplh_op = 0x182, xvldreplh_op = 0x192,
+};
+
+enum reg2i12_op {
+	slti_op = 0x8, sltui_op, addiw_op, addid_op,
+	lu52id_op, andi_op, ori_op, xori_op,
+	cacop_op = 0x18, xvldreplb_op = 0xca,
+	ldb_op = 0xa0, ldh_op, ldw_op, ldd_op, stb_op, sth_op,
+	stw_op, std_op, ldbu_op, ldhu_op, ldwu_op, preld_op,
+	flds_op, fsts_op, fldd_op, fstd_op, vld_op, vst_op, xvld_op,
+	xvst_op, ldlw_op = 0xb8, ldrw_op, ldld_op, ldrd_op, stlw_op,
+	strw_op, stld_op, strd_op, vldreplb_op = 0xc2,
+};
+
+enum reg2i14_op {
+	llw_op = 0x20, scw_op, lld_op, scd_op, ldptrw_op, stptrw_op,
+	ldptrd_op, stptrd_op,
+};
+
+enum reg2i16_op {
+	addu16id_op = 0x4,
+};
+
+enum reg2csr_op {
+	csrrd_op = 0x4, csrwr_op = 0x4, csrxchg_op = 0x4,
+	gcsrrd_op = 0x5, gcsrwr_op = 0x5, gcsrxchg_op = 0x5,
+};
+
+enum reg3_op {
+	asrtled_op = 0x2, asrtgtd_op,
+	addw_op = 0x20, addd_op, subw_op, subd_op,
+	slt_op, sltu_op, maskeqz_op, masknez_op,
+	nor_op, and_op, or_op, xor_op, orn_op,
+	andn_op, sllw_op, srlw_op, sraw_op, slld_op,
+	srld_op, srad_op, rotrb_op, rotrh_op,
+	rotrw_op, rotrd_op, mulw_op, mulhw_op,
+	mulhwu_op, muld_op, mulhd_op, mulhdu_op,
+	mulwdw_op, mulwdwu_op, divw_op, modw_op,
+	divwu_op, modwu_op, divd_op, modd_op,
+	divdu_op, moddu_op, crcwbw_op,
+	crcwhw_op, crcwww_op, crcwdw_op, crccwbw_op,
+	crccwhw_op, crccwww_op, crccwdw_op, addu12iw_op,
+	addu12id_op,
+	adcb_op = 0x60, adch_op, adcw_op, adcd_op,
+	sbcb_op, sbch_op, sbcw_op, sbcd_op,
+	rcrb_op, rcrh_op, rcrw_op, rcrd_op,
+	ldxb_op = 0x7000, ldxh_op = 0x7008, ldxw_op = 0x7010, ldxd_op = 0x7018,
+	stxb_op = 0x7020, stxh_op = 0x7028, stxw_op = 0x7030, stxd_op = 0x7038,
+	ldxbu_op = 0x7040, ldxhu_op = 0x7048, ldxwu_op = 0x7050,
+	preldx_op = 0x7058, fldxs_op = 0x7060, fldxd_op = 0x7068,
+	fstxs_op = 0x7070, fstxd_op = 0x7078, vldx_op = 0x7080,
+	vstx_op = 0x7088, xvldx_op = 0x7090, xvstx_op = 0x7098,
+	amswapw_op = 0x70c0, amswapd_op, amaddw_op, amaddd_op, amandw_op,
+	amandd_op, amorw_op, amord_op, amxorw_op, amxord_op, ammaxw_op,
+	ammaxd_op, amminw_op, ammind_op, ammaxwu_op, ammaxdu_op,
+	amminwu_op, ammindu_op, amswap_dbw_op, amswap_dbd_op, amadd_dbw_op,
+	amadd_dbd_op, amand_dbw_op, amand_dbd_op, amor_dbw_op, amor_dbd_op,
+	amxor_dbw_op, amxor_dbd_op, ammax_dbw_op, ammax_dbd_op, ammin_dbw_op,
+	ammin_dbd_op, ammax_dbwu_op, ammax_dbdu_op, ammin_dbwu_op,
+	ammin_dbdu_op, fldgts_op = 0x70e8, fldgtd_op,
+	fldles_op, fldled_op, fstgts_op, fstgtd_op, fstles_op, fstled_op,
+	ldgtb_op, ldgth_op, ldgtw_op, ldgtd_op, ldleb_op, ldleh_op, ldlew_op,
+	ldled_op, stgtb_op, stgth_op, stgtw_op, stgtd_op, stleb_op, stleh_op,
+	stlew_op, stled_op,
+};
+
+enum reg3sa2_op {
+	alslw_op = 0x2, alslwu_op, bytepickw_op, alsld_op = 0x16,
+
+};
+
+enum reg3sa3_op {
+	bytepickd_op = 0x3,
+};
+
+struct reg2_format {
+	__BITFIELD_FIELD(unsigned int opcode : 22,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))
+};
+
+struct reg2ui3_format {
+	__BITFIELD_FIELD(unsigned int opcode : 19,
+	__BITFIELD_FIELD(unsigned int simmediate : 3,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2ui4_format {
+	__BITFIELD_FIELD(unsigned int opcode : 18,
+	__BITFIELD_FIELD(unsigned int simmediate : 4,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2ui5_format {
+	__BITFIELD_FIELD(unsigned int opcode : 17,
+	__BITFIELD_FIELD(unsigned int simmediate : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2ui6_format {
+	__BITFIELD_FIELD(unsigned int opcode : 16,
+	__BITFIELD_FIELD(unsigned int simmediate : 6,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2lsbw_format {
+	__BITFIELD_FIELD(unsigned int opcode : 11,
+	__BITFIELD_FIELD(unsigned int msbw : 5,
+	__BITFIELD_FIELD(unsigned int op : 1,
+	__BITFIELD_FIELD(unsigned int lsbw : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))))
+};
+
+struct reg2lsbd_format {
+	__BITFIELD_FIELD(unsigned int opcode : 10,
+	__BITFIELD_FIELD(unsigned int msbd : 6,
+	__BITFIELD_FIELD(unsigned int lsbd : 6,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg3_format {
+	__BITFIELD_FIELD(unsigned int opcode : 17,
+	__BITFIELD_FIELD(unsigned int rk : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg3sa2_format {
+	__BITFIELD_FIELD(unsigned int opcode : 15,
+	__BITFIELD_FIELD(unsigned int simmediate : 2,
+	__BITFIELD_FIELD(unsigned int rk : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg3sa3_format {
+	__BITFIELD_FIELD(unsigned int opcode : 14,
+	__BITFIELD_FIELD(unsigned int simmediate : 3,
+	__BITFIELD_FIELD(unsigned int rk : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg3sa4_format {
+	__BITFIELD_FIELD(unsigned int opcode : 13,
+	__BITFIELD_FIELD(unsigned int simmediate : 4,
+	__BITFIELD_FIELD(unsigned int rk : 5,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg4_format {
+	__BITFIELD_FIELD(unsigned int opcode : 12,
+	__BITFIELD_FIELD(unsigned int fa : 5,
+	__BITFIELD_FIELD(unsigned int fk : 5,
+	__BITFIELD_FIELD(unsigned int fj : 5,
+	__BITFIELD_FIELD(unsigned int fd : 5,
+	;)))))
+};
+
+struct reg2i8_format {
+	__BITFIELD_FIELD(unsigned int opcode : 14,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i8idx1_format {
+	__BITFIELD_FIELD(unsigned int opcode : 13,
+	__BITFIELD_FIELD(unsigned int idx : 1,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg2i8idx2_format {
+	__BITFIELD_FIELD(unsigned int opcode : 12,
+	__BITFIELD_FIELD(unsigned int idx : 2,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg2i8idx3_format {
+	__BITFIELD_FIELD(unsigned int opcode : 11,
+	__BITFIELD_FIELD(unsigned int idx : 3,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg2i8idx4_format {
+	__BITFIELD_FIELD(unsigned int opcode : 10,
+	__BITFIELD_FIELD(unsigned int idx : 4,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg2i8idx5_format {
+	__BITFIELD_FIELD(unsigned int opcode : 9,
+	__BITFIELD_FIELD(unsigned int idx : 5,
+	__BITFIELD_FIELD(unsigned int simmediate : 8,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))))
+};
+
+struct reg2i9_format {
+	__BITFIELD_FIELD(unsigned int opcode : 13,
+	__BITFIELD_FIELD(unsigned int simmediate : 9,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i10_format {
+	__BITFIELD_FIELD(unsigned int opcode : 12,
+	__BITFIELD_FIELD(unsigned int simmediate : 10,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i11_format {
+	__BITFIELD_FIELD(unsigned int opcode : 11,
+	__BITFIELD_FIELD(unsigned int simmediate : 11,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i12_format {
+	__BITFIELD_FIELD(unsigned int opcode : 10,
+	__BITFIELD_FIELD(signed int simmediate : 12,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2u12_format {
+	__BITFIELD_FIELD(unsigned int opcode : 10,
+	__BITFIELD_FIELD(unsigned int simmediate : 12,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i14_format {
+	__BITFIELD_FIELD(unsigned int opcode : 8,
+	__BITFIELD_FIELD(unsigned int simmediate : 14,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2i16_format {
+	__BITFIELD_FIELD(unsigned int opcode : 6,
+	__BITFIELD_FIELD(unsigned int simmediate : 16,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg2csr_format {
+	__BITFIELD_FIELD(unsigned int opcode : 8,
+	__BITFIELD_FIELD(unsigned int csr : 14,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;))))
+};
+
+struct reg1i21_format {
+	__BITFIELD_FIELD(unsigned int opcode : 6,
+	__BITFIELD_FIELD(unsigned int simmediate_l : 16,
+	__BITFIELD_FIELD(unsigned int rj : 5,
+	__BITFIELD_FIELD(unsigned int simmediate_h  : 5,
+	;))))
+};
+
+struct reg1i20_format {
+	__BITFIELD_FIELD(unsigned int opcode : 7,
+	__BITFIELD_FIELD(unsigned int simmediate : 20,
+	__BITFIELD_FIELD(unsigned int rd : 5,
+	;)))
+};
+
+struct i15_format {
+	__BITFIELD_FIELD(unsigned int opcode : 17,
+	__BITFIELD_FIELD(unsigned int simmediate : 15,
+	;))
+};
+
+struct i26_format {
+	__BITFIELD_FIELD(unsigned int opcode : 6,
+	__BITFIELD_FIELD(unsigned int simmediate_l : 16,
+	__BITFIELD_FIELD(unsigned int simmediate_h : 10,
+	;)))
+};
+
+union loongarch_instruction {
+	unsigned int word;
+	unsigned short halfword[2];
+	unsigned char byte[4];
+	struct reg2_format reg2_format;
+	struct reg2ui3_format reg2ui3_format;
+	struct reg2ui4_format reg2ui4_format;
+	struct reg2ui5_format reg2ui5_format;
+	struct reg2ui6_format reg2ui6_format;
+	struct reg2lsbw_format reg2lsbw_format;
+	struct reg2lsbd_format reg2lsbd_format;
+	struct reg3_format reg3_format;
+	struct reg3sa2_format reg3sa2_format;
+	struct reg3sa3_format reg3sa3_format;
+	struct reg3sa4_format reg3sa4_format;
+	struct reg4_format reg4_format;
+	struct reg2i8_format reg2i8_format;
+	struct reg2i8idx1_format reg2i8idx1_format;
+	struct reg2i8idx2_format reg2i8idx2_format;
+	struct reg2i8idx3_format reg2i8idx3_format;
+	struct reg2i8idx4_format reg2i8idx4_format;
+	struct reg2i8idx5_format reg2i8idx5_format;
+	struct reg2i9_format reg2i9_format;
+	struct reg2i10_format reg2i10_format;
+	struct reg2i11_format reg2i11_format;
+	struct reg2i12_format reg2i12_format;
+	struct reg2i14_format reg2i14_format;
+	struct reg2i16_format reg2i16_format;
+	struct reg2csr_format reg2csr_format;
+	struct reg1i21_format reg1i21_format;
+	struct reg1i20_format reg1i20_format;
+	struct i15_format i15_format;
+	struct i26_format i26_format;
+};
+
+#endif /* _UAPI_ASM_INST_H */
diff --git a/arch/loongarch/include/uapi/asm/reg.h b/arch/loongarch/include/uapi/asm/reg.h
new file mode 100644
index 000000000000..1dec1fbfd316
--- /dev/null
+++ b/arch/loongarch/include/uapi/asm/reg.h
@@ -0,0 +1,103 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
+/*
+ * Various register offset definitions for debuggers, core file
+ * examiners and whatnot.
+ *
+ * Copyright (C) 2020-2021 Loongson Technology Corporation Limited
+ */
+
+#ifndef __UAPI_ASM_LOONGARCH_REG_H
+#define __UAPI_ASM_LOONGARCH_REG_H
+
+#define LOONGARCH64_EF_R0		0
+#define LOONGARCH64_EF_R1		1
+#define LOONGARCH64_EF_R2		2
+#define LOONGARCH64_EF_R3		3
+#define LOONGARCH64_EF_R4		4
+#define LOONGARCH64_EF_R5		5
+#define LOONGARCH64_EF_R6		6
+#define LOONGARCH64_EF_R7		7
+#define LOONGARCH64_EF_R8		8
+#define LOONGARCH64_EF_R9		9
+#define LOONGARCH64_EF_R10		10
+#define LOONGARCH64_EF_R11		11
+#define LOONGARCH64_EF_R12		12
+#define LOONGARCH64_EF_R13		13
+#define LOONGARCH64_EF_R14		14
+#define LOONGARCH64_EF_R15		15
+#define LOONGARCH64_EF_R16		16
+#define LOONGARCH64_EF_R17		17
+#define LOONGARCH64_EF_R18		18
+#define LOONGARCH64_EF_R19		19
+#define LOONGARCH64_EF_R20		20
+#define LOONGARCH64_EF_R21		21
+#define LOONGARCH64_EF_R22		22
+#define LOONGARCH64_EF_R23		23
+#define LOONGARCH64_EF_R24		24
+#define LOONGARCH64_EF_R25		25
+#define LOONGARCH64_EF_R26		26
+#define LOONGARCH64_EF_R27		27
+#define LOONGARCH64_EF_R28		28
+#define LOONGARCH64_EF_R29		29
+#define LOONGARCH64_EF_R30		30
+#define LOONGARCH64_EF_R31		31
+
+/*
+ * Saved special registers
+ */
+#define LOONGARCH64_EF_CSR_EPC		32
+#define LOONGARCH64_EF_CSR_BADVADDR	33
+#define LOONGARCH64_EF_CSR_CRMD		34
+#define LOONGARCH64_EF_CSR_PRMD		35
+#define LOONGARCH64_EF_CSR_EUEN		36
+#define LOONGARCH64_EF_CSR_ECFG		37
+#define LOONGARCH64_EF_CSR_ESTAT	38
+
+#define LOONGARCH64_EF_SIZE		320	/* size in bytes */
+
+#if _LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64
+
+#define EF_R0			LOONGARCH64_EF_R0
+#define EF_R1			LOONGARCH64_EF_R1
+#define EF_R2			LOONGARCH64_EF_R2
+#define EF_R3			LOONGARCH64_EF_R3
+#define EF_R4			LOONGARCH64_EF_R4
+#define EF_R5			LOONGARCH64_EF_R5
+#define EF_R6			LOONGARCH64_EF_R6
+#define EF_R7			LOONGARCH64_EF_R7
+#define EF_R8			LOONGARCH64_EF_R8
+#define EF_R9			LOONGARCH64_EF_R9
+#define EF_R10			LOONGARCH64_EF_R10
+#define EF_R11			LOONGARCH64_EF_R11
+#define EF_R12			LOONGARCH64_EF_R12
+#define EF_R13			LOONGARCH64_EF_R13
+#define EF_R14			LOONGARCH64_EF_R14
+#define EF_R15			LOONGARCH64_EF_R15
+#define EF_R16			LOONGARCH64_EF_R16
+#define EF_R17			LOONGARCH64_EF_R17
+#define EF_R18			LOONGARCH64_EF_R18
+#define EF_R19			LOONGARCH64_EF_R19
+#define EF_R20			LOONGARCH64_EF_R20
+#define EF_R21			LOONGARCH64_EF_R21
+#define EF_R22			LOONGARCH64_EF_R22
+#define EF_R23			LOONGARCH64_EF_R23
+#define EF_R24			LOONGARCH64_EF_R24
+#define EF_R25			LOONGARCH64_EF_R25
+#define EF_R26			LOONGARCH64_EF_R26
+#define EF_R27			LOONGARCH64_EF_R27
+#define EF_R28			LOONGARCH64_EF_R28
+#define EF_R29			LOONGARCH64_EF_R29
+#define EF_R30			LOONGARCH64_EF_R30
+#define EF_R31			LOONGARCH64_EF_R31
+#define EF_CP0_EPC		LOONGARCH64_EF_CSR_EPC
+#define EF_CP0_BADVADDR		LOONGARCH64_EF_CSR_BADVADDR
+#define EF_CP0_CRMD		LOONGARCH64_EF_CSR_CRMD
+#define EF_CP0_PRMD		LOONGARCH64_EF_CSR_PRMD
+#define EF_CP0_EUEN		LOONGARCH64_EF_CSR_EUEN
+#define EF_CP0_ECFG		LOONGARCH64_EF_CSR_ECFG
+#define EF_CP0_ESTAT		LOONGARCH64_EF_CSR_ESTAT
+#define EF_SIZE			LOONGARCH64_EF_SIZE
+
+#endif /* _LOONGARCH_SIM == _LOONGARCH_SIM_ABILP64 */
+
+#endif /* __UAPI_ASM_LOONGARCH_REG_H */
-- 
2.37.2

